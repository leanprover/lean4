// Lean compiler output
// Module: Lean.Meta.Tactic.Simp.BuiltinSimprocs.BitVec
// Imports: Lean.Meta.Tactic.Simp.BuiltinSimprocs.Nat Lean.Meta.Tactic.Simp.BuiltinSimprocs.Int Init.Data.BitVec.Basic
#include <lean/lean.h>
#if defined(__clang__)
#pragma clang diagnostic ignored "-Wunused-parameter"
#pragma clang diagnostic ignored "-Wunused-label"
#elif defined(__GNUC__) && !defined(__CLANG__)
#pragma GCC diagnostic ignored "-Wunused-parameter"
#pragma GCC diagnostic ignored "-Wunused-label"
#pragma GCC diagnostic ignored "-Wunused-but-set-variable"
#endif
#ifdef __cplusplus
extern "C" {
#endif
static lean_object* l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__3;
lean_object* l_Lean_Expr_const___override(lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__12;
static lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__12;
lean_object* lean_format_pretty(lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__3;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceZeroExtend(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__1;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2925_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3673_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__3;
static lean_object* l_Std_BitVec_reduceBin___lambda__2___closed__8;
static lean_object* l_Std_BitVec_reduceCast___closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToNat___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__2;
lean_object* l_Std_BitVec_smtSDiv(lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSDiv___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceMod___closed__3;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1412_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__8;
lean_object* l_Lean_mkNatLit(lean_object*);
lean_object* l_Std_BitVec_signExtend(lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceLE(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__6;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceHShiftLeft___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUnary___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__5;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2059_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__11;
static lean_object* l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__11;
lean_object* lean_mk_empty_array_with_capacity(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2163_(lean_object*);
static lean_object* l_Std_BitVec_reduceXOr___closed__1;
static lean_object* l_Std_BitVec_reduceNot___closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_fromExpr_x3f(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__10;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__8;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceZeroExtend_x27___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1475_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1683_(lean_object*);
static lean_object* l_Std_BitVec_reduceBin___lambda__2___closed__14;
static lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__15;
static lean_object* l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__7;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSDiv(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__14;
static lean_object* l_Std_BitVec_reduceNot___closed__3;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNeg___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791_(lean_object*);
static lean_object* l_Std_BitVec_reduceAdd___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__5;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817_(lean_object*);
static lean_object* l_Std_BitVec_reduceGetMsb___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__4;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceDiv___lambda__2(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBin(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceDiv___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSub___lambda__2(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
lean_object* l_Std_BitVec_ofInt(lean_object*, lean_object*);
lean_object* lean_whnf(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__6;
static lean_object* l_Std_BitVec_reduceBin___lambda__2___closed__7;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3146_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__7;
lean_object* l_Std_BitVec_rotateLeft(lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2968_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUShiftRight(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__9;
static lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSub(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__9;
static lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__11;
static lean_object* l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__4;
lean_object* l_Lean_mkAppB(lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2352_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__4;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceOr___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3144_(lean_object*);
static lean_object* l_Std_BitVec_reduceOr___closed__1;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1410_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAppend(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSLE___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
uint8_t l_Lean_Expr_isAppOfArity(lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__6;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMod(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__7;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBinPred(lean_object*, lean_object*, lean_object*, lean_object*, uint8_t, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2617_(lean_object*);
static lean_object* l_Std_BitVec_reduceSMTUDiv___closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShiftLeftZeroExtend___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTSDiv___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceLT___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGE(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__7;
static lean_object* l_Std_BitVec_reduceBin___lambda__2___closed__13;
static lean_object* l_Std_BitVec_reduceGetLsb___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__3;
lean_object* lean_array_push(lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
static lean_object* l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__6;
static lean_object* l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__3;
static lean_object* l_Std_BitVec_reduceLE___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__6;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2165_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceExtracLsb_x27___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__2;
static lean_object* l_Std_BitVec_reduceGetBit___lambda__1___closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__5;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1819_(lean_object*);
static lean_object* l_Std_BitVec_reduceDiv___closed__3;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1557_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1873_(lean_object*);
static lean_object* l_Std_BitVec_reduceUnary___lambda__1___closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAllOnes(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__6;
static lean_object* l_Std_BitVec_reduceDiv___closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSub___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2101_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceRotateRight___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__3;
static lean_object* l_Std_BitVec_reduceToNat___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__1;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1599_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__6;
static lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTUDiv___lambda__2(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceUShiftRight___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__10;
static lean_object* l_Std_BitVec_reduceSShiftRight___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__5;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetMsb___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceUMod___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__3;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSShiftRight___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
lean_object* l_Std_BitVec_srem(lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceULT___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__6;
static lean_object* l_Std_BitVec_reduceReplicate___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__3;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3852_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__6;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2509_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183_(lean_object*);
static lean_object* l_Std_BitVec_reduceLE___closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetBit___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetMsb___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__10;
static lean_object* l_Std_BitVec_reduceUShiftRight___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__10;
static lean_object* l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__4;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSLT___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__11;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__3;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSignExtend___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__11;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__2;
static lean_object* l_Std_BitVec_reduceToInt___closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNot___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__3;
static lean_object* l_Std_BitVec_reduceUDiv___closed__1;
lean_object* l_Lean_Meta_Simp_evalPropStep(lean_object*, uint8_t, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__6;
static lean_object* l_Std_BitVec_reduceToNat___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__12;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAbs(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__12;
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__3;
static lean_object* l_Std_BitVec_reduceHShiftLeft___closed__1;
lean_object* l_Std_BitVec_toHex(lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2923_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__4;
lean_object* l_Std_BitVec_extractLsb_x27___rarg(lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__9;
static lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__6;
lean_object* l_Lean_stringToMessageData(lean_object*);
static lean_object* l_Std_BitVec_reduceGE___closed__1;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2507_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShiftLeft(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__14;
static lean_object* l_Std_BitVec_reduceOfInt___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__1;
lean_object* l_Std_BitVec_allOnes(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1949_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1991_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2187_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__8;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371_(lean_object*);
lean_object* l_Std_BitVec_append___rarg(lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__7;
lean_object* lean_nat_shiftr(lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNot___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTUDiv___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceZeroExtend_x27___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__1;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3052_(lean_object*);
static lean_object* l_Std_BitVec_reduceDiv___closed__2;
uint8_t l_Std_BitVec_sle(lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__6;
static lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__4;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850_(lean_object*);
lean_object* l_Lean_Expr_appArg_x21(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__10;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNeg(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__8;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1795_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__4;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToInt___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2143_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceReplicate(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__13;
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__5;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceULE(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMod___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceHShiftLeft___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__6;
static lean_object* l_Std_BitVec_reduceGT___closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__7;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceExtend(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__9;
lean_object* l_Int_toExpr(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3831_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__11;
static lean_object* l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__7;
static lean_object* l_Std_BitVec_reduceGT___closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceRotateRight(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUnary___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSRem___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceHShiftRight___closed__2;
static lean_object* l_Std_BitVec_reduceBin___lambda__2___closed__11;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3098_(lean_object*);
static lean_object* l_Std_BitVec_reduceGetBit___lambda__1___closed__2;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1899_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__7;
lean_object* l_Lean_Name_mkStr3(lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__11;
static lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__9;
static lean_object* l_Std_BitVec_reduceULT___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSLT(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceRotateLeft___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__2;
uint8_t l_Std_BitVec_slt(lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__2;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1685_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__6;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGT(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceAdd___closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__7;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMul___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__8;
lean_object* l_Std_BitVec_sshiftRight(lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1951_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723_(lean_object*);
static lean_object* l_Std_BitVec_reduceMod___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__6;
static lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__3;
extern lean_object* l_Lean_Meta_Simp_builtinSimprocsRef;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__6;
static lean_object* l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__6;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597_(lean_object*);
static lean_object* l_Std_BitVec_reduceGetBit___lambda__1___closed__7;
static lean_object* l_Std_BitVec_reduceXOr___closed__3;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAllOnes___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
lean_object* l_Std_BitVec_not(lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUnary(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__6;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceULT___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__3;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1473_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__4;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAdd(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1431_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__3;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAnd(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceAnd___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__8;
static lean_object* l_Std_BitVec_reduceExtracLsb_x27___closed__2;
static lean_object* l_Std_BitVec_reduceMul___closed__3;
static lean_object* l_Std_BitVec_reduceUDiv___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__3;
static lean_object* l_Std_BitVec_reduceSDiv___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__6;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2354_(lean_object*);
static lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__10;
static lean_object* l_Std_BitVec_reduceLT___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__7;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__11;
static lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__7;
static lean_object* l_Std_BitVec_reduceRotateRight___closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceOr___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__4;
lean_object* lean_nat_div(lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceULE___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__13;
static lean_object* l_Std_BitVec_reduceGetMsb___closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToNat___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__2;
static lean_object* l_Std_BitVec_reduceGetBit___lambda__1___closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__9;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2882_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__10;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBinPred___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__9;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToNat(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3994_(lean_object*);
lean_object* l_Lean_Meta_SavedState_restore(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceHShiftRight___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__10;
static lean_object* l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceRotateRight___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__6;
static lean_object* l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__11;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2015_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1601_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__8;
static lean_object* l_Std_BitVec_reduceBin___lambda__2___closed__3;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMod___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__14;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUDiv(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMul(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShiftLeftZeroExtend___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
lean_object* l_Std_BitVec_sdiv(lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceRotateLeft___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceGetBit___lambda__1___closed__1;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3992_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__5;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceExtend___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceLT___closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__1;
lean_object* l_Std_BitVec_toInt(lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1793_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__11;
static lean_object* l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__4;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceULE___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__6;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceDiv(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__9;
static lean_object* l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__9;
static lean_object* l_Std_BitVec_reduceSLE___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__11;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSLE(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__7;
static lean_object* l_Std_BitVec_reduceSub___closed__3;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUMod(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__6;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119_(lean_object*);
static lean_object* l_Std_BitVec_reduceOfInt___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__3;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3123_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__13;
static lean_object* l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__2;
static lean_object* l_Std_BitVec_reduceHShiftLeft___closed__3;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3483_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__6;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__3;
static lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromBitVecExpr_x3f___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__4;
static lean_object* l_Std_BitVec_reduceSMod___closed__2;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3054_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__6;
static lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__13;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceLE___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToInt(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1897_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1725_(lean_object*);
static lean_object* l_Std_BitVec_reduceSMTSDiv___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__6;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__8;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSub___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceGetLsb___closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNeg___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__10;
lean_object* l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__1;
static lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__2;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__5;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1515_(lean_object*);
static lean_object* l_Std_BitVec_reduceAppend___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__6;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetBit(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToNat___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__12;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__8;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3481_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBin___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011_(lean_object*);
static lean_object* l_Std_BitVec_reduceBin___lambda__2___closed__5;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUShiftRight___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__5;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBinPred___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__3;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAdd___lambda__2(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceSMod___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__6;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMul___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceXOr___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__4;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1972_(lean_object*);
static lean_object* l_Std_BitVec_reduceAnd___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShift___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSignExtend(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShiftLeft___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceHShiftRight___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__9;
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__1;
lean_object* l_Int_fromExpr_x3f(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToInt___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
lean_object* lean_nat_land(lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3011_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__9;
static lean_object* l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__3;
static lean_object* l_Std_BitVec_reduceSub___closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShift(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__3;
static lean_object* l_Std_BitVec_reduceExtracLsb_x27___closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAbs___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__4;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2880_(lean_object*);
static lean_object* l_Std_BitVec_reduceAdd___closed__2;
static lean_object* l_Std_BitVec_reduceShiftLeft___closed__2;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__9;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2035_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBinPred___lambda__1(lean_object*, lean_object*, uint8_t, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceSLT___closed__2;
static lean_object* l_Std_BitVec_reduceNot___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__6;
static lean_object* l_Std_BitVec_reduceAllOnes___closed__2;
LEAN_EXPORT lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromBitVecExpr_x3f___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__12;
static lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__10;
static lean_object* l_Std_BitVec_reduceZeroExtend_x27___closed__1;
static lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__6;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__9;
static lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__4;
static lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__4;
static lean_object* l_Std_BitVec_reduceZeroExtend___closed__2;
lean_object* l_Lean_Meta_saveState___rarg(lean_object*, lean_object*, lean_object*, lean_object*);
lean_object* l_Std_BitVec_abs(lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2185_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1727_(lean_object*);
static lean_object* l_Std_BitVec_reduceMul___closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShiftLeft___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetLsb(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
lean_object* l_Lean_Expr_appFn_x21(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__7;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAdd___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__6;
static lean_object* l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSRem___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetMsb(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__1;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3121_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__2;
extern lean_object* l_Std_Format_defWidth;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__7;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__6;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAnd___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__10;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__3;
lean_object* l_Std_BitVec_zeroExtend(lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceExtend___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceXOr(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceAllOnes___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__6;
static lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__12;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNeg___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1767_(lean_object*);
static lean_object* l_Std_BitVec_reduceSDiv___closed__2;
static lean_object* l_Std_BitVec_reduceUMod___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__6;
static lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__8;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMod___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__6;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMod(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__11;
static lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__4;
static lean_object* l_Std_BitVec_reduceGetBit___lambda__1___closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSLE___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAnd___lambda__2(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNot___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
lean_object* l_Std_BitVec_ofNat(lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__5;
static lean_object* l_Std_BitVec_reduceRotateLeft___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__4;
lean_object* l_Std_BitVec_add(lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3075_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceLT(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__6;
static lean_object* l_Std_BitVec_reduceBin___lambda__2___closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__3;
LEAN_EXPORT lean_object* l_Std_BitVec_fromExpr_x3f___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAppend___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__12;
static lean_object* l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShiftLeftZeroExtend(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShift___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__4;
static lean_object* l_Std_BitVec_reduceSub___closed__1;
static lean_object* l_Std_BitVec_reduceBin___lambda__2___closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBin___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceCast___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceShiftLeftZeroExtend___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__2;
static lean_object* l_Std_BitVec_reduceShiftLeft___closed__1;
static lean_object* l_Std_BitVec_reduceRotateRight___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__13;
static lean_object* l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__6;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3100_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1847_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTSDiv___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
lean_object* lean_nat_lxor(lean_object*, lean_object*);
uint8_t l_Nat_testBit(lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__7;
static lean_object* l_Std_BitVec_reduceLT___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__2;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2013_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAppend___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__3;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2966_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__14;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1970_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__5;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMod___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceShiftLeftZeroExtend___closed__1;
static lean_object* l_Std_BitVec_reduceSRem___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
lean_object* l_Std_BitVec_neg(lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceGetBit___lambda__1___closed__9;
static lean_object* l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__6;
static lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__7;
static lean_object* l_Std_BitVec_reduceAppend___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__8;
lean_object* l_Lean_Meta_Simp_registerBuiltinSimproc(lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__6;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1641_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__6;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1769_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAllOnes___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
uint8_t lean_nat_dec_eq(lean_object*, lean_object*);
static lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__3;
static lean_object* l_Std_BitVec_reduceNeg___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__4;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989_(lean_object*);
static lean_object* l_Std_BitVec_reduceBin___lambda__2___closed__9;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUShiftRight___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__8;
uint8_t lean_nat_dec_lt(lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__3;
lean_object* lean_nat_mod(lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToInt___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceOr___closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__6;
static lean_object* l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__7;
static lean_object* l_Std_BitVec_reduceSignExtend___closed__2;
static lean_object* l_Std_BitVec_reduceSRem___closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMod___lambda__2(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetLsb___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2725_(lean_object*);
static lean_object* l_Std_BitVec_reduceMod___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__5;
static lean_object* l_Std_BitVec_reduceULE___closed__2;
lean_object* l_Lean_Name_mkStr2(lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceZeroExtend___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__5;
LEAN_EXPORT lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3077_(lean_object*);
LEAN_EXPORT lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__12;
lean_object* l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(lean_object*, lean_object*, uint8_t, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__2;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1871_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__1;
static lean_object* l_Std_BitVec_reduceZeroExtend___closed__1;
lean_object* l_Std_BitVec_smod(lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceCast___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__5;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2615_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__5;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765_(lean_object*);
static lean_object* l_Std_BitVec_reduceGE___closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__13;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2141_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTSDiv(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__8;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMod___lambda__2(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceLT___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__5;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1433_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceOfInt___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1925_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1993_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3873_(lean_object*);
lean_object* lean_nat_shiftl(lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__14;
lean_object* l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__5;
lean_object* lean_nat_sub(lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceSignExtend___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__13;
static lean_object* l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__14;
lean_object* lean_nat_mul(lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3675_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__4;
static lean_object* l_Std_BitVec_reduceSShiftRight___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__7;
static lean_object* l_Std_BitVec_reduceBin___lambda__2___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__4;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3854_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__11;
static lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__2;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3323_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__2;
static lean_object* l_Std_BitVec_reduceSMTSDiv___closed__1;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1517_(lean_object*);
static lean_object* l_Std_BitVec_reduceULT___closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAnd___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__7;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTUDiv(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__9;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2057_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMul___lambda__2(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__9;
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__7;
static lean_object* l_Std_BitVec_reduceCast___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__7;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSRem___lambda__2(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__13;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceRotateLeft(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
lean_object* l_Std_BitVec_sub(lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3009_(lean_object*);
static lean_object* l_Std_BitVec_reduceReplicate___closed__2;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__14;
static lean_object* l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__9;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1643_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__7;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAllOnes___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceZeroExtend_x27___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSDiv___lambda__2(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTSDiv___lambda__2(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSRem(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__3;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceCast(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceOr___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__2;
lean_object* l_Std_BitVec_mul(lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__8;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBin___lambda__2(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__4;
static lean_object* l_Std_BitVec_reduceGetBit___lambda__1___closed__10;
static lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetBit___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3325_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__13;
static lean_object* l_Std_BitVec_reduceRotateLeft___closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceReplicate___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceZeroExtend_x27(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__6;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceOfInt(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceHShiftRight(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceLE___closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__4;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__1;
static lean_object* l_Std_BitVec_reduceSMTUDiv___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__10;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSignExtend___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceBin___lambda__2___closed__15;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceXOr___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceHShiftLeft(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__10;
static lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__12;
static lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__2;
static lean_object* l_Std_BitVec_reduceSLE___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__1;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetBit___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
lean_object* l_Nat_fromExpr_x3f(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceDiv___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceOfInt___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceOr(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__3;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGE___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__6;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGT___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceNeg___closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__3;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAbs___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__9;
static lean_object* l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__6;
LEAN_EXPORT lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromBitVecExpr_x3f(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__3;
lean_object* l_Std_BitVec_replicate(lean_object*, lean_object*, lean_object*);
lean_object* l_Std_BitVec_shiftLeft(lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2723_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__9;
static lean_object* l_Std_BitVec_reduceAbs___closed__1;
lean_object* l_Std_BitVec_rotateRight(lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAbs___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetLsb___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l_Std_BitVec_reduceGetBit___lambda__1___closed__3;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1559_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__9;
static lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__5;
static lean_object* l_Std_BitVec_reduceToInt___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__8;
extern lean_object* l_Lean_Meta_Simp_builtinSEvalprocsRef;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSShiftRight___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
uint8_t lean_nat_dec_le(lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3833_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceXOr___lambda__2(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__4;
static lean_object* l_Std_BitVec_reduceULE___closed__1;
static lean_object* l_Std_BitVec_reduceGE___closed__2;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceZeroExtend___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__2;
static lean_object* l_Std_BitVec_reduceBin___lambda__2___closed__12;
static lean_object* l_Std_BitVec_reduceBin___lambda__2___closed__10;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGE___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__2;
lean_object* lean_nat_add(lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__8;
static lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__4;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055_(lean_object*);
static lean_object* l_Std_BitVec_reduceHShiftRight___closed__3;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSLT___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGT___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__2;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429_(lean_object*);
static lean_object* l_Std_BitVec_Literal_toExpr___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__5;
uint8_t l_Lean_Exception_isRuntime(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__1;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1923_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__2;
static lean_object* l_Std_BitVec_reduceSLT___closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__12;
static lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__14;
static lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__6;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1845_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__5;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNot(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
lean_object* l_Std_BitVec_smtUDiv(lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__4;
static lean_object* l_Std_BitVec_reduceMul___closed__2;
static lean_object* l_Std_BitVec_reduceGT___closed__1;
LEAN_EXPORT lean_object* l_Std_BitVec_Literal_toExpr(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__2;
lean_object* l_Lean_MessageData_ofName(lean_object*);
static lean_object* l_Std_BitVec_reduceNeg___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__1;
static lean_object* l_Std_BitVec_reduceAnd___closed__3;
static lean_object* l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__6;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTUDiv___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceExtend___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceOr___lambda__2(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceExtracLsb_x27___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__4;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1821_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__10;
lean_object* lean_nat_lor(lean_object*, lean_object*);
static lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__7;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceExtracLsb_x27(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__7;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceLE___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__2;
lean_object* l_Nat_repr(lean_object*);
static lean_object* l_Std_BitVec_reduceBin___lambda__2___closed__6;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSShiftRight(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAdd___lambda__1___boxed(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__8;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2037_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__5;
static lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__5;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921_(lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2099_(lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__5;
static lean_object* l_Std_BitVec_reduceAppend___closed__3;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSDiv___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3875_(lean_object*);
static lean_object* l_Std_BitVec_reduceAbs___closed__2;
static lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
static lean_object* l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__2;
static lean_object* l_Std_BitVec_reduceXOr___closed__2;
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142_(lean_object*);
LEAN_EXPORT lean_object* l_Std_BitVec_reduceULT(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__7;
static lean_object* l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__1;
static lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__6;
LEAN_EXPORT lean_object* l_Std_BitVec_reduceReplicate___lambda__1(lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*, lean_object*);
static lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__3;
static lean_object* _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("OfNat", 5);
return x_1;
}
}
static lean_object* _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("ofNat", 5);
return x_1;
}
}
static lean_object* _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__1;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("Std", 3);
return x_1;
}
}
static lean_object* _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("BitVec", 6);
return x_1;
}
}
static lean_object* _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = lean_box(0);
x_2 = lean_alloc_ctor(1, 1, 0);
lean_ctor_set(x_2, 0, x_1);
return x_2;
}
}
LEAN_EXPORT lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; lean_object* x_13; lean_object* x_14; 
x_10 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__3;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_117; 
x_117 = lean_box(0);
x_13 = x_117;
x_14 = x_9;
goto block_116;
}
else
{
lean_object* x_118; 
x_118 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__7;
x_13 = x_118;
x_14 = x_9;
goto block_116;
}
block_116:
{
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_15; lean_object* x_16; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_1);
x_15 = lean_box(0);
x_16 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_16, 0, x_15);
lean_ctor_set(x_16, 1, x_14);
return x_16;
}
else
{
lean_object* x_17; lean_object* x_18; lean_object* x_19; lean_object* x_20; 
lean_dec(x_13);
x_17 = l_Lean_Expr_appFn_x21(x_1);
lean_inc(x_17);
x_18 = l_Lean_Expr_appFn_x21(x_17);
x_19 = l_Lean_Expr_appArg_x21(x_18);
lean_dec(x_18);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
lean_inc(x_5);
x_20 = lean_whnf(x_19, x_5, x_6, x_7, x_8, x_14);
if (lean_obj_tag(x_20) == 0)
{
uint8_t x_21; 
x_21 = !lean_is_exclusive(x_20);
if (x_21 == 0)
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; uint8_t x_26; 
x_22 = lean_ctor_get(x_20, 0);
x_23 = lean_ctor_get(x_20, 1);
x_24 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__6;
x_25 = lean_unsigned_to_nat(1u);
x_26 = l_Lean_Expr_isAppOfArity(x_22, x_24, x_25);
if (x_26 == 0)
{
lean_object* x_27; 
lean_dec(x_22);
lean_dec(x_17);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
x_27 = lean_box(0);
lean_ctor_set(x_20, 0, x_27);
return x_20;
}
else
{
lean_object* x_28; lean_object* x_29; 
lean_free_object(x_20);
x_28 = l_Lean_Expr_appArg_x21(x_22);
lean_dec(x_22);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_29 = l_Nat_fromExpr_x3f(x_28, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_23);
if (lean_obj_tag(x_29) == 0)
{
lean_object* x_30; 
x_30 = lean_ctor_get(x_29, 0);
lean_inc(x_30);
if (lean_obj_tag(x_30) == 0)
{
uint8_t x_31; 
lean_dec(x_17);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
x_31 = !lean_is_exclusive(x_29);
if (x_31 == 0)
{
lean_object* x_32; lean_object* x_33; 
x_32 = lean_ctor_get(x_29, 0);
lean_dec(x_32);
x_33 = lean_box(0);
lean_ctor_set(x_29, 0, x_33);
return x_29;
}
else
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; 
x_34 = lean_ctor_get(x_29, 1);
lean_inc(x_34);
lean_dec(x_29);
x_35 = lean_box(0);
x_36 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_36, 0, x_35);
lean_ctor_set(x_36, 1, x_34);
return x_36;
}
}
else
{
lean_object* x_37; lean_object* x_38; lean_object* x_39; lean_object* x_40; 
x_37 = lean_ctor_get(x_29, 1);
lean_inc(x_37);
lean_dec(x_29);
x_38 = lean_ctor_get(x_30, 0);
lean_inc(x_38);
lean_dec(x_30);
x_39 = l_Lean_Expr_appArg_x21(x_17);
lean_dec(x_17);
x_40 = l_Nat_fromExpr_x3f(x_39, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_37);
lean_dec(x_5);
if (lean_obj_tag(x_40) == 0)
{
lean_object* x_41; 
x_41 = lean_ctor_get(x_40, 0);
lean_inc(x_41);
if (lean_obj_tag(x_41) == 0)
{
uint8_t x_42; 
lean_dec(x_38);
x_42 = !lean_is_exclusive(x_40);
if (x_42 == 0)
{
lean_object* x_43; lean_object* x_44; 
x_43 = lean_ctor_get(x_40, 0);
lean_dec(x_43);
x_44 = lean_box(0);
lean_ctor_set(x_40, 0, x_44);
return x_40;
}
else
{
lean_object* x_45; lean_object* x_46; lean_object* x_47; 
x_45 = lean_ctor_get(x_40, 1);
lean_inc(x_45);
lean_dec(x_40);
x_46 = lean_box(0);
x_47 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_47, 0, x_46);
lean_ctor_set(x_47, 1, x_45);
return x_47;
}
}
else
{
uint8_t x_48; 
x_48 = !lean_is_exclusive(x_40);
if (x_48 == 0)
{
lean_object* x_49; uint8_t x_50; 
x_49 = lean_ctor_get(x_40, 0);
lean_dec(x_49);
x_50 = !lean_is_exclusive(x_41);
if (x_50 == 0)
{
lean_object* x_51; lean_object* x_52; lean_object* x_53; 
x_51 = lean_ctor_get(x_41, 0);
x_52 = l_Std_BitVec_ofNat(x_38, x_51);
lean_dec(x_51);
x_53 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_53, 0, x_38);
lean_ctor_set(x_53, 1, x_52);
lean_ctor_set(x_41, 0, x_53);
return x_40;
}
else
{
lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; 
x_54 = lean_ctor_get(x_41, 0);
lean_inc(x_54);
lean_dec(x_41);
x_55 = l_Std_BitVec_ofNat(x_38, x_54);
lean_dec(x_54);
x_56 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_56, 0, x_38);
lean_ctor_set(x_56, 1, x_55);
x_57 = lean_alloc_ctor(1, 1, 0);
lean_ctor_set(x_57, 0, x_56);
lean_ctor_set(x_40, 0, x_57);
return x_40;
}
}
else
{
lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; 
x_58 = lean_ctor_get(x_40, 1);
lean_inc(x_58);
lean_dec(x_40);
x_59 = lean_ctor_get(x_41, 0);
lean_inc(x_59);
if (lean_is_exclusive(x_41)) {
 lean_ctor_release(x_41, 0);
 x_60 = x_41;
} else {
 lean_dec_ref(x_41);
 x_60 = lean_box(0);
}
x_61 = l_Std_BitVec_ofNat(x_38, x_59);
lean_dec(x_59);
x_62 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_62, 0, x_38);
lean_ctor_set(x_62, 1, x_61);
if (lean_is_scalar(x_60)) {
 x_63 = lean_alloc_ctor(1, 1, 0);
} else {
 x_63 = x_60;
}
lean_ctor_set(x_63, 0, x_62);
x_64 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_64, 0, x_63);
lean_ctor_set(x_64, 1, x_58);
return x_64;
}
}
}
else
{
uint8_t x_65; 
lean_dec(x_38);
x_65 = !lean_is_exclusive(x_40);
if (x_65 == 0)
{
return x_40;
}
else
{
lean_object* x_66; lean_object* x_67; lean_object* x_68; 
x_66 = lean_ctor_get(x_40, 0);
x_67 = lean_ctor_get(x_40, 1);
lean_inc(x_67);
lean_inc(x_66);
lean_dec(x_40);
x_68 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_68, 0, x_66);
lean_ctor_set(x_68, 1, x_67);
return x_68;
}
}
}
}
else
{
uint8_t x_69; 
lean_dec(x_17);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
x_69 = !lean_is_exclusive(x_29);
if (x_69 == 0)
{
return x_29;
}
else
{
lean_object* x_70; lean_object* x_71; lean_object* x_72; 
x_70 = lean_ctor_get(x_29, 0);
x_71 = lean_ctor_get(x_29, 1);
lean_inc(x_71);
lean_inc(x_70);
lean_dec(x_29);
x_72 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_72, 0, x_70);
lean_ctor_set(x_72, 1, x_71);
return x_72;
}
}
}
}
else
{
lean_object* x_73; lean_object* x_74; lean_object* x_75; lean_object* x_76; uint8_t x_77; 
x_73 = lean_ctor_get(x_20, 0);
x_74 = lean_ctor_get(x_20, 1);
lean_inc(x_74);
lean_inc(x_73);
lean_dec(x_20);
x_75 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__6;
x_76 = lean_unsigned_to_nat(1u);
x_77 = l_Lean_Expr_isAppOfArity(x_73, x_75, x_76);
if (x_77 == 0)
{
lean_object* x_78; lean_object* x_79; 
lean_dec(x_73);
lean_dec(x_17);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
x_78 = lean_box(0);
x_79 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_79, 0, x_78);
lean_ctor_set(x_79, 1, x_74);
return x_79;
}
else
{
lean_object* x_80; lean_object* x_81; 
x_80 = l_Lean_Expr_appArg_x21(x_73);
lean_dec(x_73);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_81 = l_Nat_fromExpr_x3f(x_80, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_74);
if (lean_obj_tag(x_81) == 0)
{
lean_object* x_82; 
x_82 = lean_ctor_get(x_81, 0);
lean_inc(x_82);
if (lean_obj_tag(x_82) == 0)
{
lean_object* x_83; lean_object* x_84; lean_object* x_85; lean_object* x_86; 
lean_dec(x_17);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
x_83 = lean_ctor_get(x_81, 1);
lean_inc(x_83);
if (lean_is_exclusive(x_81)) {
 lean_ctor_release(x_81, 0);
 lean_ctor_release(x_81, 1);
 x_84 = x_81;
} else {
 lean_dec_ref(x_81);
 x_84 = lean_box(0);
}
x_85 = lean_box(0);
if (lean_is_scalar(x_84)) {
 x_86 = lean_alloc_ctor(0, 2, 0);
} else {
 x_86 = x_84;
}
lean_ctor_set(x_86, 0, x_85);
lean_ctor_set(x_86, 1, x_83);
return x_86;
}
else
{
lean_object* x_87; lean_object* x_88; lean_object* x_89; lean_object* x_90; 
x_87 = lean_ctor_get(x_81, 1);
lean_inc(x_87);
lean_dec(x_81);
x_88 = lean_ctor_get(x_82, 0);
lean_inc(x_88);
lean_dec(x_82);
x_89 = l_Lean_Expr_appArg_x21(x_17);
lean_dec(x_17);
x_90 = l_Nat_fromExpr_x3f(x_89, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_87);
lean_dec(x_5);
if (lean_obj_tag(x_90) == 0)
{
lean_object* x_91; 
x_91 = lean_ctor_get(x_90, 0);
lean_inc(x_91);
if (lean_obj_tag(x_91) == 0)
{
lean_object* x_92; lean_object* x_93; lean_object* x_94; lean_object* x_95; 
lean_dec(x_88);
x_92 = lean_ctor_get(x_90, 1);
lean_inc(x_92);
if (lean_is_exclusive(x_90)) {
 lean_ctor_release(x_90, 0);
 lean_ctor_release(x_90, 1);
 x_93 = x_90;
} else {
 lean_dec_ref(x_90);
 x_93 = lean_box(0);
}
x_94 = lean_box(0);
if (lean_is_scalar(x_93)) {
 x_95 = lean_alloc_ctor(0, 2, 0);
} else {
 x_95 = x_93;
}
lean_ctor_set(x_95, 0, x_94);
lean_ctor_set(x_95, 1, x_92);
return x_95;
}
else
{
lean_object* x_96; lean_object* x_97; lean_object* x_98; lean_object* x_99; lean_object* x_100; lean_object* x_101; lean_object* x_102; lean_object* x_103; 
x_96 = lean_ctor_get(x_90, 1);
lean_inc(x_96);
if (lean_is_exclusive(x_90)) {
 lean_ctor_release(x_90, 0);
 lean_ctor_release(x_90, 1);
 x_97 = x_90;
} else {
 lean_dec_ref(x_90);
 x_97 = lean_box(0);
}
x_98 = lean_ctor_get(x_91, 0);
lean_inc(x_98);
if (lean_is_exclusive(x_91)) {
 lean_ctor_release(x_91, 0);
 x_99 = x_91;
} else {
 lean_dec_ref(x_91);
 x_99 = lean_box(0);
}
x_100 = l_Std_BitVec_ofNat(x_88, x_98);
lean_dec(x_98);
x_101 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_101, 0, x_88);
lean_ctor_set(x_101, 1, x_100);
if (lean_is_scalar(x_99)) {
 x_102 = lean_alloc_ctor(1, 1, 0);
} else {
 x_102 = x_99;
}
lean_ctor_set(x_102, 0, x_101);
if (lean_is_scalar(x_97)) {
 x_103 = lean_alloc_ctor(0, 2, 0);
} else {
 x_103 = x_97;
}
lean_ctor_set(x_103, 0, x_102);
lean_ctor_set(x_103, 1, x_96);
return x_103;
}
}
else
{
lean_object* x_104; lean_object* x_105; lean_object* x_106; lean_object* x_107; 
lean_dec(x_88);
x_104 = lean_ctor_get(x_90, 0);
lean_inc(x_104);
x_105 = lean_ctor_get(x_90, 1);
lean_inc(x_105);
if (lean_is_exclusive(x_90)) {
 lean_ctor_release(x_90, 0);
 lean_ctor_release(x_90, 1);
 x_106 = x_90;
} else {
 lean_dec_ref(x_90);
 x_106 = lean_box(0);
}
if (lean_is_scalar(x_106)) {
 x_107 = lean_alloc_ctor(1, 2, 0);
} else {
 x_107 = x_106;
}
lean_ctor_set(x_107, 0, x_104);
lean_ctor_set(x_107, 1, x_105);
return x_107;
}
}
}
else
{
lean_object* x_108; lean_object* x_109; lean_object* x_110; lean_object* x_111; 
lean_dec(x_17);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
x_108 = lean_ctor_get(x_81, 0);
lean_inc(x_108);
x_109 = lean_ctor_get(x_81, 1);
lean_inc(x_109);
if (lean_is_exclusive(x_81)) {
 lean_ctor_release(x_81, 0);
 lean_ctor_release(x_81, 1);
 x_110 = x_81;
} else {
 lean_dec_ref(x_81);
 x_110 = lean_box(0);
}
if (lean_is_scalar(x_110)) {
 x_111 = lean_alloc_ctor(1, 2, 0);
} else {
 x_111 = x_110;
}
lean_ctor_set(x_111, 0, x_108);
lean_ctor_set(x_111, 1, x_109);
return x_111;
}
}
}
}
else
{
uint8_t x_112; 
lean_dec(x_17);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
x_112 = !lean_is_exclusive(x_20);
if (x_112 == 0)
{
return x_20;
}
else
{
lean_object* x_113; lean_object* x_114; lean_object* x_115; 
x_113 = lean_ctor_get(x_20, 0);
x_114 = lean_ctor_get(x_20, 1);
lean_inc(x_114);
lean_inc(x_113);
lean_dec(x_20);
x_115 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_115, 0, x_113);
lean_ctor_set(x_115, 1, x_114);
return x_115;
}
}
}
}
}
}
LEAN_EXPORT lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; 
x_10 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_10;
}
}
static lean_object* _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromBitVecExpr_x3f___closed__1() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__2;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromBitVecExpr_x3f(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; lean_object* x_13; lean_object* x_14; 
x_10 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromBitVecExpr_x3f___closed__1;
x_11 = lean_unsigned_to_nat(2u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_64; 
x_64 = lean_box(0);
x_13 = x_64;
x_14 = x_9;
goto block_63;
}
else
{
lean_object* x_65; 
x_65 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__7;
x_13 = x_65;
x_14 = x_9;
goto block_63;
}
block_63:
{
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_15; lean_object* x_16; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = lean_box(0);
x_16 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_16, 0, x_15);
lean_ctor_set(x_16, 1, x_14);
return x_16;
}
else
{
lean_object* x_17; lean_object* x_18; lean_object* x_19; 
lean_dec(x_13);
lean_inc(x_1);
x_17 = l_Lean_Expr_appFn_x21(x_1);
x_18 = l_Lean_Expr_appArg_x21(x_17);
lean_dec(x_17);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_19 = l_Nat_fromExpr_x3f(x_18, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_14);
if (lean_obj_tag(x_19) == 0)
{
lean_object* x_20; 
x_20 = lean_ctor_get(x_19, 0);
lean_inc(x_20);
if (lean_obj_tag(x_20) == 0)
{
uint8_t x_21; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_21 = !lean_is_exclusive(x_19);
if (x_21 == 0)
{
lean_object* x_22; lean_object* x_23; 
x_22 = lean_ctor_get(x_19, 0);
lean_dec(x_22);
x_23 = lean_box(0);
lean_ctor_set(x_19, 0, x_23);
return x_19;
}
else
{
lean_object* x_24; lean_object* x_25; lean_object* x_26; 
x_24 = lean_ctor_get(x_19, 1);
lean_inc(x_24);
lean_dec(x_19);
x_25 = lean_box(0);
x_26 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_26, 0, x_25);
lean_ctor_set(x_26, 1, x_24);
return x_26;
}
}
else
{
lean_object* x_27; lean_object* x_28; lean_object* x_29; lean_object* x_30; 
x_27 = lean_ctor_get(x_19, 1);
lean_inc(x_27);
lean_dec(x_19);
x_28 = lean_ctor_get(x_20, 0);
lean_inc(x_28);
lean_dec(x_20);
x_29 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_30 = l_Nat_fromExpr_x3f(x_29, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_27);
if (lean_obj_tag(x_30) == 0)
{
lean_object* x_31; 
x_31 = lean_ctor_get(x_30, 0);
lean_inc(x_31);
if (lean_obj_tag(x_31) == 0)
{
uint8_t x_32; 
lean_dec(x_28);
x_32 = !lean_is_exclusive(x_30);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; 
x_33 = lean_ctor_get(x_30, 0);
lean_dec(x_33);
x_34 = lean_box(0);
lean_ctor_set(x_30, 0, x_34);
return x_30;
}
else
{
lean_object* x_35; lean_object* x_36; lean_object* x_37; 
x_35 = lean_ctor_get(x_30, 1);
lean_inc(x_35);
lean_dec(x_30);
x_36 = lean_box(0);
x_37 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_37, 0, x_36);
lean_ctor_set(x_37, 1, x_35);
return x_37;
}
}
else
{
uint8_t x_38; 
x_38 = !lean_is_exclusive(x_30);
if (x_38 == 0)
{
lean_object* x_39; uint8_t x_40; 
x_39 = lean_ctor_get(x_30, 0);
lean_dec(x_39);
x_40 = !lean_is_exclusive(x_31);
if (x_40 == 0)
{
lean_object* x_41; lean_object* x_42; lean_object* x_43; 
x_41 = lean_ctor_get(x_31, 0);
x_42 = l_Std_BitVec_ofNat(x_28, x_41);
lean_dec(x_41);
x_43 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_43, 0, x_28);
lean_ctor_set(x_43, 1, x_42);
lean_ctor_set(x_31, 0, x_43);
return x_30;
}
else
{
lean_object* x_44; lean_object* x_45; lean_object* x_46; lean_object* x_47; 
x_44 = lean_ctor_get(x_31, 0);
lean_inc(x_44);
lean_dec(x_31);
x_45 = l_Std_BitVec_ofNat(x_28, x_44);
lean_dec(x_44);
x_46 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_46, 0, x_28);
lean_ctor_set(x_46, 1, x_45);
x_47 = lean_alloc_ctor(1, 1, 0);
lean_ctor_set(x_47, 0, x_46);
lean_ctor_set(x_30, 0, x_47);
return x_30;
}
}
else
{
lean_object* x_48; lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; 
x_48 = lean_ctor_get(x_30, 1);
lean_inc(x_48);
lean_dec(x_30);
x_49 = lean_ctor_get(x_31, 0);
lean_inc(x_49);
if (lean_is_exclusive(x_31)) {
 lean_ctor_release(x_31, 0);
 x_50 = x_31;
} else {
 lean_dec_ref(x_31);
 x_50 = lean_box(0);
}
x_51 = l_Std_BitVec_ofNat(x_28, x_49);
lean_dec(x_49);
x_52 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_52, 0, x_28);
lean_ctor_set(x_52, 1, x_51);
if (lean_is_scalar(x_50)) {
 x_53 = lean_alloc_ctor(1, 1, 0);
} else {
 x_53 = x_50;
}
lean_ctor_set(x_53, 0, x_52);
x_54 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_54, 0, x_53);
lean_ctor_set(x_54, 1, x_48);
return x_54;
}
}
}
else
{
uint8_t x_55; 
lean_dec(x_28);
x_55 = !lean_is_exclusive(x_30);
if (x_55 == 0)
{
return x_30;
}
else
{
lean_object* x_56; lean_object* x_57; lean_object* x_58; 
x_56 = lean_ctor_get(x_30, 0);
x_57 = lean_ctor_get(x_30, 1);
lean_inc(x_57);
lean_inc(x_56);
lean_dec(x_30);
x_58 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_58, 0, x_56);
lean_ctor_set(x_58, 1, x_57);
return x_58;
}
}
}
}
else
{
uint8_t x_59; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_59 = !lean_is_exclusive(x_19);
if (x_59 == 0)
{
return x_19;
}
else
{
lean_object* x_60; lean_object* x_61; lean_object* x_62; 
x_60 = lean_ctor_get(x_19, 0);
x_61 = lean_ctor_get(x_19, 1);
lean_inc(x_61);
lean_inc(x_60);
lean_dec(x_19);
x_62 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_62, 0, x_60);
lean_ctor_set(x_62, 1, x_61);
return x_62;
}
}
}
}
}
}
LEAN_EXPORT lean_object* l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromBitVecExpr_x3f___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; 
x_10 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromBitVecExpr_x3f(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_10;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_fromExpr_x3f(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; lean_object* x_12; lean_object* x_13; 
x_10 = l_Lean_Meta_saveState___rarg(x_6, x_7, x_8, x_9);
x_11 = lean_ctor_get(x_10, 0);
lean_inc(x_11);
x_12 = lean_ctor_get(x_10, 1);
lean_inc(x_12);
lean_dec(x_10);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
lean_inc(x_1);
x_13 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromBitVecExpr_x3f(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_12);
if (lean_obj_tag(x_13) == 0)
{
lean_dec(x_11);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_1);
return x_13;
}
else
{
uint8_t x_14; 
x_14 = !lean_is_exclusive(x_13);
if (x_14 == 0)
{
lean_object* x_15; lean_object* x_16; uint8_t x_17; 
x_15 = lean_ctor_get(x_13, 0);
x_16 = lean_ctor_get(x_13, 1);
x_17 = l_Lean_Exception_isRuntime(x_15);
if (x_17 == 0)
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
lean_free_object(x_13);
lean_dec(x_15);
x_18 = l_Lean_Meta_SavedState_restore(x_11, x_5, x_6, x_7, x_8, x_16);
lean_dec(x_11);
x_19 = lean_ctor_get(x_18, 1);
lean_inc(x_19);
lean_dec(x_18);
x_20 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_19);
return x_20;
}
else
{
uint8_t x_21; 
x_21 = lean_ctor_get_uint8(x_7, sizeof(void*)*11);
if (x_21 == 0)
{
lean_dec(x_11);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_1);
return x_13;
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; 
lean_free_object(x_13);
lean_dec(x_15);
x_22 = l_Lean_Meta_SavedState_restore(x_11, x_5, x_6, x_7, x_8, x_16);
lean_dec(x_11);
x_23 = lean_ctor_get(x_22, 1);
lean_inc(x_23);
lean_dec(x_22);
x_24 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_23);
return x_24;
}
}
}
else
{
lean_object* x_25; lean_object* x_26; uint8_t x_27; 
x_25 = lean_ctor_get(x_13, 0);
x_26 = lean_ctor_get(x_13, 1);
lean_inc(x_26);
lean_inc(x_25);
lean_dec(x_13);
x_27 = l_Lean_Exception_isRuntime(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; lean_object* x_30; 
lean_dec(x_25);
x_28 = l_Lean_Meta_SavedState_restore(x_11, x_5, x_6, x_7, x_8, x_26);
lean_dec(x_11);
x_29 = lean_ctor_get(x_28, 1);
lean_inc(x_29);
lean_dec(x_28);
x_30 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_29);
return x_30;
}
else
{
uint8_t x_31; 
x_31 = lean_ctor_get_uint8(x_7, sizeof(void*)*11);
if (x_31 == 0)
{
lean_object* x_32; 
lean_dec(x_11);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_1);
x_32 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_32, 0, x_25);
lean_ctor_set(x_32, 1, x_26);
return x_32;
}
else
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; 
lean_dec(x_25);
x_33 = l_Lean_Meta_SavedState_restore(x_11, x_5, x_6, x_7, x_8, x_26);
lean_dec(x_11);
x_34 = lean_ctor_get(x_33, 1);
lean_inc(x_34);
lean_dec(x_33);
x_35 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_34);
return x_35;
}
}
}
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_fromExpr_x3f___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; 
x_10 = l_Std_BitVec_fromExpr_x3f(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_10;
}
}
static lean_object* _init_l_Std_BitVec_Literal_toExpr___closed__1() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = lean_box(0);
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromBitVecExpr_x3f___closed__1;
x_3 = l_Lean_Expr_const___override(x_2, x_1);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_Literal_toExpr(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; lean_object* x_6; lean_object* x_7; 
x_2 = lean_ctor_get(x_1, 0);
lean_inc(x_2);
x_3 = l_Lean_mkNatLit(x_2);
x_4 = lean_ctor_get(x_1, 1);
lean_inc(x_4);
lean_dec(x_1);
x_5 = l_Lean_mkNatLit(x_4);
x_6 = l_Std_BitVec_Literal_toExpr___closed__1;
x_7 = l_Lean_mkAppB(x_6, x_3, x_5);
return x_7;
}
}
static lean_object* _init_l_Std_BitVec_reduceUnary___lambda__1___closed__1() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = lean_box(0);
x_2 = lean_alloc_ctor(2, 1, 0);
lean_ctor_set(x_2, 0, x_1);
return x_2;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUnary___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; 
x_12 = l_Lean_Expr_appArg_x21(x_1);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_2);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; uint8_t x_22; 
x_21 = lean_ctor_get(x_14, 0);
lean_inc(x_21);
lean_dec(x_14);
x_22 = !lean_is_exclusive(x_13);
if (x_22 == 0)
{
lean_object* x_23; uint8_t x_24; 
x_23 = lean_ctor_get(x_13, 0);
lean_dec(x_23);
x_24 = !lean_is_exclusive(x_21);
if (x_24 == 0)
{
lean_object* x_25; lean_object* x_26; lean_object* x_27; lean_object* x_28; lean_object* x_29; uint32_t x_30; uint8_t x_31; lean_object* x_32; lean_object* x_33; 
x_25 = lean_ctor_get(x_21, 0);
x_26 = lean_ctor_get(x_21, 1);
lean_inc(x_25);
x_27 = lean_apply_2(x_2, x_25, x_26);
lean_ctor_set(x_21, 1, x_27);
x_28 = l_Std_BitVec_Literal_toExpr(x_21);
x_29 = lean_box(0);
x_30 = 0;
x_31 = 1;
x_32 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_32, 0, x_28);
lean_ctor_set(x_32, 1, x_29);
lean_ctor_set_uint32(x_32, sizeof(void*)*2, x_30);
lean_ctor_set_uint8(x_32, sizeof(void*)*2 + 4, x_31);
x_33 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_33, 0, x_32);
lean_ctor_set(x_13, 0, x_33);
return x_13;
}
else
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint32_t x_40; uint8_t x_41; lean_object* x_42; lean_object* x_43; 
x_34 = lean_ctor_get(x_21, 0);
x_35 = lean_ctor_get(x_21, 1);
lean_inc(x_35);
lean_inc(x_34);
lean_dec(x_21);
lean_inc(x_34);
x_36 = lean_apply_2(x_2, x_34, x_35);
x_37 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_37, 0, x_34);
lean_ctor_set(x_37, 1, x_36);
x_38 = l_Std_BitVec_Literal_toExpr(x_37);
x_39 = lean_box(0);
x_40 = 0;
x_41 = 1;
x_42 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_42, 0, x_38);
lean_ctor_set(x_42, 1, x_39);
lean_ctor_set_uint32(x_42, sizeof(void*)*2, x_40);
lean_ctor_set_uint8(x_42, sizeof(void*)*2 + 4, x_41);
x_43 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_43, 0, x_42);
lean_ctor_set(x_13, 0, x_43);
return x_13;
}
}
else
{
lean_object* x_44; lean_object* x_45; lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; lean_object* x_51; uint32_t x_52; uint8_t x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; 
x_44 = lean_ctor_get(x_13, 1);
lean_inc(x_44);
lean_dec(x_13);
x_45 = lean_ctor_get(x_21, 0);
lean_inc(x_45);
x_46 = lean_ctor_get(x_21, 1);
lean_inc(x_46);
if (lean_is_exclusive(x_21)) {
 lean_ctor_release(x_21, 0);
 lean_ctor_release(x_21, 1);
 x_47 = x_21;
} else {
 lean_dec_ref(x_21);
 x_47 = lean_box(0);
}
lean_inc(x_45);
x_48 = lean_apply_2(x_2, x_45, x_46);
if (lean_is_scalar(x_47)) {
 x_49 = lean_alloc_ctor(0, 2, 0);
} else {
 x_49 = x_47;
}
lean_ctor_set(x_49, 0, x_45);
lean_ctor_set(x_49, 1, x_48);
x_50 = l_Std_BitVec_Literal_toExpr(x_49);
x_51 = lean_box(0);
x_52 = 0;
x_53 = 1;
x_54 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_54, 0, x_50);
lean_ctor_set(x_54, 1, x_51);
lean_ctor_set_uint32(x_54, sizeof(void*)*2, x_52);
lean_ctor_set_uint8(x_54, sizeof(void*)*2 + 4, x_53);
x_55 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_55, 0, x_54);
x_56 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_56, 0, x_55);
lean_ctor_set(x_56, 1, x_44);
return x_56;
}
}
}
else
{
uint8_t x_57; 
lean_dec(x_2);
x_57 = !lean_is_exclusive(x_13);
if (x_57 == 0)
{
return x_13;
}
else
{
lean_object* x_58; lean_object* x_59; lean_object* x_60; 
x_58 = lean_ctor_get(x_13, 0);
x_59 = lean_ctor_get(x_13, 1);
lean_inc(x_59);
lean_inc(x_58);
lean_dec(x_13);
x_60 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_60, 0, x_58);
lean_ctor_set(x_60, 1, x_59);
return x_60;
}
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUnary(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
uint8_t x_13; 
x_13 = l_Lean_Expr_isAppOfArity(x_4, x_1, x_2);
lean_dec(x_1);
if (x_13 == 0)
{
lean_object* x_14; lean_object* x_15; 
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
x_14 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_14);
lean_ctor_set(x_15, 1, x_12);
return x_15;
}
else
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_box(0);
x_17 = l_Std_BitVec_reduceUnary___lambda__1(x_4, x_3, x_16, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
return x_17;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUnary___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; 
x_12 = l_Std_BitVec_reduceUnary___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_1);
return x_12;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBin___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12, lean_object* x_13) {
_start:
{
lean_object* x_14; lean_object* x_15; lean_object* x_16; lean_object* x_17; lean_object* x_18; uint32_t x_19; uint8_t x_20; lean_object* x_21; lean_object* x_22; lean_object* x_23; 
x_14 = lean_ctor_get(x_1, 1);
lean_inc(x_14);
lean_dec(x_1);
lean_inc(x_3);
x_15 = lean_apply_3(x_2, x_3, x_4, x_14);
x_16 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_16, 0, x_3);
lean_ctor_set(x_16, 1, x_15);
x_17 = l_Std_BitVec_Literal_toExpr(x_16);
x_18 = lean_box(0);
x_19 = 0;
x_20 = 1;
x_21 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_21, 0, x_17);
lean_ctor_set(x_21, 1, x_18);
lean_ctor_set_uint32(x_21, sizeof(void*)*2, x_19);
lean_ctor_set_uint8(x_21, sizeof(void*)*2 + 4, x_20);
x_22 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_22, 0, x_21);
x_23 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_23, 0, x_22);
lean_ctor_set(x_23, 1, x_13);
return x_23;
}
}
static lean_object* _init_l_Std_BitVec_reduceBin___lambda__2___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("Meta", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceBin___lambda__2___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("debug", 5);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceBin___lambda__2___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceBin___lambda__2___closed__1;
x_2 = l_Std_BitVec_reduceBin___lambda__2___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l_Std_BitVec_reduceBin___lambda__2___closed__4() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduce [", 8);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceBin___lambda__2___closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = l_Std_BitVec_reduceBin___lambda__2___closed__4;
x_2 = l_Lean_stringToMessageData(x_1);
return x_2;
}
}
static lean_object* _init_l_Std_BitVec_reduceBin___lambda__2___closed__6() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("] ", 2);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceBin___lambda__2___closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = l_Std_BitVec_reduceBin___lambda__2___closed__6;
x_2 = l_Lean_stringToMessageData(x_1);
return x_2;
}
}
static lean_object* _init_l_Std_BitVec_reduceBin___lambda__2___closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("0x", 2);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceBin___lambda__2___closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = l_Std_BitVec_reduceBin___lambda__2___closed__8;
x_2 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_2, 0, x_1);
return x_2;
}
}
static lean_object* _init_l_Std_BitVec_reduceBin___lambda__2___closed__10() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("#", 1);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceBin___lambda__2___closed__11() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = l_Std_BitVec_reduceBin___lambda__2___closed__10;
x_2 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_2, 0, x_1);
return x_2;
}
}
static lean_object* _init_l_Std_BitVec_reduceBin___lambda__2___closed__12() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes(", ", 2);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceBin___lambda__2___closed__13() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = l_Std_BitVec_reduceBin___lambda__2___closed__12;
x_2 = l_Lean_stringToMessageData(x_1);
return x_2;
}
}
static lean_object* _init_l_Std_BitVec_reduceBin___lambda__2___closed__14() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("", 0);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceBin___lambda__2___closed__15() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = l_Std_BitVec_reduceBin___lambda__2___closed__14;
x_2 = l_Lean_stringToMessageData(x_1);
return x_2;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBin___lambda__2(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; lean_object* x_14; lean_object* x_15; 
lean_dec(x_4);
lean_inc(x_1);
x_13 = l_Lean_Expr_appFn_x21(x_1);
x_14 = l_Lean_Expr_appArg_x21(x_13);
lean_dec(x_13);
lean_inc(x_11);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
x_15 = l_Std_BitVec_fromExpr_x3f(x_14, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
if (lean_obj_tag(x_15) == 0)
{
lean_object* x_16; 
x_16 = lean_ctor_get(x_15, 0);
lean_inc(x_16);
if (lean_obj_tag(x_16) == 0)
{
uint8_t x_17; 
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_17 = !lean_is_exclusive(x_15);
if (x_17 == 0)
{
lean_object* x_18; lean_object* x_19; 
x_18 = lean_ctor_get(x_15, 0);
lean_dec(x_18);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_15, 0, x_19);
return x_15;
}
else
{
lean_object* x_20; lean_object* x_21; lean_object* x_22; 
x_20 = lean_ctor_get(x_15, 1);
lean_inc(x_20);
lean_dec(x_15);
x_21 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_22 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_22, 0, x_21);
lean_ctor_set(x_22, 1, x_20);
return x_22;
}
}
else
{
lean_object* x_23; lean_object* x_24; lean_object* x_25; lean_object* x_26; 
x_23 = lean_ctor_get(x_15, 1);
lean_inc(x_23);
lean_dec(x_15);
x_24 = lean_ctor_get(x_16, 0);
lean_inc(x_24);
lean_dec(x_16);
x_25 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
lean_inc(x_11);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
x_26 = l_Std_BitVec_fromExpr_x3f(x_25, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_23);
if (lean_obj_tag(x_26) == 0)
{
lean_object* x_27; 
x_27 = lean_ctor_get(x_26, 0);
lean_inc(x_27);
if (lean_obj_tag(x_27) == 0)
{
uint8_t x_28; 
lean_dec(x_24);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_3);
lean_dec(x_2);
x_28 = !lean_is_exclusive(x_26);
if (x_28 == 0)
{
lean_object* x_29; lean_object* x_30; 
x_29 = lean_ctor_get(x_26, 0);
lean_dec(x_29);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_26, 0, x_30);
return x_26;
}
else
{
lean_object* x_31; lean_object* x_32; lean_object* x_33; 
x_31 = lean_ctor_get(x_26, 1);
lean_inc(x_31);
lean_dec(x_26);
x_32 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_33 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_33, 0, x_32);
lean_ctor_set(x_33, 1, x_31);
return x_33;
}
}
else
{
uint8_t x_34; 
x_34 = !lean_is_exclusive(x_26);
if (x_34 == 0)
{
lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; lean_object* x_40; uint8_t x_41; 
x_35 = lean_ctor_get(x_26, 1);
x_36 = lean_ctor_get(x_26, 0);
lean_dec(x_36);
x_37 = lean_ctor_get(x_27, 0);
lean_inc(x_37);
lean_dec(x_27);
x_38 = lean_ctor_get(x_24, 0);
lean_inc(x_38);
x_39 = lean_ctor_get(x_24, 1);
lean_inc(x_39);
lean_dec(x_24);
x_40 = lean_ctor_get(x_37, 0);
lean_inc(x_40);
x_41 = lean_nat_dec_eq(x_38, x_40);
if (x_41 == 0)
{
lean_object* x_42; 
lean_dec(x_40);
lean_dec(x_39);
lean_dec(x_38);
lean_dec(x_37);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_3);
lean_dec(x_2);
x_42 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_26, 0, x_42);
return x_26;
}
else
{
lean_object* x_43; lean_object* x_44; lean_object* x_45; uint8_t x_46; 
lean_free_object(x_26);
x_43 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_44 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_43, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_35);
x_45 = lean_ctor_get(x_44, 0);
lean_inc(x_45);
x_46 = lean_unbox(x_45);
lean_dec(x_45);
if (x_46 == 0)
{
lean_object* x_47; lean_object* x_48; lean_object* x_49; 
lean_dec(x_40);
lean_dec(x_3);
x_47 = lean_ctor_get(x_44, 1);
lean_inc(x_47);
lean_dec(x_44);
x_48 = lean_box(0);
x_49 = l_Std_BitVec_reduceBin___lambda__1(x_37, x_2, x_38, x_39, x_48, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_47);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
return x_49;
}
else
{
lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; lean_object* x_69; lean_object* x_70; lean_object* x_71; lean_object* x_72; lean_object* x_73; lean_object* x_74; lean_object* x_75; lean_object* x_76; lean_object* x_77; lean_object* x_78; lean_object* x_79; lean_object* x_80; lean_object* x_81; lean_object* x_82; lean_object* x_83; lean_object* x_84; lean_object* x_85; lean_object* x_86; lean_object* x_87; lean_object* x_88; lean_object* x_89; lean_object* x_90; 
x_50 = lean_ctor_get(x_44, 1);
lean_inc(x_50);
lean_dec(x_44);
x_51 = l_Lean_MessageData_ofName(x_3);
x_52 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_53 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_53, 0, x_52);
lean_ctor_set(x_53, 1, x_51);
x_54 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_55 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_55, 0, x_53);
lean_ctor_set(x_55, 1, x_54);
lean_inc(x_39);
x_56 = l_Std_BitVec_toHex(x_38, x_39);
x_57 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_57, 0, x_56);
x_58 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_59 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_59, 0, x_58);
lean_ctor_set(x_59, 1, x_57);
x_60 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_61 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_61, 0, x_59);
lean_ctor_set(x_61, 1, x_60);
lean_inc(x_38);
x_62 = l_Nat_repr(x_38);
x_63 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_63, 0, x_62);
x_64 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_64, 0, x_61);
lean_ctor_set(x_64, 1, x_63);
x_65 = l_Std_Format_defWidth;
x_66 = lean_unsigned_to_nat(0u);
x_67 = lean_format_pretty(x_64, x_65, x_66, x_66);
x_68 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_68, 0, x_67);
x_69 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_69, 0, x_68);
x_70 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_70, 0, x_55);
lean_ctor_set(x_70, 1, x_69);
x_71 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_72 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_72, 0, x_70);
lean_ctor_set(x_72, 1, x_71);
x_73 = lean_ctor_get(x_37, 1);
lean_inc(x_73);
x_74 = l_Std_BitVec_toHex(x_40, x_73);
x_75 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_75, 0, x_74);
x_76 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_76, 0, x_58);
lean_ctor_set(x_76, 1, x_75);
x_77 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_77, 0, x_76);
lean_ctor_set(x_77, 1, x_60);
x_78 = l_Nat_repr(x_40);
x_79 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_79, 0, x_78);
x_80 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_80, 0, x_77);
lean_ctor_set(x_80, 1, x_79);
x_81 = lean_format_pretty(x_80, x_65, x_66, x_66);
x_82 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_82, 0, x_81);
x_83 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_83, 0, x_82);
x_84 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_84, 0, x_72);
lean_ctor_set(x_84, 1, x_83);
x_85 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_86 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_86, 0, x_84);
lean_ctor_set(x_86, 1, x_85);
x_87 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_43, x_86, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_50);
x_88 = lean_ctor_get(x_87, 0);
lean_inc(x_88);
x_89 = lean_ctor_get(x_87, 1);
lean_inc(x_89);
lean_dec(x_87);
x_90 = l_Std_BitVec_reduceBin___lambda__1(x_37, x_2, x_38, x_39, x_88, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_89);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_88);
return x_90;
}
}
}
else
{
lean_object* x_91; lean_object* x_92; lean_object* x_93; lean_object* x_94; lean_object* x_95; uint8_t x_96; 
x_91 = lean_ctor_get(x_26, 1);
lean_inc(x_91);
lean_dec(x_26);
x_92 = lean_ctor_get(x_27, 0);
lean_inc(x_92);
lean_dec(x_27);
x_93 = lean_ctor_get(x_24, 0);
lean_inc(x_93);
x_94 = lean_ctor_get(x_24, 1);
lean_inc(x_94);
lean_dec(x_24);
x_95 = lean_ctor_get(x_92, 0);
lean_inc(x_95);
x_96 = lean_nat_dec_eq(x_93, x_95);
if (x_96 == 0)
{
lean_object* x_97; lean_object* x_98; 
lean_dec(x_95);
lean_dec(x_94);
lean_dec(x_93);
lean_dec(x_92);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_3);
lean_dec(x_2);
x_97 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_98 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_98, 0, x_97);
lean_ctor_set(x_98, 1, x_91);
return x_98;
}
else
{
lean_object* x_99; lean_object* x_100; lean_object* x_101; uint8_t x_102; 
x_99 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_100 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_99, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_91);
x_101 = lean_ctor_get(x_100, 0);
lean_inc(x_101);
x_102 = lean_unbox(x_101);
lean_dec(x_101);
if (x_102 == 0)
{
lean_object* x_103; lean_object* x_104; lean_object* x_105; 
lean_dec(x_95);
lean_dec(x_3);
x_103 = lean_ctor_get(x_100, 1);
lean_inc(x_103);
lean_dec(x_100);
x_104 = lean_box(0);
x_105 = l_Std_BitVec_reduceBin___lambda__1(x_92, x_2, x_93, x_94, x_104, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_103);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
return x_105;
}
else
{
lean_object* x_106; lean_object* x_107; lean_object* x_108; lean_object* x_109; lean_object* x_110; lean_object* x_111; lean_object* x_112; lean_object* x_113; lean_object* x_114; lean_object* x_115; lean_object* x_116; lean_object* x_117; lean_object* x_118; lean_object* x_119; lean_object* x_120; lean_object* x_121; lean_object* x_122; lean_object* x_123; lean_object* x_124; lean_object* x_125; lean_object* x_126; lean_object* x_127; lean_object* x_128; lean_object* x_129; lean_object* x_130; lean_object* x_131; lean_object* x_132; lean_object* x_133; lean_object* x_134; lean_object* x_135; lean_object* x_136; lean_object* x_137; lean_object* x_138; lean_object* x_139; lean_object* x_140; lean_object* x_141; lean_object* x_142; lean_object* x_143; lean_object* x_144; lean_object* x_145; lean_object* x_146; 
x_106 = lean_ctor_get(x_100, 1);
lean_inc(x_106);
lean_dec(x_100);
x_107 = l_Lean_MessageData_ofName(x_3);
x_108 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_109 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_109, 0, x_108);
lean_ctor_set(x_109, 1, x_107);
x_110 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_111 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_111, 0, x_109);
lean_ctor_set(x_111, 1, x_110);
lean_inc(x_94);
x_112 = l_Std_BitVec_toHex(x_93, x_94);
x_113 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_113, 0, x_112);
x_114 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_115 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_115, 0, x_114);
lean_ctor_set(x_115, 1, x_113);
x_116 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_117 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_117, 0, x_115);
lean_ctor_set(x_117, 1, x_116);
lean_inc(x_93);
x_118 = l_Nat_repr(x_93);
x_119 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_119, 0, x_118);
x_120 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_120, 0, x_117);
lean_ctor_set(x_120, 1, x_119);
x_121 = l_Std_Format_defWidth;
x_122 = lean_unsigned_to_nat(0u);
x_123 = lean_format_pretty(x_120, x_121, x_122, x_122);
x_124 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_124, 0, x_123);
x_125 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_125, 0, x_124);
x_126 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_126, 0, x_111);
lean_ctor_set(x_126, 1, x_125);
x_127 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_128 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_128, 0, x_126);
lean_ctor_set(x_128, 1, x_127);
x_129 = lean_ctor_get(x_92, 1);
lean_inc(x_129);
x_130 = l_Std_BitVec_toHex(x_95, x_129);
x_131 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_131, 0, x_130);
x_132 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_132, 0, x_114);
lean_ctor_set(x_132, 1, x_131);
x_133 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_133, 0, x_132);
lean_ctor_set(x_133, 1, x_116);
x_134 = l_Nat_repr(x_95);
x_135 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_135, 0, x_134);
x_136 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_136, 0, x_133);
lean_ctor_set(x_136, 1, x_135);
x_137 = lean_format_pretty(x_136, x_121, x_122, x_122);
x_138 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_138, 0, x_137);
x_139 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_139, 0, x_138);
x_140 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_140, 0, x_128);
lean_ctor_set(x_140, 1, x_139);
x_141 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_142 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_142, 0, x_140);
lean_ctor_set(x_142, 1, x_141);
x_143 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_99, x_142, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_106);
x_144 = lean_ctor_get(x_143, 0);
lean_inc(x_144);
x_145 = lean_ctor_get(x_143, 1);
lean_inc(x_145);
lean_dec(x_143);
x_146 = l_Std_BitVec_reduceBin___lambda__1(x_92, x_2, x_93, x_94, x_144, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_145);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_144);
return x_146;
}
}
}
}
}
else
{
uint8_t x_147; 
lean_dec(x_24);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_3);
lean_dec(x_2);
x_147 = !lean_is_exclusive(x_26);
if (x_147 == 0)
{
return x_26;
}
else
{
lean_object* x_148; lean_object* x_149; lean_object* x_150; 
x_148 = lean_ctor_get(x_26, 0);
x_149 = lean_ctor_get(x_26, 1);
lean_inc(x_149);
lean_inc(x_148);
lean_dec(x_26);
x_150 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_150, 0, x_148);
lean_ctor_set(x_150, 1, x_149);
return x_150;
}
}
}
}
else
{
uint8_t x_151; 
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_151 = !lean_is_exclusive(x_15);
if (x_151 == 0)
{
return x_15;
}
else
{
lean_object* x_152; lean_object* x_153; lean_object* x_154; 
x_152 = lean_ctor_get(x_15, 0);
x_153 = lean_ctor_get(x_15, 1);
lean_inc(x_153);
lean_inc(x_152);
lean_dec(x_15);
x_154 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_154, 0, x_152);
lean_ctor_set(x_154, 1, x_153);
return x_154;
}
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBin(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
uint8_t x_13; 
x_13 = l_Lean_Expr_isAppOfArity(x_4, x_1, x_2);
if (x_13 == 0)
{
lean_object* x_14; lean_object* x_15; 
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_1);
x_14 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_14);
lean_ctor_set(x_15, 1, x_12);
return x_15;
}
else
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_box(0);
x_17 = l_Std_BitVec_reduceBin___lambda__2(x_4, x_3, x_1, x_16, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
return x_17;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBin___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12, lean_object* x_13) {
_start:
{
lean_object* x_14; 
x_14 = l_Std_BitVec_reduceBin___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12, x_13);
lean_dec(x_12);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
return x_14;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceExtend___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; 
x_12 = l_Lean_Expr_appArg_x21(x_1);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_2);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appFn_x21(x_1);
x_24 = l_Lean_Expr_appArg_x21(x_23);
lean_dec(x_23);
x_25 = l_Nat_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_21);
lean_dec(x_7);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_22);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; lean_object* x_40; lean_object* x_41; uint32_t x_42; uint8_t x_43; lean_object* x_44; lean_object* x_45; 
x_34 = lean_ctor_get(x_25, 0);
lean_dec(x_34);
x_35 = lean_ctor_get(x_26, 0);
lean_inc(x_35);
lean_dec(x_26);
x_36 = lean_ctor_get(x_22, 0);
lean_inc(x_36);
x_37 = lean_ctor_get(x_22, 1);
lean_inc(x_37);
lean_dec(x_22);
lean_inc(x_35);
x_38 = lean_apply_3(x_2, x_36, x_35, x_37);
x_39 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_39, 0, x_35);
lean_ctor_set(x_39, 1, x_38);
x_40 = l_Std_BitVec_Literal_toExpr(x_39);
x_41 = lean_box(0);
x_42 = 0;
x_43 = 1;
x_44 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_44, 0, x_40);
lean_ctor_set(x_44, 1, x_41);
lean_ctor_set_uint32(x_44, sizeof(void*)*2, x_42);
lean_ctor_set_uint8(x_44, sizeof(void*)*2 + 4, x_43);
x_45 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_45, 0, x_44);
lean_ctor_set(x_25, 0, x_45);
return x_25;
}
else
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; uint32_t x_54; uint8_t x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; 
x_46 = lean_ctor_get(x_25, 1);
lean_inc(x_46);
lean_dec(x_25);
x_47 = lean_ctor_get(x_26, 0);
lean_inc(x_47);
lean_dec(x_26);
x_48 = lean_ctor_get(x_22, 0);
lean_inc(x_48);
x_49 = lean_ctor_get(x_22, 1);
lean_inc(x_49);
lean_dec(x_22);
lean_inc(x_47);
x_50 = lean_apply_3(x_2, x_48, x_47, x_49);
x_51 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_51, 0, x_47);
lean_ctor_set(x_51, 1, x_50);
x_52 = l_Std_BitVec_Literal_toExpr(x_51);
x_53 = lean_box(0);
x_54 = 0;
x_55 = 1;
x_56 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_56, 0, x_52);
lean_ctor_set(x_56, 1, x_53);
lean_ctor_set_uint32(x_56, sizeof(void*)*2, x_54);
lean_ctor_set_uint8(x_56, sizeof(void*)*2 + 4, x_55);
x_57 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_57, 0, x_56);
x_58 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_58, 0, x_57);
lean_ctor_set(x_58, 1, x_46);
return x_58;
}
}
}
else
{
uint8_t x_59; 
lean_dec(x_22);
lean_dec(x_2);
x_59 = !lean_is_exclusive(x_25);
if (x_59 == 0)
{
return x_25;
}
else
{
lean_object* x_60; lean_object* x_61; lean_object* x_62; 
x_60 = lean_ctor_get(x_25, 0);
x_61 = lean_ctor_get(x_25, 1);
lean_inc(x_61);
lean_inc(x_60);
lean_dec(x_25);
x_62 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_62, 0, x_60);
lean_ctor_set(x_62, 1, x_61);
return x_62;
}
}
}
}
else
{
uint8_t x_63; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_2);
lean_dec(x_1);
x_63 = !lean_is_exclusive(x_13);
if (x_63 == 0)
{
return x_13;
}
else
{
lean_object* x_64; lean_object* x_65; lean_object* x_66; 
x_64 = lean_ctor_get(x_13, 0);
x_65 = lean_ctor_get(x_13, 1);
lean_inc(x_65);
lean_inc(x_64);
lean_dec(x_13);
x_66 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_66, 0, x_64);
lean_ctor_set(x_66, 1, x_65);
return x_66;
}
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceExtend(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; uint8_t x_13; 
x_12 = lean_unsigned_to_nat(3u);
x_13 = l_Lean_Expr_isAppOfArity(x_3, x_1, x_12);
lean_dec(x_1);
if (x_13 == 0)
{
lean_object* x_14; lean_object* x_15; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_3);
lean_dec(x_2);
x_14 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_14);
lean_ctor_set(x_15, 1, x_11);
return x_15;
}
else
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_box(0);
x_17 = l_Std_BitVec_reduceExtend___lambda__1(x_3, x_2, x_16, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
return x_17;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceExtend___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; 
x_12 = l_Std_BitVec_reduceExtend___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
return x_12;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceExtend___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; 
x_12 = l_Std_BitVec_reduceExtend(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
return x_12;
}
}
static lean_object* _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("Bool", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("false", 5);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceGetBit___lambda__1___closed__1;
x_2 = l_Std_BitVec_reduceGetBit___lambda__1___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = lean_box(0);
x_2 = l_Std_BitVec_reduceGetBit___lambda__1___closed__3;
x_3 = l_Lean_Expr_const___override(x_2, x_1);
return x_3;
}
}
static lean_object* _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__5() {
_start:
{
lean_object* x_1; uint32_t x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; 
x_1 = lean_box(0);
x_2 = 0;
x_3 = l_Std_BitVec_reduceGetBit___lambda__1___closed__4;
x_4 = 1;
x_5 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_5, 0, x_3);
lean_ctor_set(x_5, 1, x_1);
lean_ctor_set_uint32(x_5, sizeof(void*)*2, x_2);
lean_ctor_set_uint8(x_5, sizeof(void*)*2 + 4, x_4);
return x_5;
}
}
static lean_object* _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = l_Std_BitVec_reduceGetBit___lambda__1___closed__5;
x_2 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_2, 0, x_1);
return x_2;
}
}
static lean_object* _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__7() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("true", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceGetBit___lambda__1___closed__1;
x_2 = l_Std_BitVec_reduceGetBit___lambda__1___closed__7;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = lean_box(0);
x_2 = l_Std_BitVec_reduceGetBit___lambda__1___closed__8;
x_3 = l_Lean_Expr_const___override(x_2, x_1);
return x_3;
}
}
static lean_object* _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__10() {
_start:
{
lean_object* x_1; uint32_t x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; 
x_1 = lean_box(0);
x_2 = 0;
x_3 = l_Std_BitVec_reduceGetBit___lambda__1___closed__9;
x_4 = 1;
x_5 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_5, 0, x_3);
lean_ctor_set(x_5, 1, x_1);
lean_ctor_set_uint32(x_5, sizeof(void*)*2, x_2);
lean_ctor_set_uint8(x_5, sizeof(void*)*2 + 4, x_4);
return x_5;
}
}
static lean_object* _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__11() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = l_Std_BitVec_reduceGetBit___lambda__1___closed__10;
x_2 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_2, 0, x_1);
return x_2;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetBit___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; lean_object* x_14; 
lean_inc(x_1);
x_12 = l_Lean_Expr_appFn_x21(x_1);
x_13 = l_Lean_Expr_appArg_x21(x_12);
lean_dec(x_12);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_14 = l_Std_BitVec_fromExpr_x3f(x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_14) == 0)
{
lean_object* x_15; 
x_15 = lean_ctor_get(x_14, 0);
lean_inc(x_15);
if (lean_obj_tag(x_15) == 0)
{
uint8_t x_16; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_2);
lean_dec(x_1);
x_16 = !lean_is_exclusive(x_14);
if (x_16 == 0)
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_ctor_get(x_14, 0);
lean_dec(x_17);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_14, 0, x_18);
return x_14;
}
else
{
lean_object* x_19; lean_object* x_20; lean_object* x_21; 
x_19 = lean_ctor_get(x_14, 1);
lean_inc(x_19);
lean_dec(x_14);
x_20 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_21 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_21, 0, x_20);
lean_ctor_set(x_21, 1, x_19);
return x_21;
}
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_22 = lean_ctor_get(x_14, 1);
lean_inc(x_22);
lean_dec(x_14);
x_23 = lean_ctor_get(x_15, 0);
lean_inc(x_23);
lean_dec(x_15);
x_24 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_25 = l_Nat_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_22);
lean_dec(x_7);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_23);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; uint8_t x_39; 
x_34 = lean_ctor_get(x_25, 0);
lean_dec(x_34);
x_35 = lean_ctor_get(x_26, 0);
lean_inc(x_35);
lean_dec(x_26);
x_36 = lean_ctor_get(x_23, 0);
lean_inc(x_36);
x_37 = lean_ctor_get(x_23, 1);
lean_inc(x_37);
lean_dec(x_23);
x_38 = lean_apply_3(x_2, x_36, x_37, x_35);
x_39 = lean_unbox(x_38);
lean_dec(x_38);
if (x_39 == 0)
{
lean_object* x_40; 
x_40 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
lean_ctor_set(x_25, 0, x_40);
return x_25;
}
else
{
lean_object* x_41; 
x_41 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
lean_ctor_set(x_25, 0, x_41);
return x_25;
}
}
else
{
lean_object* x_42; lean_object* x_43; lean_object* x_44; lean_object* x_45; lean_object* x_46; uint8_t x_47; 
x_42 = lean_ctor_get(x_25, 1);
lean_inc(x_42);
lean_dec(x_25);
x_43 = lean_ctor_get(x_26, 0);
lean_inc(x_43);
lean_dec(x_26);
x_44 = lean_ctor_get(x_23, 0);
lean_inc(x_44);
x_45 = lean_ctor_get(x_23, 1);
lean_inc(x_45);
lean_dec(x_23);
x_46 = lean_apply_3(x_2, x_44, x_45, x_43);
x_47 = lean_unbox(x_46);
lean_dec(x_46);
if (x_47 == 0)
{
lean_object* x_48; lean_object* x_49; 
x_48 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
x_49 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_49, 0, x_48);
lean_ctor_set(x_49, 1, x_42);
return x_49;
}
else
{
lean_object* x_50; lean_object* x_51; 
x_50 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
x_51 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_51, 0, x_50);
lean_ctor_set(x_51, 1, x_42);
return x_51;
}
}
}
}
else
{
uint8_t x_52; 
lean_dec(x_23);
lean_dec(x_2);
x_52 = !lean_is_exclusive(x_25);
if (x_52 == 0)
{
return x_25;
}
else
{
lean_object* x_53; lean_object* x_54; lean_object* x_55; 
x_53 = lean_ctor_get(x_25, 0);
x_54 = lean_ctor_get(x_25, 1);
lean_inc(x_54);
lean_inc(x_53);
lean_dec(x_25);
x_55 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_55, 0, x_53);
lean_ctor_set(x_55, 1, x_54);
return x_55;
}
}
}
}
else
{
uint8_t x_56; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_2);
lean_dec(x_1);
x_56 = !lean_is_exclusive(x_14);
if (x_56 == 0)
{
return x_14;
}
else
{
lean_object* x_57; lean_object* x_58; lean_object* x_59; 
x_57 = lean_ctor_get(x_14, 0);
x_58 = lean_ctor_get(x_14, 1);
lean_inc(x_58);
lean_inc(x_57);
lean_dec(x_14);
x_59 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_59, 0, x_57);
lean_ctor_set(x_59, 1, x_58);
return x_59;
}
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetBit(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; uint8_t x_13; 
x_12 = lean_unsigned_to_nat(3u);
x_13 = l_Lean_Expr_isAppOfArity(x_3, x_1, x_12);
lean_dec(x_1);
if (x_13 == 0)
{
lean_object* x_14; lean_object* x_15; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_3);
lean_dec(x_2);
x_14 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_14);
lean_ctor_set(x_15, 1, x_11);
return x_15;
}
else
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_box(0);
x_17 = l_Std_BitVec_reduceGetBit___lambda__1(x_3, x_2, x_16, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
return x_17;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetBit___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; 
x_12 = l_Std_BitVec_reduceGetBit___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
return x_12;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetBit___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; 
x_12 = l_Std_BitVec_reduceGetBit(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
return x_12;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShift___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; lean_object* x_14; 
lean_inc(x_1);
x_12 = l_Lean_Expr_appFn_x21(x_1);
x_13 = l_Lean_Expr_appArg_x21(x_12);
lean_dec(x_12);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_14 = l_Std_BitVec_fromExpr_x3f(x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_14) == 0)
{
lean_object* x_15; 
x_15 = lean_ctor_get(x_14, 0);
lean_inc(x_15);
if (lean_obj_tag(x_15) == 0)
{
uint8_t x_16; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_2);
lean_dec(x_1);
x_16 = !lean_is_exclusive(x_14);
if (x_16 == 0)
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_ctor_get(x_14, 0);
lean_dec(x_17);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_14, 0, x_18);
return x_14;
}
else
{
lean_object* x_19; lean_object* x_20; lean_object* x_21; 
x_19 = lean_ctor_get(x_14, 1);
lean_inc(x_19);
lean_dec(x_14);
x_20 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_21 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_21, 0, x_20);
lean_ctor_set(x_21, 1, x_19);
return x_21;
}
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_22 = lean_ctor_get(x_14, 1);
lean_inc(x_22);
lean_dec(x_14);
x_23 = lean_ctor_get(x_15, 0);
lean_inc(x_23);
lean_dec(x_15);
x_24 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_25 = l_Nat_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_22);
lean_dec(x_7);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_23);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; uint8_t x_36; 
x_34 = lean_ctor_get(x_25, 0);
lean_dec(x_34);
x_35 = lean_ctor_get(x_26, 0);
lean_inc(x_35);
lean_dec(x_26);
x_36 = !lean_is_exclusive(x_23);
if (x_36 == 0)
{
lean_object* x_37; lean_object* x_38; lean_object* x_39; lean_object* x_40; lean_object* x_41; uint32_t x_42; uint8_t x_43; lean_object* x_44; lean_object* x_45; 
x_37 = lean_ctor_get(x_23, 0);
x_38 = lean_ctor_get(x_23, 1);
lean_inc(x_37);
x_39 = lean_apply_3(x_2, x_37, x_38, x_35);
lean_ctor_set(x_23, 1, x_39);
x_40 = l_Std_BitVec_Literal_toExpr(x_23);
x_41 = lean_box(0);
x_42 = 0;
x_43 = 1;
x_44 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_44, 0, x_40);
lean_ctor_set(x_44, 1, x_41);
lean_ctor_set_uint32(x_44, sizeof(void*)*2, x_42);
lean_ctor_set_uint8(x_44, sizeof(void*)*2 + 4, x_43);
x_45 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_45, 0, x_44);
lean_ctor_set(x_25, 0, x_45);
return x_25;
}
else
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; lean_object* x_51; uint32_t x_52; uint8_t x_53; lean_object* x_54; lean_object* x_55; 
x_46 = lean_ctor_get(x_23, 0);
x_47 = lean_ctor_get(x_23, 1);
lean_inc(x_47);
lean_inc(x_46);
lean_dec(x_23);
lean_inc(x_46);
x_48 = lean_apply_3(x_2, x_46, x_47, x_35);
x_49 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_49, 0, x_46);
lean_ctor_set(x_49, 1, x_48);
x_50 = l_Std_BitVec_Literal_toExpr(x_49);
x_51 = lean_box(0);
x_52 = 0;
x_53 = 1;
x_54 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_54, 0, x_50);
lean_ctor_set(x_54, 1, x_51);
lean_ctor_set_uint32(x_54, sizeof(void*)*2, x_52);
lean_ctor_set_uint8(x_54, sizeof(void*)*2 + 4, x_53);
x_55 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_55, 0, x_54);
lean_ctor_set(x_25, 0, x_55);
return x_25;
}
}
else
{
lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; uint32_t x_65; uint8_t x_66; lean_object* x_67; lean_object* x_68; lean_object* x_69; 
x_56 = lean_ctor_get(x_25, 1);
lean_inc(x_56);
lean_dec(x_25);
x_57 = lean_ctor_get(x_26, 0);
lean_inc(x_57);
lean_dec(x_26);
x_58 = lean_ctor_get(x_23, 0);
lean_inc(x_58);
x_59 = lean_ctor_get(x_23, 1);
lean_inc(x_59);
if (lean_is_exclusive(x_23)) {
 lean_ctor_release(x_23, 0);
 lean_ctor_release(x_23, 1);
 x_60 = x_23;
} else {
 lean_dec_ref(x_23);
 x_60 = lean_box(0);
}
lean_inc(x_58);
x_61 = lean_apply_3(x_2, x_58, x_59, x_57);
if (lean_is_scalar(x_60)) {
 x_62 = lean_alloc_ctor(0, 2, 0);
} else {
 x_62 = x_60;
}
lean_ctor_set(x_62, 0, x_58);
lean_ctor_set(x_62, 1, x_61);
x_63 = l_Std_BitVec_Literal_toExpr(x_62);
x_64 = lean_box(0);
x_65 = 0;
x_66 = 1;
x_67 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_67, 0, x_63);
lean_ctor_set(x_67, 1, x_64);
lean_ctor_set_uint32(x_67, sizeof(void*)*2, x_65);
lean_ctor_set_uint8(x_67, sizeof(void*)*2 + 4, x_66);
x_68 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_68, 0, x_67);
x_69 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_69, 0, x_68);
lean_ctor_set(x_69, 1, x_56);
return x_69;
}
}
}
else
{
uint8_t x_70; 
lean_dec(x_23);
lean_dec(x_2);
x_70 = !lean_is_exclusive(x_25);
if (x_70 == 0)
{
return x_25;
}
else
{
lean_object* x_71; lean_object* x_72; lean_object* x_73; 
x_71 = lean_ctor_get(x_25, 0);
x_72 = lean_ctor_get(x_25, 1);
lean_inc(x_72);
lean_inc(x_71);
lean_dec(x_25);
x_73 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_73, 0, x_71);
lean_ctor_set(x_73, 1, x_72);
return x_73;
}
}
}
}
else
{
uint8_t x_74; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_2);
lean_dec(x_1);
x_74 = !lean_is_exclusive(x_14);
if (x_74 == 0)
{
return x_14;
}
else
{
lean_object* x_75; lean_object* x_76; lean_object* x_77; 
x_75 = lean_ctor_get(x_14, 0);
x_76 = lean_ctor_get(x_14, 1);
lean_inc(x_76);
lean_inc(x_75);
lean_dec(x_14);
x_77 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_77, 0, x_75);
lean_ctor_set(x_77, 1, x_76);
return x_77;
}
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShift(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
uint8_t x_13; 
x_13 = l_Lean_Expr_isAppOfArity(x_4, x_1, x_2);
lean_dec(x_1);
if (x_13 == 0)
{
lean_object* x_14; lean_object* x_15; 
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
x_14 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_14);
lean_ctor_set(x_15, 1, x_12);
return x_15;
}
else
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_box(0);
x_17 = l_Std_BitVec_reduceShift___lambda__1(x_4, x_3, x_16, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
return x_17;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShift___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; 
x_12 = l_Std_BitVec_reduceShift___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
return x_12;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBinPred___lambda__1(lean_object* x_1, lean_object* x_2, uint8_t x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; lean_object* x_14; lean_object* x_15; 
lean_inc(x_1);
x_13 = l_Lean_Expr_appFn_x21(x_1);
x_14 = l_Lean_Expr_appArg_x21(x_13);
lean_dec(x_13);
lean_inc(x_11);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
x_15 = l_Std_BitVec_fromExpr_x3f(x_14, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
if (lean_obj_tag(x_15) == 0)
{
lean_object* x_16; 
x_16 = lean_ctor_get(x_15, 0);
lean_inc(x_16);
if (lean_obj_tag(x_16) == 0)
{
uint8_t x_17; 
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_2);
lean_dec(x_1);
x_17 = !lean_is_exclusive(x_15);
if (x_17 == 0)
{
lean_object* x_18; lean_object* x_19; 
x_18 = lean_ctor_get(x_15, 0);
lean_dec(x_18);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_15, 0, x_19);
return x_15;
}
else
{
lean_object* x_20; lean_object* x_21; lean_object* x_22; 
x_20 = lean_ctor_get(x_15, 1);
lean_inc(x_20);
lean_dec(x_15);
x_21 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_22 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_22, 0, x_21);
lean_ctor_set(x_22, 1, x_20);
return x_22;
}
}
else
{
lean_object* x_23; lean_object* x_24; lean_object* x_25; lean_object* x_26; 
x_23 = lean_ctor_get(x_15, 1);
lean_inc(x_23);
lean_dec(x_15);
x_24 = lean_ctor_get(x_16, 0);
lean_inc(x_24);
lean_dec(x_16);
x_25 = l_Lean_Expr_appArg_x21(x_1);
lean_inc(x_11);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
x_26 = l_Std_BitVec_fromExpr_x3f(x_25, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_23);
if (lean_obj_tag(x_26) == 0)
{
lean_object* x_27; 
x_27 = lean_ctor_get(x_26, 0);
lean_inc(x_27);
if (lean_obj_tag(x_27) == 0)
{
uint8_t x_28; 
lean_dec(x_24);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_2);
lean_dec(x_1);
x_28 = !lean_is_exclusive(x_26);
if (x_28 == 0)
{
lean_object* x_29; lean_object* x_30; 
x_29 = lean_ctor_get(x_26, 0);
lean_dec(x_29);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_26, 0, x_30);
return x_26;
}
else
{
lean_object* x_31; lean_object* x_32; lean_object* x_33; 
x_31 = lean_ctor_get(x_26, 1);
lean_inc(x_31);
lean_dec(x_26);
x_32 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_33 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_33, 0, x_32);
lean_ctor_set(x_33, 1, x_31);
return x_33;
}
}
else
{
uint8_t x_34; 
x_34 = !lean_is_exclusive(x_26);
if (x_34 == 0)
{
lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint8_t x_40; 
x_35 = lean_ctor_get(x_26, 1);
x_36 = lean_ctor_get(x_26, 0);
lean_dec(x_36);
x_37 = lean_ctor_get(x_27, 0);
lean_inc(x_37);
lean_dec(x_27);
x_38 = lean_ctor_get(x_24, 0);
lean_inc(x_38);
x_39 = lean_ctor_get(x_37, 0);
lean_inc(x_39);
x_40 = lean_nat_dec_eq(x_38, x_39);
lean_dec(x_39);
if (x_40 == 0)
{
lean_object* x_41; 
lean_dec(x_38);
lean_dec(x_37);
lean_dec(x_24);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_2);
lean_dec(x_1);
x_41 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_26, 0, x_41);
return x_26;
}
else
{
lean_object* x_42; lean_object* x_43; lean_object* x_44; uint8_t x_45; 
x_42 = lean_ctor_get(x_24, 1);
lean_inc(x_42);
lean_dec(x_24);
x_43 = lean_ctor_get(x_37, 1);
lean_inc(x_43);
lean_dec(x_37);
x_44 = lean_apply_3(x_2, x_38, x_42, x_43);
x_45 = lean_unbox(x_44);
lean_dec(x_44);
if (x_3 == 0)
{
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_1);
if (x_45 == 0)
{
lean_object* x_46; 
x_46 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
lean_ctor_set(x_26, 0, x_46);
return x_26;
}
else
{
lean_object* x_47; 
x_47 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
lean_ctor_set(x_26, 0, x_47);
return x_26;
}
}
else
{
lean_object* x_48; 
lean_free_object(x_26);
x_48 = l_Lean_Meta_Simp_evalPropStep(x_1, x_45, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_35);
return x_48;
}
}
}
else
{
lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; uint8_t x_53; 
x_49 = lean_ctor_get(x_26, 1);
lean_inc(x_49);
lean_dec(x_26);
x_50 = lean_ctor_get(x_27, 0);
lean_inc(x_50);
lean_dec(x_27);
x_51 = lean_ctor_get(x_24, 0);
lean_inc(x_51);
x_52 = lean_ctor_get(x_50, 0);
lean_inc(x_52);
x_53 = lean_nat_dec_eq(x_51, x_52);
lean_dec(x_52);
if (x_53 == 0)
{
lean_object* x_54; lean_object* x_55; 
lean_dec(x_51);
lean_dec(x_50);
lean_dec(x_24);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_2);
lean_dec(x_1);
x_54 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_55 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_55, 0, x_54);
lean_ctor_set(x_55, 1, x_49);
return x_55;
}
else
{
lean_object* x_56; lean_object* x_57; lean_object* x_58; uint8_t x_59; 
x_56 = lean_ctor_get(x_24, 1);
lean_inc(x_56);
lean_dec(x_24);
x_57 = lean_ctor_get(x_50, 1);
lean_inc(x_57);
lean_dec(x_50);
x_58 = lean_apply_3(x_2, x_51, x_56, x_57);
x_59 = lean_unbox(x_58);
lean_dec(x_58);
if (x_3 == 0)
{
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_1);
if (x_59 == 0)
{
lean_object* x_60; lean_object* x_61; 
x_60 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
x_61 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_61, 0, x_60);
lean_ctor_set(x_61, 1, x_49);
return x_61;
}
else
{
lean_object* x_62; lean_object* x_63; 
x_62 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
x_63 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_63, 0, x_62);
lean_ctor_set(x_63, 1, x_49);
return x_63;
}
}
else
{
lean_object* x_64; 
x_64 = l_Lean_Meta_Simp_evalPropStep(x_1, x_59, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_49);
return x_64;
}
}
}
}
}
else
{
uint8_t x_65; 
lean_dec(x_24);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_2);
lean_dec(x_1);
x_65 = !lean_is_exclusive(x_26);
if (x_65 == 0)
{
return x_26;
}
else
{
lean_object* x_66; lean_object* x_67; lean_object* x_68; 
x_66 = lean_ctor_get(x_26, 0);
x_67 = lean_ctor_get(x_26, 1);
lean_inc(x_67);
lean_inc(x_66);
lean_dec(x_26);
x_68 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_68, 0, x_66);
lean_ctor_set(x_68, 1, x_67);
return x_68;
}
}
}
}
else
{
uint8_t x_69; 
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_2);
lean_dec(x_1);
x_69 = !lean_is_exclusive(x_15);
if (x_69 == 0)
{
return x_15;
}
else
{
lean_object* x_70; lean_object* x_71; lean_object* x_72; 
x_70 = lean_ctor_get(x_15, 0);
x_71 = lean_ctor_get(x_15, 1);
lean_inc(x_71);
lean_inc(x_70);
lean_dec(x_15);
x_72 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_72, 0, x_70);
lean_ctor_set(x_72, 1, x_71);
return x_72;
}
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBinPred(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, uint8_t x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12, lean_object* x_13) {
_start:
{
uint8_t x_14; 
x_14 = l_Lean_Expr_isAppOfArity(x_4, x_1, x_2);
lean_dec(x_1);
if (x_14 == 0)
{
lean_object* x_15; lean_object* x_16; 
lean_dec(x_12);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_4);
lean_dec(x_3);
x_15 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_16 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_16, 0, x_15);
lean_ctor_set(x_16, 1, x_13);
return x_16;
}
else
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_box(0);
x_18 = l_Std_BitVec_reduceBinPred___lambda__1(x_4, x_3, x_5, x_17, x_6, x_7, x_8, x_9, x_10, x_11, x_12, x_13);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
return x_18;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBinPred___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
uint8_t x_13; lean_object* x_14; 
x_13 = lean_unbox(x_3);
lean_dec(x_3);
x_14 = l_Std_BitVec_reduceBinPred___lambda__1(x_1, x_2, x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
return x_14;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceBinPred___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12, lean_object* x_13) {
_start:
{
uint8_t x_14; lean_object* x_15; 
x_14 = lean_unbox(x_5);
lean_dec(x_5);
x_15 = l_Std_BitVec_reduceBinPred(x_1, x_2, x_3, x_4, x_14, x_6, x_7, x_8, x_9, x_10, x_11, x_12, x_13);
return x_15;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNeg___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; 
x_11 = l_Lean_Expr_appArg_x21(x_1);
x_12 = l_Std_BitVec_fromExpr_x3f(x_11, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_12) == 0)
{
lean_object* x_13; 
x_13 = lean_ctor_get(x_12, 0);
lean_inc(x_13);
if (lean_obj_tag(x_13) == 0)
{
uint8_t x_14; 
x_14 = !lean_is_exclusive(x_12);
if (x_14 == 0)
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_ctor_get(x_12, 0);
lean_dec(x_15);
x_16 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_12, 0, x_16);
return x_12;
}
else
{
lean_object* x_17; lean_object* x_18; lean_object* x_19; 
x_17 = lean_ctor_get(x_12, 1);
lean_inc(x_17);
lean_dec(x_12);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_19 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_19, 0, x_18);
lean_ctor_set(x_19, 1, x_17);
return x_19;
}
}
else
{
lean_object* x_20; uint8_t x_21; 
x_20 = lean_ctor_get(x_13, 0);
lean_inc(x_20);
lean_dec(x_13);
x_21 = !lean_is_exclusive(x_12);
if (x_21 == 0)
{
lean_object* x_22; uint8_t x_23; 
x_22 = lean_ctor_get(x_12, 0);
lean_dec(x_22);
x_23 = !lean_is_exclusive(x_20);
if (x_23 == 0)
{
lean_object* x_24; lean_object* x_25; lean_object* x_26; lean_object* x_27; lean_object* x_28; uint32_t x_29; uint8_t x_30; lean_object* x_31; lean_object* x_32; 
x_24 = lean_ctor_get(x_20, 0);
x_25 = lean_ctor_get(x_20, 1);
x_26 = l_Std_BitVec_neg(x_24, x_25);
lean_dec(x_25);
lean_ctor_set(x_20, 1, x_26);
x_27 = l_Std_BitVec_Literal_toExpr(x_20);
x_28 = lean_box(0);
x_29 = 0;
x_30 = 1;
x_31 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_31, 0, x_27);
lean_ctor_set(x_31, 1, x_28);
lean_ctor_set_uint32(x_31, sizeof(void*)*2, x_29);
lean_ctor_set_uint8(x_31, sizeof(void*)*2 + 4, x_30);
x_32 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_12, 0, x_32);
return x_12;
}
else
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; uint32_t x_39; uint8_t x_40; lean_object* x_41; lean_object* x_42; 
x_33 = lean_ctor_get(x_20, 0);
x_34 = lean_ctor_get(x_20, 1);
lean_inc(x_34);
lean_inc(x_33);
lean_dec(x_20);
x_35 = l_Std_BitVec_neg(x_33, x_34);
lean_dec(x_34);
x_36 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_36, 0, x_33);
lean_ctor_set(x_36, 1, x_35);
x_37 = l_Std_BitVec_Literal_toExpr(x_36);
x_38 = lean_box(0);
x_39 = 0;
x_40 = 1;
x_41 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_41, 0, x_37);
lean_ctor_set(x_41, 1, x_38);
lean_ctor_set_uint32(x_41, sizeof(void*)*2, x_39);
lean_ctor_set_uint8(x_41, sizeof(void*)*2 + 4, x_40);
x_42 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_42, 0, x_41);
lean_ctor_set(x_12, 0, x_42);
return x_12;
}
}
else
{
lean_object* x_43; lean_object* x_44; lean_object* x_45; lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; uint32_t x_51; uint8_t x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; 
x_43 = lean_ctor_get(x_12, 1);
lean_inc(x_43);
lean_dec(x_12);
x_44 = lean_ctor_get(x_20, 0);
lean_inc(x_44);
x_45 = lean_ctor_get(x_20, 1);
lean_inc(x_45);
if (lean_is_exclusive(x_20)) {
 lean_ctor_release(x_20, 0);
 lean_ctor_release(x_20, 1);
 x_46 = x_20;
} else {
 lean_dec_ref(x_20);
 x_46 = lean_box(0);
}
x_47 = l_Std_BitVec_neg(x_44, x_45);
lean_dec(x_45);
if (lean_is_scalar(x_46)) {
 x_48 = lean_alloc_ctor(0, 2, 0);
} else {
 x_48 = x_46;
}
lean_ctor_set(x_48, 0, x_44);
lean_ctor_set(x_48, 1, x_47);
x_49 = l_Std_BitVec_Literal_toExpr(x_48);
x_50 = lean_box(0);
x_51 = 0;
x_52 = 1;
x_53 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_53, 0, x_49);
lean_ctor_set(x_53, 1, x_50);
lean_ctor_set_uint32(x_53, sizeof(void*)*2, x_51);
lean_ctor_set_uint8(x_53, sizeof(void*)*2 + 4, x_52);
x_54 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_54, 0, x_53);
x_55 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_55, 0, x_54);
lean_ctor_set(x_55, 1, x_43);
return x_55;
}
}
}
else
{
uint8_t x_56; 
x_56 = !lean_is_exclusive(x_12);
if (x_56 == 0)
{
return x_12;
}
else
{
lean_object* x_57; lean_object* x_58; lean_object* x_59; 
x_57 = lean_ctor_get(x_12, 0);
x_58 = lean_ctor_get(x_12, 1);
lean_inc(x_58);
lean_inc(x_57);
lean_dec(x_12);
x_59 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_59, 0, x_57);
lean_ctor_set(x_59, 1, x_58);
return x_59;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceNeg___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("Neg", 3);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceNeg___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("neg", 3);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceNeg___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceNeg___closed__1;
x_2 = l_Std_BitVec_reduceNeg___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNeg(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceNeg___closed__3;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceNeg___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNeg___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceNeg___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
return x_11;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNeg___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; 
x_10 = l_Std_BitVec_reduceNeg(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_1);
return x_10;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceNeg", 9);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceNeg___closed__3;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__6;
x_2 = lean_unsigned_to_nat(1u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = lean_unsigned_to_nat(5u);
x_2 = lean_mk_empty_array_with_capacity(x_1);
return x_2;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__5;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__6;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__8;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__10() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__9;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__11() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceNeg___boxed), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__10;
x_4 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__11;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = l_Lean_Meta_Simp_builtinSimprocsRef;
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__11;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = l_Lean_Meta_Simp_builtinSEvalprocsRef;
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__11;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNot___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; 
x_11 = l_Lean_Expr_appArg_x21(x_1);
x_12 = l_Std_BitVec_fromExpr_x3f(x_11, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_12) == 0)
{
lean_object* x_13; 
x_13 = lean_ctor_get(x_12, 0);
lean_inc(x_13);
if (lean_obj_tag(x_13) == 0)
{
uint8_t x_14; 
x_14 = !lean_is_exclusive(x_12);
if (x_14 == 0)
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_ctor_get(x_12, 0);
lean_dec(x_15);
x_16 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_12, 0, x_16);
return x_12;
}
else
{
lean_object* x_17; lean_object* x_18; lean_object* x_19; 
x_17 = lean_ctor_get(x_12, 1);
lean_inc(x_17);
lean_dec(x_12);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_19 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_19, 0, x_18);
lean_ctor_set(x_19, 1, x_17);
return x_19;
}
}
else
{
lean_object* x_20; uint8_t x_21; 
x_20 = lean_ctor_get(x_13, 0);
lean_inc(x_20);
lean_dec(x_13);
x_21 = !lean_is_exclusive(x_12);
if (x_21 == 0)
{
lean_object* x_22; uint8_t x_23; 
x_22 = lean_ctor_get(x_12, 0);
lean_dec(x_22);
x_23 = !lean_is_exclusive(x_20);
if (x_23 == 0)
{
lean_object* x_24; lean_object* x_25; lean_object* x_26; lean_object* x_27; lean_object* x_28; uint32_t x_29; uint8_t x_30; lean_object* x_31; lean_object* x_32; 
x_24 = lean_ctor_get(x_20, 0);
x_25 = lean_ctor_get(x_20, 1);
x_26 = l_Std_BitVec_not(x_24, x_25);
lean_dec(x_25);
lean_ctor_set(x_20, 1, x_26);
x_27 = l_Std_BitVec_Literal_toExpr(x_20);
x_28 = lean_box(0);
x_29 = 0;
x_30 = 1;
x_31 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_31, 0, x_27);
lean_ctor_set(x_31, 1, x_28);
lean_ctor_set_uint32(x_31, sizeof(void*)*2, x_29);
lean_ctor_set_uint8(x_31, sizeof(void*)*2 + 4, x_30);
x_32 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_12, 0, x_32);
return x_12;
}
else
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; uint32_t x_39; uint8_t x_40; lean_object* x_41; lean_object* x_42; 
x_33 = lean_ctor_get(x_20, 0);
x_34 = lean_ctor_get(x_20, 1);
lean_inc(x_34);
lean_inc(x_33);
lean_dec(x_20);
x_35 = l_Std_BitVec_not(x_33, x_34);
lean_dec(x_34);
x_36 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_36, 0, x_33);
lean_ctor_set(x_36, 1, x_35);
x_37 = l_Std_BitVec_Literal_toExpr(x_36);
x_38 = lean_box(0);
x_39 = 0;
x_40 = 1;
x_41 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_41, 0, x_37);
lean_ctor_set(x_41, 1, x_38);
lean_ctor_set_uint32(x_41, sizeof(void*)*2, x_39);
lean_ctor_set_uint8(x_41, sizeof(void*)*2 + 4, x_40);
x_42 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_42, 0, x_41);
lean_ctor_set(x_12, 0, x_42);
return x_12;
}
}
else
{
lean_object* x_43; lean_object* x_44; lean_object* x_45; lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; uint32_t x_51; uint8_t x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; 
x_43 = lean_ctor_get(x_12, 1);
lean_inc(x_43);
lean_dec(x_12);
x_44 = lean_ctor_get(x_20, 0);
lean_inc(x_44);
x_45 = lean_ctor_get(x_20, 1);
lean_inc(x_45);
if (lean_is_exclusive(x_20)) {
 lean_ctor_release(x_20, 0);
 lean_ctor_release(x_20, 1);
 x_46 = x_20;
} else {
 lean_dec_ref(x_20);
 x_46 = lean_box(0);
}
x_47 = l_Std_BitVec_not(x_44, x_45);
lean_dec(x_45);
if (lean_is_scalar(x_46)) {
 x_48 = lean_alloc_ctor(0, 2, 0);
} else {
 x_48 = x_46;
}
lean_ctor_set(x_48, 0, x_44);
lean_ctor_set(x_48, 1, x_47);
x_49 = l_Std_BitVec_Literal_toExpr(x_48);
x_50 = lean_box(0);
x_51 = 0;
x_52 = 1;
x_53 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_53, 0, x_49);
lean_ctor_set(x_53, 1, x_50);
lean_ctor_set_uint32(x_53, sizeof(void*)*2, x_51);
lean_ctor_set_uint8(x_53, sizeof(void*)*2 + 4, x_52);
x_54 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_54, 0, x_53);
x_55 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_55, 0, x_54);
lean_ctor_set(x_55, 1, x_43);
return x_55;
}
}
}
else
{
uint8_t x_56; 
x_56 = !lean_is_exclusive(x_12);
if (x_56 == 0)
{
return x_12;
}
else
{
lean_object* x_57; lean_object* x_58; lean_object* x_59; 
x_57 = lean_ctor_get(x_12, 0);
x_58 = lean_ctor_get(x_12, 1);
lean_inc(x_58);
lean_inc(x_57);
lean_dec(x_12);
x_59 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_59, 0, x_57);
lean_ctor_set(x_59, 1, x_58);
return x_59;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceNot___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("Complement", 10);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceNot___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("complement", 10);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceNot___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceNot___closed__1;
x_2 = l_Std_BitVec_reduceNot___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNot(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceNot___closed__3;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceNot___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNot___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceNot___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
return x_11;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceNot___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; 
x_10 = l_Std_BitVec_reduceNot(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_1);
return x_10;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceNot", 9);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceNot___closed__3;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__5;
x_2 = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__9() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceNot___boxed), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__8;
x_4 = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__9;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1410_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__9;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1412_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__9;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAbs___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; 
x_11 = l_Lean_Expr_appArg_x21(x_1);
x_12 = l_Std_BitVec_fromExpr_x3f(x_11, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_12) == 0)
{
lean_object* x_13; 
x_13 = lean_ctor_get(x_12, 0);
lean_inc(x_13);
if (lean_obj_tag(x_13) == 0)
{
uint8_t x_14; 
x_14 = !lean_is_exclusive(x_12);
if (x_14 == 0)
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_ctor_get(x_12, 0);
lean_dec(x_15);
x_16 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_12, 0, x_16);
return x_12;
}
else
{
lean_object* x_17; lean_object* x_18; lean_object* x_19; 
x_17 = lean_ctor_get(x_12, 1);
lean_inc(x_17);
lean_dec(x_12);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_19 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_19, 0, x_18);
lean_ctor_set(x_19, 1, x_17);
return x_19;
}
}
else
{
lean_object* x_20; uint8_t x_21; 
x_20 = lean_ctor_get(x_13, 0);
lean_inc(x_20);
lean_dec(x_13);
x_21 = !lean_is_exclusive(x_12);
if (x_21 == 0)
{
lean_object* x_22; uint8_t x_23; 
x_22 = lean_ctor_get(x_12, 0);
lean_dec(x_22);
x_23 = !lean_is_exclusive(x_20);
if (x_23 == 0)
{
lean_object* x_24; lean_object* x_25; lean_object* x_26; lean_object* x_27; lean_object* x_28; uint32_t x_29; uint8_t x_30; lean_object* x_31; lean_object* x_32; 
x_24 = lean_ctor_get(x_20, 0);
x_25 = lean_ctor_get(x_20, 1);
x_26 = l_Std_BitVec_abs(x_24, x_25);
lean_dec(x_25);
lean_ctor_set(x_20, 1, x_26);
x_27 = l_Std_BitVec_Literal_toExpr(x_20);
x_28 = lean_box(0);
x_29 = 0;
x_30 = 1;
x_31 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_31, 0, x_27);
lean_ctor_set(x_31, 1, x_28);
lean_ctor_set_uint32(x_31, sizeof(void*)*2, x_29);
lean_ctor_set_uint8(x_31, sizeof(void*)*2 + 4, x_30);
x_32 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_12, 0, x_32);
return x_12;
}
else
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; uint32_t x_39; uint8_t x_40; lean_object* x_41; lean_object* x_42; 
x_33 = lean_ctor_get(x_20, 0);
x_34 = lean_ctor_get(x_20, 1);
lean_inc(x_34);
lean_inc(x_33);
lean_dec(x_20);
x_35 = l_Std_BitVec_abs(x_33, x_34);
lean_dec(x_34);
x_36 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_36, 0, x_33);
lean_ctor_set(x_36, 1, x_35);
x_37 = l_Std_BitVec_Literal_toExpr(x_36);
x_38 = lean_box(0);
x_39 = 0;
x_40 = 1;
x_41 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_41, 0, x_37);
lean_ctor_set(x_41, 1, x_38);
lean_ctor_set_uint32(x_41, sizeof(void*)*2, x_39);
lean_ctor_set_uint8(x_41, sizeof(void*)*2 + 4, x_40);
x_42 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_42, 0, x_41);
lean_ctor_set(x_12, 0, x_42);
return x_12;
}
}
else
{
lean_object* x_43; lean_object* x_44; lean_object* x_45; lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; uint32_t x_51; uint8_t x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; 
x_43 = lean_ctor_get(x_12, 1);
lean_inc(x_43);
lean_dec(x_12);
x_44 = lean_ctor_get(x_20, 0);
lean_inc(x_44);
x_45 = lean_ctor_get(x_20, 1);
lean_inc(x_45);
if (lean_is_exclusive(x_20)) {
 lean_ctor_release(x_20, 0);
 lean_ctor_release(x_20, 1);
 x_46 = x_20;
} else {
 lean_dec_ref(x_20);
 x_46 = lean_box(0);
}
x_47 = l_Std_BitVec_abs(x_44, x_45);
lean_dec(x_45);
if (lean_is_scalar(x_46)) {
 x_48 = lean_alloc_ctor(0, 2, 0);
} else {
 x_48 = x_46;
}
lean_ctor_set(x_48, 0, x_44);
lean_ctor_set(x_48, 1, x_47);
x_49 = l_Std_BitVec_Literal_toExpr(x_48);
x_50 = lean_box(0);
x_51 = 0;
x_52 = 1;
x_53 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_53, 0, x_49);
lean_ctor_set(x_53, 1, x_50);
lean_ctor_set_uint32(x_53, sizeof(void*)*2, x_51);
lean_ctor_set_uint8(x_53, sizeof(void*)*2 + 4, x_52);
x_54 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_54, 0, x_53);
x_55 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_55, 0, x_54);
lean_ctor_set(x_55, 1, x_43);
return x_55;
}
}
}
else
{
uint8_t x_56; 
x_56 = !lean_is_exclusive(x_12);
if (x_56 == 0)
{
return x_12;
}
else
{
lean_object* x_57; lean_object* x_58; lean_object* x_59; 
x_57 = lean_ctor_get(x_12, 0);
x_58 = lean_ctor_get(x_12, 1);
lean_inc(x_58);
lean_inc(x_57);
lean_dec(x_12);
x_59 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_59, 0, x_57);
lean_ctor_set(x_59, 1, x_58);
return x_59;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceAbs___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("abs", 3);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceAbs___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceAbs___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAbs(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceAbs___closed__2;
x_11 = lean_unsigned_to_nat(2u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceAbs___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAbs___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceAbs___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
return x_11;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAbs___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; 
x_10 = l_Std_BitVec_reduceAbs(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_1);
return x_10;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceAbs", 9);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceAbs___closed__2;
x_2 = lean_unsigned_to_nat(2u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = lean_unsigned_to_nat(3u);
x_2 = lean_mk_empty_array_with_capacity(x_1);
return x_2;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceAbs___boxed), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1431_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1433_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAnd___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; lean_object* x_14; lean_object* x_15; lean_object* x_16; lean_object* x_17; uint32_t x_18; uint8_t x_19; lean_object* x_20; lean_object* x_21; lean_object* x_22; 
x_13 = lean_ctor_get(x_1, 1);
x_14 = lean_nat_land(x_2, x_13);
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_3);
lean_ctor_set(x_15, 1, x_14);
x_16 = l_Std_BitVec_Literal_toExpr(x_15);
x_17 = lean_box(0);
x_18 = 0;
x_19 = 1;
x_20 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_20, 0, x_16);
lean_ctor_set(x_20, 1, x_17);
lean_ctor_set_uint32(x_20, sizeof(void*)*2, x_18);
lean_ctor_set_uint8(x_20, sizeof(void*)*2 + 4, x_19);
x_21 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_21, 0, x_20);
x_22 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_22, 0, x_21);
lean_ctor_set(x_22, 1, x_12);
return x_22;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAnd___lambda__2(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; lean_object* x_14; 
lean_dec(x_3);
lean_inc(x_1);
x_12 = l_Lean_Expr_appFn_x21(x_1);
x_13 = l_Lean_Expr_appArg_x21(x_12);
lean_dec(x_12);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_14 = l_Std_BitVec_fromExpr_x3f(x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_14) == 0)
{
lean_object* x_15; 
x_15 = lean_ctor_get(x_14, 0);
lean_inc(x_15);
if (lean_obj_tag(x_15) == 0)
{
uint8_t x_16; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_16 = !lean_is_exclusive(x_14);
if (x_16 == 0)
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_ctor_get(x_14, 0);
lean_dec(x_17);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_14, 0, x_18);
return x_14;
}
else
{
lean_object* x_19; lean_object* x_20; lean_object* x_21; 
x_19 = lean_ctor_get(x_14, 1);
lean_inc(x_19);
lean_dec(x_14);
x_20 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_21 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_21, 0, x_20);
lean_ctor_set(x_21, 1, x_19);
return x_21;
}
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_22 = lean_ctor_get(x_14, 1);
lean_inc(x_22);
lean_dec(x_14);
x_23 = lean_ctor_get(x_15, 0);
lean_inc(x_23);
lean_dec(x_15);
x_24 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_25 = l_Std_BitVec_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_22);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint8_t x_40; 
x_34 = lean_ctor_get(x_25, 1);
x_35 = lean_ctor_get(x_25, 0);
lean_dec(x_35);
x_36 = lean_ctor_get(x_26, 0);
lean_inc(x_36);
lean_dec(x_26);
x_37 = lean_ctor_get(x_23, 0);
lean_inc(x_37);
x_38 = lean_ctor_get(x_23, 1);
lean_inc(x_38);
lean_dec(x_23);
x_39 = lean_ctor_get(x_36, 0);
lean_inc(x_39);
x_40 = lean_nat_dec_eq(x_37, x_39);
if (x_40 == 0)
{
lean_object* x_41; 
lean_dec(x_39);
lean_dec(x_38);
lean_dec(x_37);
lean_dec(x_36);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_41 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_41);
return x_25;
}
else
{
lean_object* x_42; lean_object* x_43; lean_object* x_44; uint8_t x_45; 
lean_free_object(x_25);
x_42 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_43 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_42, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_34);
x_44 = lean_ctor_get(x_43, 0);
lean_inc(x_44);
x_45 = lean_unbox(x_44);
lean_dec(x_44);
if (x_45 == 0)
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; 
lean_dec(x_39);
lean_dec(x_2);
x_46 = lean_ctor_get(x_43, 1);
lean_inc(x_46);
lean_dec(x_43);
x_47 = lean_box(0);
x_48 = l_Std_BitVec_reduceAnd___lambda__1(x_36, x_38, x_37, x_47, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_46);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_38);
lean_dec(x_36);
return x_48;
}
else
{
lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; lean_object* x_69; lean_object* x_70; lean_object* x_71; lean_object* x_72; lean_object* x_73; lean_object* x_74; lean_object* x_75; lean_object* x_76; lean_object* x_77; lean_object* x_78; lean_object* x_79; lean_object* x_80; lean_object* x_81; lean_object* x_82; lean_object* x_83; lean_object* x_84; lean_object* x_85; lean_object* x_86; lean_object* x_87; lean_object* x_88; lean_object* x_89; 
x_49 = lean_ctor_get(x_43, 1);
lean_inc(x_49);
lean_dec(x_43);
x_50 = l_Lean_MessageData_ofName(x_2);
x_51 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_52 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_52, 0, x_51);
lean_ctor_set(x_52, 1, x_50);
x_53 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_54 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_54, 0, x_52);
lean_ctor_set(x_54, 1, x_53);
lean_inc(x_38);
x_55 = l_Std_BitVec_toHex(x_37, x_38);
x_56 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_56, 0, x_55);
x_57 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_58 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_58, 0, x_57);
lean_ctor_set(x_58, 1, x_56);
x_59 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_60 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_60, 0, x_58);
lean_ctor_set(x_60, 1, x_59);
lean_inc(x_37);
x_61 = l_Nat_repr(x_37);
x_62 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_62, 0, x_61);
x_63 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_63, 0, x_60);
lean_ctor_set(x_63, 1, x_62);
x_64 = l_Std_Format_defWidth;
x_65 = lean_unsigned_to_nat(0u);
x_66 = lean_format_pretty(x_63, x_64, x_65, x_65);
x_67 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_68, 0, x_67);
x_69 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_69, 0, x_54);
lean_ctor_set(x_69, 1, x_68);
x_70 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_71 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
x_72 = lean_ctor_get(x_36, 1);
lean_inc(x_72);
x_73 = l_Std_BitVec_toHex(x_39, x_72);
x_74 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_74, 0, x_73);
x_75 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_75, 0, x_57);
lean_ctor_set(x_75, 1, x_74);
x_76 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_76, 0, x_75);
lean_ctor_set(x_76, 1, x_59);
x_77 = l_Nat_repr(x_39);
x_78 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_78, 0, x_77);
x_79 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_79, 0, x_76);
lean_ctor_set(x_79, 1, x_78);
x_80 = lean_format_pretty(x_79, x_64, x_65, x_65);
x_81 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_81, 0, x_80);
x_82 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_82, 0, x_81);
x_83 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_83, 0, x_71);
lean_ctor_set(x_83, 1, x_82);
x_84 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_85 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_85, 0, x_83);
lean_ctor_set(x_85, 1, x_84);
x_86 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_42, x_85, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_49);
x_87 = lean_ctor_get(x_86, 0);
lean_inc(x_87);
x_88 = lean_ctor_get(x_86, 1);
lean_inc(x_88);
lean_dec(x_86);
x_89 = l_Std_BitVec_reduceAnd___lambda__1(x_36, x_38, x_37, x_87, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_88);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_87);
lean_dec(x_38);
lean_dec(x_36);
return x_89;
}
}
}
else
{
lean_object* x_90; lean_object* x_91; lean_object* x_92; lean_object* x_93; lean_object* x_94; uint8_t x_95; 
x_90 = lean_ctor_get(x_25, 1);
lean_inc(x_90);
lean_dec(x_25);
x_91 = lean_ctor_get(x_26, 0);
lean_inc(x_91);
lean_dec(x_26);
x_92 = lean_ctor_get(x_23, 0);
lean_inc(x_92);
x_93 = lean_ctor_get(x_23, 1);
lean_inc(x_93);
lean_dec(x_23);
x_94 = lean_ctor_get(x_91, 0);
lean_inc(x_94);
x_95 = lean_nat_dec_eq(x_92, x_94);
if (x_95 == 0)
{
lean_object* x_96; lean_object* x_97; 
lean_dec(x_94);
lean_dec(x_93);
lean_dec(x_92);
lean_dec(x_91);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_96 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_97 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_97, 0, x_96);
lean_ctor_set(x_97, 1, x_90);
return x_97;
}
else
{
lean_object* x_98; lean_object* x_99; lean_object* x_100; uint8_t x_101; 
x_98 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_99 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_98, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_90);
x_100 = lean_ctor_get(x_99, 0);
lean_inc(x_100);
x_101 = lean_unbox(x_100);
lean_dec(x_100);
if (x_101 == 0)
{
lean_object* x_102; lean_object* x_103; lean_object* x_104; 
lean_dec(x_94);
lean_dec(x_2);
x_102 = lean_ctor_get(x_99, 1);
lean_inc(x_102);
lean_dec(x_99);
x_103 = lean_box(0);
x_104 = l_Std_BitVec_reduceAnd___lambda__1(x_91, x_93, x_92, x_103, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_102);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_93);
lean_dec(x_91);
return x_104;
}
else
{
lean_object* x_105; lean_object* x_106; lean_object* x_107; lean_object* x_108; lean_object* x_109; lean_object* x_110; lean_object* x_111; lean_object* x_112; lean_object* x_113; lean_object* x_114; lean_object* x_115; lean_object* x_116; lean_object* x_117; lean_object* x_118; lean_object* x_119; lean_object* x_120; lean_object* x_121; lean_object* x_122; lean_object* x_123; lean_object* x_124; lean_object* x_125; lean_object* x_126; lean_object* x_127; lean_object* x_128; lean_object* x_129; lean_object* x_130; lean_object* x_131; lean_object* x_132; lean_object* x_133; lean_object* x_134; lean_object* x_135; lean_object* x_136; lean_object* x_137; lean_object* x_138; lean_object* x_139; lean_object* x_140; lean_object* x_141; lean_object* x_142; lean_object* x_143; lean_object* x_144; lean_object* x_145; 
x_105 = lean_ctor_get(x_99, 1);
lean_inc(x_105);
lean_dec(x_99);
x_106 = l_Lean_MessageData_ofName(x_2);
x_107 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_108 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_108, 0, x_107);
lean_ctor_set(x_108, 1, x_106);
x_109 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_110 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_110, 0, x_108);
lean_ctor_set(x_110, 1, x_109);
lean_inc(x_93);
x_111 = l_Std_BitVec_toHex(x_92, x_93);
x_112 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_112, 0, x_111);
x_113 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_114 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_114, 0, x_113);
lean_ctor_set(x_114, 1, x_112);
x_115 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_116 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_116, 0, x_114);
lean_ctor_set(x_116, 1, x_115);
lean_inc(x_92);
x_117 = l_Nat_repr(x_92);
x_118 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_118, 0, x_117);
x_119 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_119, 0, x_116);
lean_ctor_set(x_119, 1, x_118);
x_120 = l_Std_Format_defWidth;
x_121 = lean_unsigned_to_nat(0u);
x_122 = lean_format_pretty(x_119, x_120, x_121, x_121);
x_123 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_123, 0, x_122);
x_124 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_124, 0, x_123);
x_125 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_125, 0, x_110);
lean_ctor_set(x_125, 1, x_124);
x_126 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_127 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_127, 0, x_125);
lean_ctor_set(x_127, 1, x_126);
x_128 = lean_ctor_get(x_91, 1);
lean_inc(x_128);
x_129 = l_Std_BitVec_toHex(x_94, x_128);
x_130 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_130, 0, x_129);
x_131 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_131, 0, x_113);
lean_ctor_set(x_131, 1, x_130);
x_132 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_132, 0, x_131);
lean_ctor_set(x_132, 1, x_115);
x_133 = l_Nat_repr(x_94);
x_134 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_134, 0, x_133);
x_135 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_135, 0, x_132);
lean_ctor_set(x_135, 1, x_134);
x_136 = lean_format_pretty(x_135, x_120, x_121, x_121);
x_137 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_137, 0, x_136);
x_138 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_138, 0, x_137);
x_139 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_139, 0, x_127);
lean_ctor_set(x_139, 1, x_138);
x_140 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_141 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_141, 0, x_139);
lean_ctor_set(x_141, 1, x_140);
x_142 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_98, x_141, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_105);
x_143 = lean_ctor_get(x_142, 0);
lean_inc(x_143);
x_144 = lean_ctor_get(x_142, 1);
lean_inc(x_144);
lean_dec(x_142);
x_145 = l_Std_BitVec_reduceAnd___lambda__1(x_91, x_93, x_92, x_143, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_144);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_143);
lean_dec(x_93);
lean_dec(x_91);
return x_145;
}
}
}
}
}
else
{
uint8_t x_146; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_146 = !lean_is_exclusive(x_25);
if (x_146 == 0)
{
return x_25;
}
else
{
lean_object* x_147; lean_object* x_148; lean_object* x_149; 
x_147 = lean_ctor_get(x_25, 0);
x_148 = lean_ctor_get(x_25, 1);
lean_inc(x_148);
lean_inc(x_147);
lean_dec(x_25);
x_149 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_149, 0, x_147);
lean_ctor_set(x_149, 1, x_148);
return x_149;
}
}
}
}
else
{
uint8_t x_150; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_150 = !lean_is_exclusive(x_14);
if (x_150 == 0)
{
return x_14;
}
else
{
lean_object* x_151; lean_object* x_152; lean_object* x_153; 
x_151 = lean_ctor_get(x_14, 0);
x_152 = lean_ctor_get(x_14, 1);
lean_inc(x_152);
lean_inc(x_151);
lean_dec(x_14);
x_153 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_153, 0, x_151);
lean_ctor_set(x_153, 1, x_152);
return x_153;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceAnd___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("HAnd", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceAnd___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("hAnd", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceAnd___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceAnd___closed__1;
x_2 = l_Std_BitVec_reduceAnd___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAnd(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceAnd___closed__3;
x_11 = lean_unsigned_to_nat(6u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceAnd___lambda__2(x_1, x_10, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAnd___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; 
x_13 = l_Std_BitVec_reduceAnd___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
return x_13;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceAnd", 9);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceAnd___closed__3;
x_2 = lean_unsigned_to_nat(6u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = lean_unsigned_to_nat(10u);
x_2 = lean_mk_empty_array_with_capacity(x_1);
return x_2;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__5;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__7;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__8;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__10() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__9;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__11() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__10;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__12() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__11;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__13() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__12;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__14() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__13;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__15() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceAnd), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__14;
x_4 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__15;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1473_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__15;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1475_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__15;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceOr___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; lean_object* x_14; lean_object* x_15; lean_object* x_16; lean_object* x_17; uint32_t x_18; uint8_t x_19; lean_object* x_20; lean_object* x_21; lean_object* x_22; 
x_13 = lean_ctor_get(x_1, 1);
x_14 = lean_nat_lor(x_2, x_13);
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_3);
lean_ctor_set(x_15, 1, x_14);
x_16 = l_Std_BitVec_Literal_toExpr(x_15);
x_17 = lean_box(0);
x_18 = 0;
x_19 = 1;
x_20 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_20, 0, x_16);
lean_ctor_set(x_20, 1, x_17);
lean_ctor_set_uint32(x_20, sizeof(void*)*2, x_18);
lean_ctor_set_uint8(x_20, sizeof(void*)*2 + 4, x_19);
x_21 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_21, 0, x_20);
x_22 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_22, 0, x_21);
lean_ctor_set(x_22, 1, x_12);
return x_22;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceOr___lambda__2(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; lean_object* x_14; 
lean_dec(x_3);
lean_inc(x_1);
x_12 = l_Lean_Expr_appFn_x21(x_1);
x_13 = l_Lean_Expr_appArg_x21(x_12);
lean_dec(x_12);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_14 = l_Std_BitVec_fromExpr_x3f(x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_14) == 0)
{
lean_object* x_15; 
x_15 = lean_ctor_get(x_14, 0);
lean_inc(x_15);
if (lean_obj_tag(x_15) == 0)
{
uint8_t x_16; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_16 = !lean_is_exclusive(x_14);
if (x_16 == 0)
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_ctor_get(x_14, 0);
lean_dec(x_17);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_14, 0, x_18);
return x_14;
}
else
{
lean_object* x_19; lean_object* x_20; lean_object* x_21; 
x_19 = lean_ctor_get(x_14, 1);
lean_inc(x_19);
lean_dec(x_14);
x_20 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_21 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_21, 0, x_20);
lean_ctor_set(x_21, 1, x_19);
return x_21;
}
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_22 = lean_ctor_get(x_14, 1);
lean_inc(x_22);
lean_dec(x_14);
x_23 = lean_ctor_get(x_15, 0);
lean_inc(x_23);
lean_dec(x_15);
x_24 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_25 = l_Std_BitVec_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_22);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint8_t x_40; 
x_34 = lean_ctor_get(x_25, 1);
x_35 = lean_ctor_get(x_25, 0);
lean_dec(x_35);
x_36 = lean_ctor_get(x_26, 0);
lean_inc(x_36);
lean_dec(x_26);
x_37 = lean_ctor_get(x_23, 0);
lean_inc(x_37);
x_38 = lean_ctor_get(x_23, 1);
lean_inc(x_38);
lean_dec(x_23);
x_39 = lean_ctor_get(x_36, 0);
lean_inc(x_39);
x_40 = lean_nat_dec_eq(x_37, x_39);
if (x_40 == 0)
{
lean_object* x_41; 
lean_dec(x_39);
lean_dec(x_38);
lean_dec(x_37);
lean_dec(x_36);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_41 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_41);
return x_25;
}
else
{
lean_object* x_42; lean_object* x_43; lean_object* x_44; uint8_t x_45; 
lean_free_object(x_25);
x_42 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_43 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_42, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_34);
x_44 = lean_ctor_get(x_43, 0);
lean_inc(x_44);
x_45 = lean_unbox(x_44);
lean_dec(x_44);
if (x_45 == 0)
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; 
lean_dec(x_39);
lean_dec(x_2);
x_46 = lean_ctor_get(x_43, 1);
lean_inc(x_46);
lean_dec(x_43);
x_47 = lean_box(0);
x_48 = l_Std_BitVec_reduceOr___lambda__1(x_36, x_38, x_37, x_47, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_46);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_38);
lean_dec(x_36);
return x_48;
}
else
{
lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; lean_object* x_69; lean_object* x_70; lean_object* x_71; lean_object* x_72; lean_object* x_73; lean_object* x_74; lean_object* x_75; lean_object* x_76; lean_object* x_77; lean_object* x_78; lean_object* x_79; lean_object* x_80; lean_object* x_81; lean_object* x_82; lean_object* x_83; lean_object* x_84; lean_object* x_85; lean_object* x_86; lean_object* x_87; lean_object* x_88; lean_object* x_89; 
x_49 = lean_ctor_get(x_43, 1);
lean_inc(x_49);
lean_dec(x_43);
x_50 = l_Lean_MessageData_ofName(x_2);
x_51 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_52 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_52, 0, x_51);
lean_ctor_set(x_52, 1, x_50);
x_53 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_54 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_54, 0, x_52);
lean_ctor_set(x_54, 1, x_53);
lean_inc(x_38);
x_55 = l_Std_BitVec_toHex(x_37, x_38);
x_56 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_56, 0, x_55);
x_57 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_58 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_58, 0, x_57);
lean_ctor_set(x_58, 1, x_56);
x_59 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_60 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_60, 0, x_58);
lean_ctor_set(x_60, 1, x_59);
lean_inc(x_37);
x_61 = l_Nat_repr(x_37);
x_62 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_62, 0, x_61);
x_63 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_63, 0, x_60);
lean_ctor_set(x_63, 1, x_62);
x_64 = l_Std_Format_defWidth;
x_65 = lean_unsigned_to_nat(0u);
x_66 = lean_format_pretty(x_63, x_64, x_65, x_65);
x_67 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_68, 0, x_67);
x_69 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_69, 0, x_54);
lean_ctor_set(x_69, 1, x_68);
x_70 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_71 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
x_72 = lean_ctor_get(x_36, 1);
lean_inc(x_72);
x_73 = l_Std_BitVec_toHex(x_39, x_72);
x_74 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_74, 0, x_73);
x_75 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_75, 0, x_57);
lean_ctor_set(x_75, 1, x_74);
x_76 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_76, 0, x_75);
lean_ctor_set(x_76, 1, x_59);
x_77 = l_Nat_repr(x_39);
x_78 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_78, 0, x_77);
x_79 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_79, 0, x_76);
lean_ctor_set(x_79, 1, x_78);
x_80 = lean_format_pretty(x_79, x_64, x_65, x_65);
x_81 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_81, 0, x_80);
x_82 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_82, 0, x_81);
x_83 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_83, 0, x_71);
lean_ctor_set(x_83, 1, x_82);
x_84 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_85 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_85, 0, x_83);
lean_ctor_set(x_85, 1, x_84);
x_86 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_42, x_85, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_49);
x_87 = lean_ctor_get(x_86, 0);
lean_inc(x_87);
x_88 = lean_ctor_get(x_86, 1);
lean_inc(x_88);
lean_dec(x_86);
x_89 = l_Std_BitVec_reduceOr___lambda__1(x_36, x_38, x_37, x_87, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_88);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_87);
lean_dec(x_38);
lean_dec(x_36);
return x_89;
}
}
}
else
{
lean_object* x_90; lean_object* x_91; lean_object* x_92; lean_object* x_93; lean_object* x_94; uint8_t x_95; 
x_90 = lean_ctor_get(x_25, 1);
lean_inc(x_90);
lean_dec(x_25);
x_91 = lean_ctor_get(x_26, 0);
lean_inc(x_91);
lean_dec(x_26);
x_92 = lean_ctor_get(x_23, 0);
lean_inc(x_92);
x_93 = lean_ctor_get(x_23, 1);
lean_inc(x_93);
lean_dec(x_23);
x_94 = lean_ctor_get(x_91, 0);
lean_inc(x_94);
x_95 = lean_nat_dec_eq(x_92, x_94);
if (x_95 == 0)
{
lean_object* x_96; lean_object* x_97; 
lean_dec(x_94);
lean_dec(x_93);
lean_dec(x_92);
lean_dec(x_91);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_96 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_97 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_97, 0, x_96);
lean_ctor_set(x_97, 1, x_90);
return x_97;
}
else
{
lean_object* x_98; lean_object* x_99; lean_object* x_100; uint8_t x_101; 
x_98 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_99 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_98, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_90);
x_100 = lean_ctor_get(x_99, 0);
lean_inc(x_100);
x_101 = lean_unbox(x_100);
lean_dec(x_100);
if (x_101 == 0)
{
lean_object* x_102; lean_object* x_103; lean_object* x_104; 
lean_dec(x_94);
lean_dec(x_2);
x_102 = lean_ctor_get(x_99, 1);
lean_inc(x_102);
lean_dec(x_99);
x_103 = lean_box(0);
x_104 = l_Std_BitVec_reduceOr___lambda__1(x_91, x_93, x_92, x_103, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_102);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_93);
lean_dec(x_91);
return x_104;
}
else
{
lean_object* x_105; lean_object* x_106; lean_object* x_107; lean_object* x_108; lean_object* x_109; lean_object* x_110; lean_object* x_111; lean_object* x_112; lean_object* x_113; lean_object* x_114; lean_object* x_115; lean_object* x_116; lean_object* x_117; lean_object* x_118; lean_object* x_119; lean_object* x_120; lean_object* x_121; lean_object* x_122; lean_object* x_123; lean_object* x_124; lean_object* x_125; lean_object* x_126; lean_object* x_127; lean_object* x_128; lean_object* x_129; lean_object* x_130; lean_object* x_131; lean_object* x_132; lean_object* x_133; lean_object* x_134; lean_object* x_135; lean_object* x_136; lean_object* x_137; lean_object* x_138; lean_object* x_139; lean_object* x_140; lean_object* x_141; lean_object* x_142; lean_object* x_143; lean_object* x_144; lean_object* x_145; 
x_105 = lean_ctor_get(x_99, 1);
lean_inc(x_105);
lean_dec(x_99);
x_106 = l_Lean_MessageData_ofName(x_2);
x_107 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_108 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_108, 0, x_107);
lean_ctor_set(x_108, 1, x_106);
x_109 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_110 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_110, 0, x_108);
lean_ctor_set(x_110, 1, x_109);
lean_inc(x_93);
x_111 = l_Std_BitVec_toHex(x_92, x_93);
x_112 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_112, 0, x_111);
x_113 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_114 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_114, 0, x_113);
lean_ctor_set(x_114, 1, x_112);
x_115 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_116 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_116, 0, x_114);
lean_ctor_set(x_116, 1, x_115);
lean_inc(x_92);
x_117 = l_Nat_repr(x_92);
x_118 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_118, 0, x_117);
x_119 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_119, 0, x_116);
lean_ctor_set(x_119, 1, x_118);
x_120 = l_Std_Format_defWidth;
x_121 = lean_unsigned_to_nat(0u);
x_122 = lean_format_pretty(x_119, x_120, x_121, x_121);
x_123 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_123, 0, x_122);
x_124 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_124, 0, x_123);
x_125 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_125, 0, x_110);
lean_ctor_set(x_125, 1, x_124);
x_126 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_127 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_127, 0, x_125);
lean_ctor_set(x_127, 1, x_126);
x_128 = lean_ctor_get(x_91, 1);
lean_inc(x_128);
x_129 = l_Std_BitVec_toHex(x_94, x_128);
x_130 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_130, 0, x_129);
x_131 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_131, 0, x_113);
lean_ctor_set(x_131, 1, x_130);
x_132 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_132, 0, x_131);
lean_ctor_set(x_132, 1, x_115);
x_133 = l_Nat_repr(x_94);
x_134 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_134, 0, x_133);
x_135 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_135, 0, x_132);
lean_ctor_set(x_135, 1, x_134);
x_136 = lean_format_pretty(x_135, x_120, x_121, x_121);
x_137 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_137, 0, x_136);
x_138 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_138, 0, x_137);
x_139 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_139, 0, x_127);
lean_ctor_set(x_139, 1, x_138);
x_140 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_141 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_141, 0, x_139);
lean_ctor_set(x_141, 1, x_140);
x_142 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_98, x_141, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_105);
x_143 = lean_ctor_get(x_142, 0);
lean_inc(x_143);
x_144 = lean_ctor_get(x_142, 1);
lean_inc(x_144);
lean_dec(x_142);
x_145 = l_Std_BitVec_reduceOr___lambda__1(x_91, x_93, x_92, x_143, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_144);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_143);
lean_dec(x_93);
lean_dec(x_91);
return x_145;
}
}
}
}
}
else
{
uint8_t x_146; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_146 = !lean_is_exclusive(x_25);
if (x_146 == 0)
{
return x_25;
}
else
{
lean_object* x_147; lean_object* x_148; lean_object* x_149; 
x_147 = lean_ctor_get(x_25, 0);
x_148 = lean_ctor_get(x_25, 1);
lean_inc(x_148);
lean_inc(x_147);
lean_dec(x_25);
x_149 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_149, 0, x_147);
lean_ctor_set(x_149, 1, x_148);
return x_149;
}
}
}
}
else
{
uint8_t x_150; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_150 = !lean_is_exclusive(x_14);
if (x_150 == 0)
{
return x_14;
}
else
{
lean_object* x_151; lean_object* x_152; lean_object* x_153; 
x_151 = lean_ctor_get(x_14, 0);
x_152 = lean_ctor_get(x_14, 1);
lean_inc(x_152);
lean_inc(x_151);
lean_dec(x_14);
x_153 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_153, 0, x_151);
lean_ctor_set(x_153, 1, x_152);
return x_153;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceOr___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("HOr", 3);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceOr___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("hOr", 3);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceOr___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceOr___closed__1;
x_2 = l_Std_BitVec_reduceOr___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceOr(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceOr___closed__3;
x_11 = lean_unsigned_to_nat(6u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceOr___lambda__2(x_1, x_10, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceOr___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; 
x_13 = l_Std_BitVec_reduceOr___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
return x_13;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceOr", 8);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceOr___closed__3;
x_2 = lean_unsigned_to_nat(6u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__6;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__8;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__10() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__9;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__11() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__10;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__12() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__11;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__13() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__12;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__14() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceOr), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__13;
x_4 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__14;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1515_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__14;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1517_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__14;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceXOr___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; lean_object* x_14; lean_object* x_15; lean_object* x_16; lean_object* x_17; uint32_t x_18; uint8_t x_19; lean_object* x_20; lean_object* x_21; lean_object* x_22; 
x_13 = lean_ctor_get(x_1, 1);
x_14 = lean_nat_lxor(x_2, x_13);
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_3);
lean_ctor_set(x_15, 1, x_14);
x_16 = l_Std_BitVec_Literal_toExpr(x_15);
x_17 = lean_box(0);
x_18 = 0;
x_19 = 1;
x_20 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_20, 0, x_16);
lean_ctor_set(x_20, 1, x_17);
lean_ctor_set_uint32(x_20, sizeof(void*)*2, x_18);
lean_ctor_set_uint8(x_20, sizeof(void*)*2 + 4, x_19);
x_21 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_21, 0, x_20);
x_22 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_22, 0, x_21);
lean_ctor_set(x_22, 1, x_12);
return x_22;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceXOr___lambda__2(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; lean_object* x_14; 
lean_dec(x_3);
lean_inc(x_1);
x_12 = l_Lean_Expr_appFn_x21(x_1);
x_13 = l_Lean_Expr_appArg_x21(x_12);
lean_dec(x_12);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_14 = l_Std_BitVec_fromExpr_x3f(x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_14) == 0)
{
lean_object* x_15; 
x_15 = lean_ctor_get(x_14, 0);
lean_inc(x_15);
if (lean_obj_tag(x_15) == 0)
{
uint8_t x_16; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_16 = !lean_is_exclusive(x_14);
if (x_16 == 0)
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_ctor_get(x_14, 0);
lean_dec(x_17);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_14, 0, x_18);
return x_14;
}
else
{
lean_object* x_19; lean_object* x_20; lean_object* x_21; 
x_19 = lean_ctor_get(x_14, 1);
lean_inc(x_19);
lean_dec(x_14);
x_20 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_21 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_21, 0, x_20);
lean_ctor_set(x_21, 1, x_19);
return x_21;
}
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_22 = lean_ctor_get(x_14, 1);
lean_inc(x_22);
lean_dec(x_14);
x_23 = lean_ctor_get(x_15, 0);
lean_inc(x_23);
lean_dec(x_15);
x_24 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_25 = l_Std_BitVec_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_22);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint8_t x_40; 
x_34 = lean_ctor_get(x_25, 1);
x_35 = lean_ctor_get(x_25, 0);
lean_dec(x_35);
x_36 = lean_ctor_get(x_26, 0);
lean_inc(x_36);
lean_dec(x_26);
x_37 = lean_ctor_get(x_23, 0);
lean_inc(x_37);
x_38 = lean_ctor_get(x_23, 1);
lean_inc(x_38);
lean_dec(x_23);
x_39 = lean_ctor_get(x_36, 0);
lean_inc(x_39);
x_40 = lean_nat_dec_eq(x_37, x_39);
if (x_40 == 0)
{
lean_object* x_41; 
lean_dec(x_39);
lean_dec(x_38);
lean_dec(x_37);
lean_dec(x_36);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_41 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_41);
return x_25;
}
else
{
lean_object* x_42; lean_object* x_43; lean_object* x_44; uint8_t x_45; 
lean_free_object(x_25);
x_42 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_43 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_42, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_34);
x_44 = lean_ctor_get(x_43, 0);
lean_inc(x_44);
x_45 = lean_unbox(x_44);
lean_dec(x_44);
if (x_45 == 0)
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; 
lean_dec(x_39);
lean_dec(x_2);
x_46 = lean_ctor_get(x_43, 1);
lean_inc(x_46);
lean_dec(x_43);
x_47 = lean_box(0);
x_48 = l_Std_BitVec_reduceXOr___lambda__1(x_36, x_38, x_37, x_47, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_46);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_38);
lean_dec(x_36);
return x_48;
}
else
{
lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; lean_object* x_69; lean_object* x_70; lean_object* x_71; lean_object* x_72; lean_object* x_73; lean_object* x_74; lean_object* x_75; lean_object* x_76; lean_object* x_77; lean_object* x_78; lean_object* x_79; lean_object* x_80; lean_object* x_81; lean_object* x_82; lean_object* x_83; lean_object* x_84; lean_object* x_85; lean_object* x_86; lean_object* x_87; lean_object* x_88; lean_object* x_89; 
x_49 = lean_ctor_get(x_43, 1);
lean_inc(x_49);
lean_dec(x_43);
x_50 = l_Lean_MessageData_ofName(x_2);
x_51 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_52 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_52, 0, x_51);
lean_ctor_set(x_52, 1, x_50);
x_53 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_54 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_54, 0, x_52);
lean_ctor_set(x_54, 1, x_53);
lean_inc(x_38);
x_55 = l_Std_BitVec_toHex(x_37, x_38);
x_56 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_56, 0, x_55);
x_57 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_58 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_58, 0, x_57);
lean_ctor_set(x_58, 1, x_56);
x_59 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_60 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_60, 0, x_58);
lean_ctor_set(x_60, 1, x_59);
lean_inc(x_37);
x_61 = l_Nat_repr(x_37);
x_62 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_62, 0, x_61);
x_63 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_63, 0, x_60);
lean_ctor_set(x_63, 1, x_62);
x_64 = l_Std_Format_defWidth;
x_65 = lean_unsigned_to_nat(0u);
x_66 = lean_format_pretty(x_63, x_64, x_65, x_65);
x_67 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_68, 0, x_67);
x_69 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_69, 0, x_54);
lean_ctor_set(x_69, 1, x_68);
x_70 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_71 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
x_72 = lean_ctor_get(x_36, 1);
lean_inc(x_72);
x_73 = l_Std_BitVec_toHex(x_39, x_72);
x_74 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_74, 0, x_73);
x_75 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_75, 0, x_57);
lean_ctor_set(x_75, 1, x_74);
x_76 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_76, 0, x_75);
lean_ctor_set(x_76, 1, x_59);
x_77 = l_Nat_repr(x_39);
x_78 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_78, 0, x_77);
x_79 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_79, 0, x_76);
lean_ctor_set(x_79, 1, x_78);
x_80 = lean_format_pretty(x_79, x_64, x_65, x_65);
x_81 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_81, 0, x_80);
x_82 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_82, 0, x_81);
x_83 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_83, 0, x_71);
lean_ctor_set(x_83, 1, x_82);
x_84 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_85 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_85, 0, x_83);
lean_ctor_set(x_85, 1, x_84);
x_86 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_42, x_85, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_49);
x_87 = lean_ctor_get(x_86, 0);
lean_inc(x_87);
x_88 = lean_ctor_get(x_86, 1);
lean_inc(x_88);
lean_dec(x_86);
x_89 = l_Std_BitVec_reduceXOr___lambda__1(x_36, x_38, x_37, x_87, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_88);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_87);
lean_dec(x_38);
lean_dec(x_36);
return x_89;
}
}
}
else
{
lean_object* x_90; lean_object* x_91; lean_object* x_92; lean_object* x_93; lean_object* x_94; uint8_t x_95; 
x_90 = lean_ctor_get(x_25, 1);
lean_inc(x_90);
lean_dec(x_25);
x_91 = lean_ctor_get(x_26, 0);
lean_inc(x_91);
lean_dec(x_26);
x_92 = lean_ctor_get(x_23, 0);
lean_inc(x_92);
x_93 = lean_ctor_get(x_23, 1);
lean_inc(x_93);
lean_dec(x_23);
x_94 = lean_ctor_get(x_91, 0);
lean_inc(x_94);
x_95 = lean_nat_dec_eq(x_92, x_94);
if (x_95 == 0)
{
lean_object* x_96; lean_object* x_97; 
lean_dec(x_94);
lean_dec(x_93);
lean_dec(x_92);
lean_dec(x_91);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_96 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_97 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_97, 0, x_96);
lean_ctor_set(x_97, 1, x_90);
return x_97;
}
else
{
lean_object* x_98; lean_object* x_99; lean_object* x_100; uint8_t x_101; 
x_98 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_99 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_98, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_90);
x_100 = lean_ctor_get(x_99, 0);
lean_inc(x_100);
x_101 = lean_unbox(x_100);
lean_dec(x_100);
if (x_101 == 0)
{
lean_object* x_102; lean_object* x_103; lean_object* x_104; 
lean_dec(x_94);
lean_dec(x_2);
x_102 = lean_ctor_get(x_99, 1);
lean_inc(x_102);
lean_dec(x_99);
x_103 = lean_box(0);
x_104 = l_Std_BitVec_reduceXOr___lambda__1(x_91, x_93, x_92, x_103, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_102);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_93);
lean_dec(x_91);
return x_104;
}
else
{
lean_object* x_105; lean_object* x_106; lean_object* x_107; lean_object* x_108; lean_object* x_109; lean_object* x_110; lean_object* x_111; lean_object* x_112; lean_object* x_113; lean_object* x_114; lean_object* x_115; lean_object* x_116; lean_object* x_117; lean_object* x_118; lean_object* x_119; lean_object* x_120; lean_object* x_121; lean_object* x_122; lean_object* x_123; lean_object* x_124; lean_object* x_125; lean_object* x_126; lean_object* x_127; lean_object* x_128; lean_object* x_129; lean_object* x_130; lean_object* x_131; lean_object* x_132; lean_object* x_133; lean_object* x_134; lean_object* x_135; lean_object* x_136; lean_object* x_137; lean_object* x_138; lean_object* x_139; lean_object* x_140; lean_object* x_141; lean_object* x_142; lean_object* x_143; lean_object* x_144; lean_object* x_145; 
x_105 = lean_ctor_get(x_99, 1);
lean_inc(x_105);
lean_dec(x_99);
x_106 = l_Lean_MessageData_ofName(x_2);
x_107 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_108 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_108, 0, x_107);
lean_ctor_set(x_108, 1, x_106);
x_109 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_110 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_110, 0, x_108);
lean_ctor_set(x_110, 1, x_109);
lean_inc(x_93);
x_111 = l_Std_BitVec_toHex(x_92, x_93);
x_112 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_112, 0, x_111);
x_113 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_114 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_114, 0, x_113);
lean_ctor_set(x_114, 1, x_112);
x_115 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_116 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_116, 0, x_114);
lean_ctor_set(x_116, 1, x_115);
lean_inc(x_92);
x_117 = l_Nat_repr(x_92);
x_118 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_118, 0, x_117);
x_119 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_119, 0, x_116);
lean_ctor_set(x_119, 1, x_118);
x_120 = l_Std_Format_defWidth;
x_121 = lean_unsigned_to_nat(0u);
x_122 = lean_format_pretty(x_119, x_120, x_121, x_121);
x_123 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_123, 0, x_122);
x_124 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_124, 0, x_123);
x_125 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_125, 0, x_110);
lean_ctor_set(x_125, 1, x_124);
x_126 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_127 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_127, 0, x_125);
lean_ctor_set(x_127, 1, x_126);
x_128 = lean_ctor_get(x_91, 1);
lean_inc(x_128);
x_129 = l_Std_BitVec_toHex(x_94, x_128);
x_130 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_130, 0, x_129);
x_131 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_131, 0, x_113);
lean_ctor_set(x_131, 1, x_130);
x_132 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_132, 0, x_131);
lean_ctor_set(x_132, 1, x_115);
x_133 = l_Nat_repr(x_94);
x_134 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_134, 0, x_133);
x_135 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_135, 0, x_132);
lean_ctor_set(x_135, 1, x_134);
x_136 = lean_format_pretty(x_135, x_120, x_121, x_121);
x_137 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_137, 0, x_136);
x_138 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_138, 0, x_137);
x_139 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_139, 0, x_127);
lean_ctor_set(x_139, 1, x_138);
x_140 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_141 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_141, 0, x_139);
lean_ctor_set(x_141, 1, x_140);
x_142 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_98, x_141, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_105);
x_143 = lean_ctor_get(x_142, 0);
lean_inc(x_143);
x_144 = lean_ctor_get(x_142, 1);
lean_inc(x_144);
lean_dec(x_142);
x_145 = l_Std_BitVec_reduceXOr___lambda__1(x_91, x_93, x_92, x_143, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_144);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_143);
lean_dec(x_93);
lean_dec(x_91);
return x_145;
}
}
}
}
}
else
{
uint8_t x_146; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_146 = !lean_is_exclusive(x_25);
if (x_146 == 0)
{
return x_25;
}
else
{
lean_object* x_147; lean_object* x_148; lean_object* x_149; 
x_147 = lean_ctor_get(x_25, 0);
x_148 = lean_ctor_get(x_25, 1);
lean_inc(x_148);
lean_inc(x_147);
lean_dec(x_25);
x_149 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_149, 0, x_147);
lean_ctor_set(x_149, 1, x_148);
return x_149;
}
}
}
}
else
{
uint8_t x_150; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_150 = !lean_is_exclusive(x_14);
if (x_150 == 0)
{
return x_14;
}
else
{
lean_object* x_151; lean_object* x_152; lean_object* x_153; 
x_151 = lean_ctor_get(x_14, 0);
x_152 = lean_ctor_get(x_14, 1);
lean_inc(x_152);
lean_inc(x_151);
lean_dec(x_14);
x_153 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_153, 0, x_151);
lean_ctor_set(x_153, 1, x_152);
return x_153;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceXOr___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("HXor", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceXOr___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("hXor", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceXOr___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceXOr___closed__1;
x_2 = l_Std_BitVec_reduceXOr___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceXOr(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceXOr___closed__3;
x_11 = lean_unsigned_to_nat(6u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceXOr___lambda__2(x_1, x_10, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceXOr___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; 
x_13 = l_Std_BitVec_reduceXOr___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
return x_13;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceXOr", 9);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceXOr___closed__3;
x_2 = lean_unsigned_to_nat(6u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__6;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__8;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__10() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__9;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__11() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__10;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__12() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__11;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__13() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__12;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__14() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceXOr), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__13;
x_4 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__14;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1557_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__14;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1559_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__14;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAdd___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; lean_object* x_14; lean_object* x_15; lean_object* x_16; lean_object* x_17; uint32_t x_18; uint8_t x_19; lean_object* x_20; lean_object* x_21; lean_object* x_22; 
x_13 = lean_ctor_get(x_1, 1);
x_14 = l_Std_BitVec_add(x_2, x_3, x_13);
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_2);
lean_ctor_set(x_15, 1, x_14);
x_16 = l_Std_BitVec_Literal_toExpr(x_15);
x_17 = lean_box(0);
x_18 = 0;
x_19 = 1;
x_20 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_20, 0, x_16);
lean_ctor_set(x_20, 1, x_17);
lean_ctor_set_uint32(x_20, sizeof(void*)*2, x_18);
lean_ctor_set_uint8(x_20, sizeof(void*)*2 + 4, x_19);
x_21 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_21, 0, x_20);
x_22 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_22, 0, x_21);
lean_ctor_set(x_22, 1, x_12);
return x_22;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAdd___lambda__2(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; lean_object* x_14; 
lean_dec(x_3);
lean_inc(x_1);
x_12 = l_Lean_Expr_appFn_x21(x_1);
x_13 = l_Lean_Expr_appArg_x21(x_12);
lean_dec(x_12);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_14 = l_Std_BitVec_fromExpr_x3f(x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_14) == 0)
{
lean_object* x_15; 
x_15 = lean_ctor_get(x_14, 0);
lean_inc(x_15);
if (lean_obj_tag(x_15) == 0)
{
uint8_t x_16; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_16 = !lean_is_exclusive(x_14);
if (x_16 == 0)
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_ctor_get(x_14, 0);
lean_dec(x_17);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_14, 0, x_18);
return x_14;
}
else
{
lean_object* x_19; lean_object* x_20; lean_object* x_21; 
x_19 = lean_ctor_get(x_14, 1);
lean_inc(x_19);
lean_dec(x_14);
x_20 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_21 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_21, 0, x_20);
lean_ctor_set(x_21, 1, x_19);
return x_21;
}
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_22 = lean_ctor_get(x_14, 1);
lean_inc(x_22);
lean_dec(x_14);
x_23 = lean_ctor_get(x_15, 0);
lean_inc(x_23);
lean_dec(x_15);
x_24 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_25 = l_Std_BitVec_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_22);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint8_t x_40; 
x_34 = lean_ctor_get(x_25, 1);
x_35 = lean_ctor_get(x_25, 0);
lean_dec(x_35);
x_36 = lean_ctor_get(x_26, 0);
lean_inc(x_36);
lean_dec(x_26);
x_37 = lean_ctor_get(x_23, 0);
lean_inc(x_37);
x_38 = lean_ctor_get(x_23, 1);
lean_inc(x_38);
lean_dec(x_23);
x_39 = lean_ctor_get(x_36, 0);
lean_inc(x_39);
x_40 = lean_nat_dec_eq(x_37, x_39);
if (x_40 == 0)
{
lean_object* x_41; 
lean_dec(x_39);
lean_dec(x_38);
lean_dec(x_37);
lean_dec(x_36);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_41 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_41);
return x_25;
}
else
{
lean_object* x_42; lean_object* x_43; lean_object* x_44; uint8_t x_45; 
lean_free_object(x_25);
x_42 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_43 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_42, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_34);
x_44 = lean_ctor_get(x_43, 0);
lean_inc(x_44);
x_45 = lean_unbox(x_44);
lean_dec(x_44);
if (x_45 == 0)
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; 
lean_dec(x_39);
lean_dec(x_2);
x_46 = lean_ctor_get(x_43, 1);
lean_inc(x_46);
lean_dec(x_43);
x_47 = lean_box(0);
x_48 = l_Std_BitVec_reduceAdd___lambda__1(x_36, x_37, x_38, x_47, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_46);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_38);
lean_dec(x_36);
return x_48;
}
else
{
lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; lean_object* x_69; lean_object* x_70; lean_object* x_71; lean_object* x_72; lean_object* x_73; lean_object* x_74; lean_object* x_75; lean_object* x_76; lean_object* x_77; lean_object* x_78; lean_object* x_79; lean_object* x_80; lean_object* x_81; lean_object* x_82; lean_object* x_83; lean_object* x_84; lean_object* x_85; lean_object* x_86; lean_object* x_87; lean_object* x_88; lean_object* x_89; 
x_49 = lean_ctor_get(x_43, 1);
lean_inc(x_49);
lean_dec(x_43);
x_50 = l_Lean_MessageData_ofName(x_2);
x_51 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_52 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_52, 0, x_51);
lean_ctor_set(x_52, 1, x_50);
x_53 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_54 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_54, 0, x_52);
lean_ctor_set(x_54, 1, x_53);
lean_inc(x_38);
x_55 = l_Std_BitVec_toHex(x_37, x_38);
x_56 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_56, 0, x_55);
x_57 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_58 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_58, 0, x_57);
lean_ctor_set(x_58, 1, x_56);
x_59 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_60 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_60, 0, x_58);
lean_ctor_set(x_60, 1, x_59);
lean_inc(x_37);
x_61 = l_Nat_repr(x_37);
x_62 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_62, 0, x_61);
x_63 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_63, 0, x_60);
lean_ctor_set(x_63, 1, x_62);
x_64 = l_Std_Format_defWidth;
x_65 = lean_unsigned_to_nat(0u);
x_66 = lean_format_pretty(x_63, x_64, x_65, x_65);
x_67 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_68, 0, x_67);
x_69 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_69, 0, x_54);
lean_ctor_set(x_69, 1, x_68);
x_70 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_71 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
x_72 = lean_ctor_get(x_36, 1);
lean_inc(x_72);
x_73 = l_Std_BitVec_toHex(x_39, x_72);
x_74 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_74, 0, x_73);
x_75 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_75, 0, x_57);
lean_ctor_set(x_75, 1, x_74);
x_76 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_76, 0, x_75);
lean_ctor_set(x_76, 1, x_59);
x_77 = l_Nat_repr(x_39);
x_78 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_78, 0, x_77);
x_79 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_79, 0, x_76);
lean_ctor_set(x_79, 1, x_78);
x_80 = lean_format_pretty(x_79, x_64, x_65, x_65);
x_81 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_81, 0, x_80);
x_82 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_82, 0, x_81);
x_83 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_83, 0, x_71);
lean_ctor_set(x_83, 1, x_82);
x_84 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_85 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_85, 0, x_83);
lean_ctor_set(x_85, 1, x_84);
x_86 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_42, x_85, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_49);
x_87 = lean_ctor_get(x_86, 0);
lean_inc(x_87);
x_88 = lean_ctor_get(x_86, 1);
lean_inc(x_88);
lean_dec(x_86);
x_89 = l_Std_BitVec_reduceAdd___lambda__1(x_36, x_37, x_38, x_87, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_88);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_87);
lean_dec(x_38);
lean_dec(x_36);
return x_89;
}
}
}
else
{
lean_object* x_90; lean_object* x_91; lean_object* x_92; lean_object* x_93; lean_object* x_94; uint8_t x_95; 
x_90 = lean_ctor_get(x_25, 1);
lean_inc(x_90);
lean_dec(x_25);
x_91 = lean_ctor_get(x_26, 0);
lean_inc(x_91);
lean_dec(x_26);
x_92 = lean_ctor_get(x_23, 0);
lean_inc(x_92);
x_93 = lean_ctor_get(x_23, 1);
lean_inc(x_93);
lean_dec(x_23);
x_94 = lean_ctor_get(x_91, 0);
lean_inc(x_94);
x_95 = lean_nat_dec_eq(x_92, x_94);
if (x_95 == 0)
{
lean_object* x_96; lean_object* x_97; 
lean_dec(x_94);
lean_dec(x_93);
lean_dec(x_92);
lean_dec(x_91);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_96 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_97 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_97, 0, x_96);
lean_ctor_set(x_97, 1, x_90);
return x_97;
}
else
{
lean_object* x_98; lean_object* x_99; lean_object* x_100; uint8_t x_101; 
x_98 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_99 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_98, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_90);
x_100 = lean_ctor_get(x_99, 0);
lean_inc(x_100);
x_101 = lean_unbox(x_100);
lean_dec(x_100);
if (x_101 == 0)
{
lean_object* x_102; lean_object* x_103; lean_object* x_104; 
lean_dec(x_94);
lean_dec(x_2);
x_102 = lean_ctor_get(x_99, 1);
lean_inc(x_102);
lean_dec(x_99);
x_103 = lean_box(0);
x_104 = l_Std_BitVec_reduceAdd___lambda__1(x_91, x_92, x_93, x_103, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_102);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_93);
lean_dec(x_91);
return x_104;
}
else
{
lean_object* x_105; lean_object* x_106; lean_object* x_107; lean_object* x_108; lean_object* x_109; lean_object* x_110; lean_object* x_111; lean_object* x_112; lean_object* x_113; lean_object* x_114; lean_object* x_115; lean_object* x_116; lean_object* x_117; lean_object* x_118; lean_object* x_119; lean_object* x_120; lean_object* x_121; lean_object* x_122; lean_object* x_123; lean_object* x_124; lean_object* x_125; lean_object* x_126; lean_object* x_127; lean_object* x_128; lean_object* x_129; lean_object* x_130; lean_object* x_131; lean_object* x_132; lean_object* x_133; lean_object* x_134; lean_object* x_135; lean_object* x_136; lean_object* x_137; lean_object* x_138; lean_object* x_139; lean_object* x_140; lean_object* x_141; lean_object* x_142; lean_object* x_143; lean_object* x_144; lean_object* x_145; 
x_105 = lean_ctor_get(x_99, 1);
lean_inc(x_105);
lean_dec(x_99);
x_106 = l_Lean_MessageData_ofName(x_2);
x_107 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_108 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_108, 0, x_107);
lean_ctor_set(x_108, 1, x_106);
x_109 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_110 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_110, 0, x_108);
lean_ctor_set(x_110, 1, x_109);
lean_inc(x_93);
x_111 = l_Std_BitVec_toHex(x_92, x_93);
x_112 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_112, 0, x_111);
x_113 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_114 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_114, 0, x_113);
lean_ctor_set(x_114, 1, x_112);
x_115 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_116 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_116, 0, x_114);
lean_ctor_set(x_116, 1, x_115);
lean_inc(x_92);
x_117 = l_Nat_repr(x_92);
x_118 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_118, 0, x_117);
x_119 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_119, 0, x_116);
lean_ctor_set(x_119, 1, x_118);
x_120 = l_Std_Format_defWidth;
x_121 = lean_unsigned_to_nat(0u);
x_122 = lean_format_pretty(x_119, x_120, x_121, x_121);
x_123 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_123, 0, x_122);
x_124 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_124, 0, x_123);
x_125 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_125, 0, x_110);
lean_ctor_set(x_125, 1, x_124);
x_126 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_127 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_127, 0, x_125);
lean_ctor_set(x_127, 1, x_126);
x_128 = lean_ctor_get(x_91, 1);
lean_inc(x_128);
x_129 = l_Std_BitVec_toHex(x_94, x_128);
x_130 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_130, 0, x_129);
x_131 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_131, 0, x_113);
lean_ctor_set(x_131, 1, x_130);
x_132 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_132, 0, x_131);
lean_ctor_set(x_132, 1, x_115);
x_133 = l_Nat_repr(x_94);
x_134 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_134, 0, x_133);
x_135 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_135, 0, x_132);
lean_ctor_set(x_135, 1, x_134);
x_136 = lean_format_pretty(x_135, x_120, x_121, x_121);
x_137 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_137, 0, x_136);
x_138 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_138, 0, x_137);
x_139 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_139, 0, x_127);
lean_ctor_set(x_139, 1, x_138);
x_140 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_141 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_141, 0, x_139);
lean_ctor_set(x_141, 1, x_140);
x_142 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_98, x_141, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_105);
x_143 = lean_ctor_get(x_142, 0);
lean_inc(x_143);
x_144 = lean_ctor_get(x_142, 1);
lean_inc(x_144);
lean_dec(x_142);
x_145 = l_Std_BitVec_reduceAdd___lambda__1(x_91, x_92, x_93, x_143, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_144);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_143);
lean_dec(x_93);
lean_dec(x_91);
return x_145;
}
}
}
}
}
else
{
uint8_t x_146; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_146 = !lean_is_exclusive(x_25);
if (x_146 == 0)
{
return x_25;
}
else
{
lean_object* x_147; lean_object* x_148; lean_object* x_149; 
x_147 = lean_ctor_get(x_25, 0);
x_148 = lean_ctor_get(x_25, 1);
lean_inc(x_148);
lean_inc(x_147);
lean_dec(x_25);
x_149 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_149, 0, x_147);
lean_ctor_set(x_149, 1, x_148);
return x_149;
}
}
}
}
else
{
uint8_t x_150; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_150 = !lean_is_exclusive(x_14);
if (x_150 == 0)
{
return x_14;
}
else
{
lean_object* x_151; lean_object* x_152; lean_object* x_153; 
x_151 = lean_ctor_get(x_14, 0);
x_152 = lean_ctor_get(x_14, 1);
lean_inc(x_152);
lean_inc(x_151);
lean_dec(x_14);
x_153 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_153, 0, x_151);
lean_ctor_set(x_153, 1, x_152);
return x_153;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceAdd___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("HAdd", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceAdd___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("hAdd", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceAdd___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceAdd___closed__1;
x_2 = l_Std_BitVec_reduceAdd___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAdd(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceAdd___closed__3;
x_11 = lean_unsigned_to_nat(6u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceAdd___lambda__2(x_1, x_10, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAdd___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; 
x_13 = l_Std_BitVec_reduceAdd___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_1);
return x_13;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceAdd", 9);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceAdd___closed__3;
x_2 = lean_unsigned_to_nat(6u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__6;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__8;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__10() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__9;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__11() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__10;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__12() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__11;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__13() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__12;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__14() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceAdd), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__13;
x_4 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__14;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1599_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__14;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1601_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__14;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMul___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; lean_object* x_14; lean_object* x_15; lean_object* x_16; lean_object* x_17; uint32_t x_18; uint8_t x_19; lean_object* x_20; lean_object* x_21; lean_object* x_22; 
x_13 = lean_ctor_get(x_1, 1);
x_14 = l_Std_BitVec_mul(x_2, x_3, x_13);
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_2);
lean_ctor_set(x_15, 1, x_14);
x_16 = l_Std_BitVec_Literal_toExpr(x_15);
x_17 = lean_box(0);
x_18 = 0;
x_19 = 1;
x_20 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_20, 0, x_16);
lean_ctor_set(x_20, 1, x_17);
lean_ctor_set_uint32(x_20, sizeof(void*)*2, x_18);
lean_ctor_set_uint8(x_20, sizeof(void*)*2 + 4, x_19);
x_21 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_21, 0, x_20);
x_22 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_22, 0, x_21);
lean_ctor_set(x_22, 1, x_12);
return x_22;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMul___lambda__2(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; lean_object* x_14; 
lean_dec(x_3);
lean_inc(x_1);
x_12 = l_Lean_Expr_appFn_x21(x_1);
x_13 = l_Lean_Expr_appArg_x21(x_12);
lean_dec(x_12);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_14 = l_Std_BitVec_fromExpr_x3f(x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_14) == 0)
{
lean_object* x_15; 
x_15 = lean_ctor_get(x_14, 0);
lean_inc(x_15);
if (lean_obj_tag(x_15) == 0)
{
uint8_t x_16; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_16 = !lean_is_exclusive(x_14);
if (x_16 == 0)
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_ctor_get(x_14, 0);
lean_dec(x_17);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_14, 0, x_18);
return x_14;
}
else
{
lean_object* x_19; lean_object* x_20; lean_object* x_21; 
x_19 = lean_ctor_get(x_14, 1);
lean_inc(x_19);
lean_dec(x_14);
x_20 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_21 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_21, 0, x_20);
lean_ctor_set(x_21, 1, x_19);
return x_21;
}
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_22 = lean_ctor_get(x_14, 1);
lean_inc(x_22);
lean_dec(x_14);
x_23 = lean_ctor_get(x_15, 0);
lean_inc(x_23);
lean_dec(x_15);
x_24 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_25 = l_Std_BitVec_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_22);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint8_t x_40; 
x_34 = lean_ctor_get(x_25, 1);
x_35 = lean_ctor_get(x_25, 0);
lean_dec(x_35);
x_36 = lean_ctor_get(x_26, 0);
lean_inc(x_36);
lean_dec(x_26);
x_37 = lean_ctor_get(x_23, 0);
lean_inc(x_37);
x_38 = lean_ctor_get(x_23, 1);
lean_inc(x_38);
lean_dec(x_23);
x_39 = lean_ctor_get(x_36, 0);
lean_inc(x_39);
x_40 = lean_nat_dec_eq(x_37, x_39);
if (x_40 == 0)
{
lean_object* x_41; 
lean_dec(x_39);
lean_dec(x_38);
lean_dec(x_37);
lean_dec(x_36);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_41 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_41);
return x_25;
}
else
{
lean_object* x_42; lean_object* x_43; lean_object* x_44; uint8_t x_45; 
lean_free_object(x_25);
x_42 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_43 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_42, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_34);
x_44 = lean_ctor_get(x_43, 0);
lean_inc(x_44);
x_45 = lean_unbox(x_44);
lean_dec(x_44);
if (x_45 == 0)
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; 
lean_dec(x_39);
lean_dec(x_2);
x_46 = lean_ctor_get(x_43, 1);
lean_inc(x_46);
lean_dec(x_43);
x_47 = lean_box(0);
x_48 = l_Std_BitVec_reduceMul___lambda__1(x_36, x_37, x_38, x_47, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_46);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_38);
lean_dec(x_36);
return x_48;
}
else
{
lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; lean_object* x_69; lean_object* x_70; lean_object* x_71; lean_object* x_72; lean_object* x_73; lean_object* x_74; lean_object* x_75; lean_object* x_76; lean_object* x_77; lean_object* x_78; lean_object* x_79; lean_object* x_80; lean_object* x_81; lean_object* x_82; lean_object* x_83; lean_object* x_84; lean_object* x_85; lean_object* x_86; lean_object* x_87; lean_object* x_88; lean_object* x_89; 
x_49 = lean_ctor_get(x_43, 1);
lean_inc(x_49);
lean_dec(x_43);
x_50 = l_Lean_MessageData_ofName(x_2);
x_51 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_52 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_52, 0, x_51);
lean_ctor_set(x_52, 1, x_50);
x_53 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_54 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_54, 0, x_52);
lean_ctor_set(x_54, 1, x_53);
lean_inc(x_38);
x_55 = l_Std_BitVec_toHex(x_37, x_38);
x_56 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_56, 0, x_55);
x_57 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_58 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_58, 0, x_57);
lean_ctor_set(x_58, 1, x_56);
x_59 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_60 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_60, 0, x_58);
lean_ctor_set(x_60, 1, x_59);
lean_inc(x_37);
x_61 = l_Nat_repr(x_37);
x_62 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_62, 0, x_61);
x_63 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_63, 0, x_60);
lean_ctor_set(x_63, 1, x_62);
x_64 = l_Std_Format_defWidth;
x_65 = lean_unsigned_to_nat(0u);
x_66 = lean_format_pretty(x_63, x_64, x_65, x_65);
x_67 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_68, 0, x_67);
x_69 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_69, 0, x_54);
lean_ctor_set(x_69, 1, x_68);
x_70 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_71 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
x_72 = lean_ctor_get(x_36, 1);
lean_inc(x_72);
x_73 = l_Std_BitVec_toHex(x_39, x_72);
x_74 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_74, 0, x_73);
x_75 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_75, 0, x_57);
lean_ctor_set(x_75, 1, x_74);
x_76 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_76, 0, x_75);
lean_ctor_set(x_76, 1, x_59);
x_77 = l_Nat_repr(x_39);
x_78 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_78, 0, x_77);
x_79 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_79, 0, x_76);
lean_ctor_set(x_79, 1, x_78);
x_80 = lean_format_pretty(x_79, x_64, x_65, x_65);
x_81 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_81, 0, x_80);
x_82 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_82, 0, x_81);
x_83 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_83, 0, x_71);
lean_ctor_set(x_83, 1, x_82);
x_84 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_85 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_85, 0, x_83);
lean_ctor_set(x_85, 1, x_84);
x_86 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_42, x_85, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_49);
x_87 = lean_ctor_get(x_86, 0);
lean_inc(x_87);
x_88 = lean_ctor_get(x_86, 1);
lean_inc(x_88);
lean_dec(x_86);
x_89 = l_Std_BitVec_reduceMul___lambda__1(x_36, x_37, x_38, x_87, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_88);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_87);
lean_dec(x_38);
lean_dec(x_36);
return x_89;
}
}
}
else
{
lean_object* x_90; lean_object* x_91; lean_object* x_92; lean_object* x_93; lean_object* x_94; uint8_t x_95; 
x_90 = lean_ctor_get(x_25, 1);
lean_inc(x_90);
lean_dec(x_25);
x_91 = lean_ctor_get(x_26, 0);
lean_inc(x_91);
lean_dec(x_26);
x_92 = lean_ctor_get(x_23, 0);
lean_inc(x_92);
x_93 = lean_ctor_get(x_23, 1);
lean_inc(x_93);
lean_dec(x_23);
x_94 = lean_ctor_get(x_91, 0);
lean_inc(x_94);
x_95 = lean_nat_dec_eq(x_92, x_94);
if (x_95 == 0)
{
lean_object* x_96; lean_object* x_97; 
lean_dec(x_94);
lean_dec(x_93);
lean_dec(x_92);
lean_dec(x_91);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_96 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_97 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_97, 0, x_96);
lean_ctor_set(x_97, 1, x_90);
return x_97;
}
else
{
lean_object* x_98; lean_object* x_99; lean_object* x_100; uint8_t x_101; 
x_98 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_99 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_98, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_90);
x_100 = lean_ctor_get(x_99, 0);
lean_inc(x_100);
x_101 = lean_unbox(x_100);
lean_dec(x_100);
if (x_101 == 0)
{
lean_object* x_102; lean_object* x_103; lean_object* x_104; 
lean_dec(x_94);
lean_dec(x_2);
x_102 = lean_ctor_get(x_99, 1);
lean_inc(x_102);
lean_dec(x_99);
x_103 = lean_box(0);
x_104 = l_Std_BitVec_reduceMul___lambda__1(x_91, x_92, x_93, x_103, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_102);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_93);
lean_dec(x_91);
return x_104;
}
else
{
lean_object* x_105; lean_object* x_106; lean_object* x_107; lean_object* x_108; lean_object* x_109; lean_object* x_110; lean_object* x_111; lean_object* x_112; lean_object* x_113; lean_object* x_114; lean_object* x_115; lean_object* x_116; lean_object* x_117; lean_object* x_118; lean_object* x_119; lean_object* x_120; lean_object* x_121; lean_object* x_122; lean_object* x_123; lean_object* x_124; lean_object* x_125; lean_object* x_126; lean_object* x_127; lean_object* x_128; lean_object* x_129; lean_object* x_130; lean_object* x_131; lean_object* x_132; lean_object* x_133; lean_object* x_134; lean_object* x_135; lean_object* x_136; lean_object* x_137; lean_object* x_138; lean_object* x_139; lean_object* x_140; lean_object* x_141; lean_object* x_142; lean_object* x_143; lean_object* x_144; lean_object* x_145; 
x_105 = lean_ctor_get(x_99, 1);
lean_inc(x_105);
lean_dec(x_99);
x_106 = l_Lean_MessageData_ofName(x_2);
x_107 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_108 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_108, 0, x_107);
lean_ctor_set(x_108, 1, x_106);
x_109 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_110 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_110, 0, x_108);
lean_ctor_set(x_110, 1, x_109);
lean_inc(x_93);
x_111 = l_Std_BitVec_toHex(x_92, x_93);
x_112 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_112, 0, x_111);
x_113 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_114 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_114, 0, x_113);
lean_ctor_set(x_114, 1, x_112);
x_115 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_116 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_116, 0, x_114);
lean_ctor_set(x_116, 1, x_115);
lean_inc(x_92);
x_117 = l_Nat_repr(x_92);
x_118 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_118, 0, x_117);
x_119 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_119, 0, x_116);
lean_ctor_set(x_119, 1, x_118);
x_120 = l_Std_Format_defWidth;
x_121 = lean_unsigned_to_nat(0u);
x_122 = lean_format_pretty(x_119, x_120, x_121, x_121);
x_123 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_123, 0, x_122);
x_124 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_124, 0, x_123);
x_125 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_125, 0, x_110);
lean_ctor_set(x_125, 1, x_124);
x_126 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_127 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_127, 0, x_125);
lean_ctor_set(x_127, 1, x_126);
x_128 = lean_ctor_get(x_91, 1);
lean_inc(x_128);
x_129 = l_Std_BitVec_toHex(x_94, x_128);
x_130 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_130, 0, x_129);
x_131 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_131, 0, x_113);
lean_ctor_set(x_131, 1, x_130);
x_132 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_132, 0, x_131);
lean_ctor_set(x_132, 1, x_115);
x_133 = l_Nat_repr(x_94);
x_134 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_134, 0, x_133);
x_135 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_135, 0, x_132);
lean_ctor_set(x_135, 1, x_134);
x_136 = lean_format_pretty(x_135, x_120, x_121, x_121);
x_137 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_137, 0, x_136);
x_138 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_138, 0, x_137);
x_139 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_139, 0, x_127);
lean_ctor_set(x_139, 1, x_138);
x_140 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_141 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_141, 0, x_139);
lean_ctor_set(x_141, 1, x_140);
x_142 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_98, x_141, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_105);
x_143 = lean_ctor_get(x_142, 0);
lean_inc(x_143);
x_144 = lean_ctor_get(x_142, 1);
lean_inc(x_144);
lean_dec(x_142);
x_145 = l_Std_BitVec_reduceMul___lambda__1(x_91, x_92, x_93, x_143, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_144);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_143);
lean_dec(x_93);
lean_dec(x_91);
return x_145;
}
}
}
}
}
else
{
uint8_t x_146; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_146 = !lean_is_exclusive(x_25);
if (x_146 == 0)
{
return x_25;
}
else
{
lean_object* x_147; lean_object* x_148; lean_object* x_149; 
x_147 = lean_ctor_get(x_25, 0);
x_148 = lean_ctor_get(x_25, 1);
lean_inc(x_148);
lean_inc(x_147);
lean_dec(x_25);
x_149 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_149, 0, x_147);
lean_ctor_set(x_149, 1, x_148);
return x_149;
}
}
}
}
else
{
uint8_t x_150; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_150 = !lean_is_exclusive(x_14);
if (x_150 == 0)
{
return x_14;
}
else
{
lean_object* x_151; lean_object* x_152; lean_object* x_153; 
x_151 = lean_ctor_get(x_14, 0);
x_152 = lean_ctor_get(x_14, 1);
lean_inc(x_152);
lean_inc(x_151);
lean_dec(x_14);
x_153 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_153, 0, x_151);
lean_ctor_set(x_153, 1, x_152);
return x_153;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceMul___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("HMul", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceMul___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("hMul", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceMul___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceMul___closed__1;
x_2 = l_Std_BitVec_reduceMul___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMul(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceMul___closed__3;
x_11 = lean_unsigned_to_nat(6u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceMul___lambda__2(x_1, x_10, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMul___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; 
x_13 = l_Std_BitVec_reduceMul___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_1);
return x_13;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceMul", 9);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceMul___closed__3;
x_2 = lean_unsigned_to_nat(6u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__6;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__8;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__10() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__9;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__11() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__10;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__12() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__11;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__13() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__12;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__14() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceMul), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__13;
x_4 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__14;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1641_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__14;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1643_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__14;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSub___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; lean_object* x_14; lean_object* x_15; lean_object* x_16; lean_object* x_17; uint32_t x_18; uint8_t x_19; lean_object* x_20; lean_object* x_21; lean_object* x_22; 
x_13 = lean_ctor_get(x_1, 1);
x_14 = l_Std_BitVec_sub(x_2, x_3, x_13);
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_2);
lean_ctor_set(x_15, 1, x_14);
x_16 = l_Std_BitVec_Literal_toExpr(x_15);
x_17 = lean_box(0);
x_18 = 0;
x_19 = 1;
x_20 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_20, 0, x_16);
lean_ctor_set(x_20, 1, x_17);
lean_ctor_set_uint32(x_20, sizeof(void*)*2, x_18);
lean_ctor_set_uint8(x_20, sizeof(void*)*2 + 4, x_19);
x_21 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_21, 0, x_20);
x_22 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_22, 0, x_21);
lean_ctor_set(x_22, 1, x_12);
return x_22;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSub___lambda__2(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; lean_object* x_14; 
lean_dec(x_3);
lean_inc(x_1);
x_12 = l_Lean_Expr_appFn_x21(x_1);
x_13 = l_Lean_Expr_appArg_x21(x_12);
lean_dec(x_12);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_14 = l_Std_BitVec_fromExpr_x3f(x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_14) == 0)
{
lean_object* x_15; 
x_15 = lean_ctor_get(x_14, 0);
lean_inc(x_15);
if (lean_obj_tag(x_15) == 0)
{
uint8_t x_16; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_16 = !lean_is_exclusive(x_14);
if (x_16 == 0)
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_ctor_get(x_14, 0);
lean_dec(x_17);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_14, 0, x_18);
return x_14;
}
else
{
lean_object* x_19; lean_object* x_20; lean_object* x_21; 
x_19 = lean_ctor_get(x_14, 1);
lean_inc(x_19);
lean_dec(x_14);
x_20 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_21 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_21, 0, x_20);
lean_ctor_set(x_21, 1, x_19);
return x_21;
}
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_22 = lean_ctor_get(x_14, 1);
lean_inc(x_22);
lean_dec(x_14);
x_23 = lean_ctor_get(x_15, 0);
lean_inc(x_23);
lean_dec(x_15);
x_24 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_25 = l_Std_BitVec_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_22);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint8_t x_40; 
x_34 = lean_ctor_get(x_25, 1);
x_35 = lean_ctor_get(x_25, 0);
lean_dec(x_35);
x_36 = lean_ctor_get(x_26, 0);
lean_inc(x_36);
lean_dec(x_26);
x_37 = lean_ctor_get(x_23, 0);
lean_inc(x_37);
x_38 = lean_ctor_get(x_23, 1);
lean_inc(x_38);
lean_dec(x_23);
x_39 = lean_ctor_get(x_36, 0);
lean_inc(x_39);
x_40 = lean_nat_dec_eq(x_37, x_39);
if (x_40 == 0)
{
lean_object* x_41; 
lean_dec(x_39);
lean_dec(x_38);
lean_dec(x_37);
lean_dec(x_36);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_41 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_41);
return x_25;
}
else
{
lean_object* x_42; lean_object* x_43; lean_object* x_44; uint8_t x_45; 
lean_free_object(x_25);
x_42 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_43 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_42, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_34);
x_44 = lean_ctor_get(x_43, 0);
lean_inc(x_44);
x_45 = lean_unbox(x_44);
lean_dec(x_44);
if (x_45 == 0)
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; 
lean_dec(x_39);
lean_dec(x_2);
x_46 = lean_ctor_get(x_43, 1);
lean_inc(x_46);
lean_dec(x_43);
x_47 = lean_box(0);
x_48 = l_Std_BitVec_reduceSub___lambda__1(x_36, x_37, x_38, x_47, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_46);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_38);
lean_dec(x_36);
return x_48;
}
else
{
lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; lean_object* x_69; lean_object* x_70; lean_object* x_71; lean_object* x_72; lean_object* x_73; lean_object* x_74; lean_object* x_75; lean_object* x_76; lean_object* x_77; lean_object* x_78; lean_object* x_79; lean_object* x_80; lean_object* x_81; lean_object* x_82; lean_object* x_83; lean_object* x_84; lean_object* x_85; lean_object* x_86; lean_object* x_87; lean_object* x_88; lean_object* x_89; 
x_49 = lean_ctor_get(x_43, 1);
lean_inc(x_49);
lean_dec(x_43);
x_50 = l_Lean_MessageData_ofName(x_2);
x_51 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_52 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_52, 0, x_51);
lean_ctor_set(x_52, 1, x_50);
x_53 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_54 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_54, 0, x_52);
lean_ctor_set(x_54, 1, x_53);
lean_inc(x_38);
x_55 = l_Std_BitVec_toHex(x_37, x_38);
x_56 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_56, 0, x_55);
x_57 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_58 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_58, 0, x_57);
lean_ctor_set(x_58, 1, x_56);
x_59 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_60 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_60, 0, x_58);
lean_ctor_set(x_60, 1, x_59);
lean_inc(x_37);
x_61 = l_Nat_repr(x_37);
x_62 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_62, 0, x_61);
x_63 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_63, 0, x_60);
lean_ctor_set(x_63, 1, x_62);
x_64 = l_Std_Format_defWidth;
x_65 = lean_unsigned_to_nat(0u);
x_66 = lean_format_pretty(x_63, x_64, x_65, x_65);
x_67 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_68, 0, x_67);
x_69 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_69, 0, x_54);
lean_ctor_set(x_69, 1, x_68);
x_70 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_71 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
x_72 = lean_ctor_get(x_36, 1);
lean_inc(x_72);
x_73 = l_Std_BitVec_toHex(x_39, x_72);
x_74 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_74, 0, x_73);
x_75 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_75, 0, x_57);
lean_ctor_set(x_75, 1, x_74);
x_76 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_76, 0, x_75);
lean_ctor_set(x_76, 1, x_59);
x_77 = l_Nat_repr(x_39);
x_78 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_78, 0, x_77);
x_79 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_79, 0, x_76);
lean_ctor_set(x_79, 1, x_78);
x_80 = lean_format_pretty(x_79, x_64, x_65, x_65);
x_81 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_81, 0, x_80);
x_82 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_82, 0, x_81);
x_83 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_83, 0, x_71);
lean_ctor_set(x_83, 1, x_82);
x_84 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_85 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_85, 0, x_83);
lean_ctor_set(x_85, 1, x_84);
x_86 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_42, x_85, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_49);
x_87 = lean_ctor_get(x_86, 0);
lean_inc(x_87);
x_88 = lean_ctor_get(x_86, 1);
lean_inc(x_88);
lean_dec(x_86);
x_89 = l_Std_BitVec_reduceSub___lambda__1(x_36, x_37, x_38, x_87, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_88);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_87);
lean_dec(x_38);
lean_dec(x_36);
return x_89;
}
}
}
else
{
lean_object* x_90; lean_object* x_91; lean_object* x_92; lean_object* x_93; lean_object* x_94; uint8_t x_95; 
x_90 = lean_ctor_get(x_25, 1);
lean_inc(x_90);
lean_dec(x_25);
x_91 = lean_ctor_get(x_26, 0);
lean_inc(x_91);
lean_dec(x_26);
x_92 = lean_ctor_get(x_23, 0);
lean_inc(x_92);
x_93 = lean_ctor_get(x_23, 1);
lean_inc(x_93);
lean_dec(x_23);
x_94 = lean_ctor_get(x_91, 0);
lean_inc(x_94);
x_95 = lean_nat_dec_eq(x_92, x_94);
if (x_95 == 0)
{
lean_object* x_96; lean_object* x_97; 
lean_dec(x_94);
lean_dec(x_93);
lean_dec(x_92);
lean_dec(x_91);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_96 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_97 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_97, 0, x_96);
lean_ctor_set(x_97, 1, x_90);
return x_97;
}
else
{
lean_object* x_98; lean_object* x_99; lean_object* x_100; uint8_t x_101; 
x_98 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_99 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_98, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_90);
x_100 = lean_ctor_get(x_99, 0);
lean_inc(x_100);
x_101 = lean_unbox(x_100);
lean_dec(x_100);
if (x_101 == 0)
{
lean_object* x_102; lean_object* x_103; lean_object* x_104; 
lean_dec(x_94);
lean_dec(x_2);
x_102 = lean_ctor_get(x_99, 1);
lean_inc(x_102);
lean_dec(x_99);
x_103 = lean_box(0);
x_104 = l_Std_BitVec_reduceSub___lambda__1(x_91, x_92, x_93, x_103, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_102);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_93);
lean_dec(x_91);
return x_104;
}
else
{
lean_object* x_105; lean_object* x_106; lean_object* x_107; lean_object* x_108; lean_object* x_109; lean_object* x_110; lean_object* x_111; lean_object* x_112; lean_object* x_113; lean_object* x_114; lean_object* x_115; lean_object* x_116; lean_object* x_117; lean_object* x_118; lean_object* x_119; lean_object* x_120; lean_object* x_121; lean_object* x_122; lean_object* x_123; lean_object* x_124; lean_object* x_125; lean_object* x_126; lean_object* x_127; lean_object* x_128; lean_object* x_129; lean_object* x_130; lean_object* x_131; lean_object* x_132; lean_object* x_133; lean_object* x_134; lean_object* x_135; lean_object* x_136; lean_object* x_137; lean_object* x_138; lean_object* x_139; lean_object* x_140; lean_object* x_141; lean_object* x_142; lean_object* x_143; lean_object* x_144; lean_object* x_145; 
x_105 = lean_ctor_get(x_99, 1);
lean_inc(x_105);
lean_dec(x_99);
x_106 = l_Lean_MessageData_ofName(x_2);
x_107 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_108 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_108, 0, x_107);
lean_ctor_set(x_108, 1, x_106);
x_109 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_110 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_110, 0, x_108);
lean_ctor_set(x_110, 1, x_109);
lean_inc(x_93);
x_111 = l_Std_BitVec_toHex(x_92, x_93);
x_112 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_112, 0, x_111);
x_113 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_114 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_114, 0, x_113);
lean_ctor_set(x_114, 1, x_112);
x_115 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_116 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_116, 0, x_114);
lean_ctor_set(x_116, 1, x_115);
lean_inc(x_92);
x_117 = l_Nat_repr(x_92);
x_118 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_118, 0, x_117);
x_119 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_119, 0, x_116);
lean_ctor_set(x_119, 1, x_118);
x_120 = l_Std_Format_defWidth;
x_121 = lean_unsigned_to_nat(0u);
x_122 = lean_format_pretty(x_119, x_120, x_121, x_121);
x_123 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_123, 0, x_122);
x_124 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_124, 0, x_123);
x_125 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_125, 0, x_110);
lean_ctor_set(x_125, 1, x_124);
x_126 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_127 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_127, 0, x_125);
lean_ctor_set(x_127, 1, x_126);
x_128 = lean_ctor_get(x_91, 1);
lean_inc(x_128);
x_129 = l_Std_BitVec_toHex(x_94, x_128);
x_130 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_130, 0, x_129);
x_131 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_131, 0, x_113);
lean_ctor_set(x_131, 1, x_130);
x_132 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_132, 0, x_131);
lean_ctor_set(x_132, 1, x_115);
x_133 = l_Nat_repr(x_94);
x_134 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_134, 0, x_133);
x_135 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_135, 0, x_132);
lean_ctor_set(x_135, 1, x_134);
x_136 = lean_format_pretty(x_135, x_120, x_121, x_121);
x_137 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_137, 0, x_136);
x_138 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_138, 0, x_137);
x_139 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_139, 0, x_127);
lean_ctor_set(x_139, 1, x_138);
x_140 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_141 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_141, 0, x_139);
lean_ctor_set(x_141, 1, x_140);
x_142 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_98, x_141, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_105);
x_143 = lean_ctor_get(x_142, 0);
lean_inc(x_143);
x_144 = lean_ctor_get(x_142, 1);
lean_inc(x_144);
lean_dec(x_142);
x_145 = l_Std_BitVec_reduceSub___lambda__1(x_91, x_92, x_93, x_143, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_144);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_143);
lean_dec(x_93);
lean_dec(x_91);
return x_145;
}
}
}
}
}
else
{
uint8_t x_146; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_146 = !lean_is_exclusive(x_25);
if (x_146 == 0)
{
return x_25;
}
else
{
lean_object* x_147; lean_object* x_148; lean_object* x_149; 
x_147 = lean_ctor_get(x_25, 0);
x_148 = lean_ctor_get(x_25, 1);
lean_inc(x_148);
lean_inc(x_147);
lean_dec(x_25);
x_149 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_149, 0, x_147);
lean_ctor_set(x_149, 1, x_148);
return x_149;
}
}
}
}
else
{
uint8_t x_150; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_150 = !lean_is_exclusive(x_14);
if (x_150 == 0)
{
return x_14;
}
else
{
lean_object* x_151; lean_object* x_152; lean_object* x_153; 
x_151 = lean_ctor_get(x_14, 0);
x_152 = lean_ctor_get(x_14, 1);
lean_inc(x_152);
lean_inc(x_151);
lean_dec(x_14);
x_153 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_153, 0, x_151);
lean_ctor_set(x_153, 1, x_152);
return x_153;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceSub___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("HSub", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceSub___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("hSub", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceSub___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceSub___closed__1;
x_2 = l_Std_BitVec_reduceSub___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSub(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceSub___closed__3;
x_11 = lean_unsigned_to_nat(6u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceSub___lambda__2(x_1, x_10, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSub___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; 
x_13 = l_Std_BitVec_reduceSub___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_1);
return x_13;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceSub", 9);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceSub___closed__3;
x_2 = lean_unsigned_to_nat(6u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__6;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__8;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__10() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__9;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__11() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__10;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__12() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__11;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__13() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__12;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__14() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceSub), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__13;
x_4 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__14;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1683_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__14;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1685_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__14;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceDiv___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; lean_object* x_14; lean_object* x_15; lean_object* x_16; lean_object* x_17; uint32_t x_18; uint8_t x_19; lean_object* x_20; lean_object* x_21; lean_object* x_22; 
x_13 = lean_ctor_get(x_1, 1);
x_14 = lean_nat_div(x_2, x_13);
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_3);
lean_ctor_set(x_15, 1, x_14);
x_16 = l_Std_BitVec_Literal_toExpr(x_15);
x_17 = lean_box(0);
x_18 = 0;
x_19 = 1;
x_20 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_20, 0, x_16);
lean_ctor_set(x_20, 1, x_17);
lean_ctor_set_uint32(x_20, sizeof(void*)*2, x_18);
lean_ctor_set_uint8(x_20, sizeof(void*)*2 + 4, x_19);
x_21 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_21, 0, x_20);
x_22 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_22, 0, x_21);
lean_ctor_set(x_22, 1, x_12);
return x_22;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceDiv___lambda__2(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; lean_object* x_14; 
lean_dec(x_3);
lean_inc(x_1);
x_12 = l_Lean_Expr_appFn_x21(x_1);
x_13 = l_Lean_Expr_appArg_x21(x_12);
lean_dec(x_12);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_14 = l_Std_BitVec_fromExpr_x3f(x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_14) == 0)
{
lean_object* x_15; 
x_15 = lean_ctor_get(x_14, 0);
lean_inc(x_15);
if (lean_obj_tag(x_15) == 0)
{
uint8_t x_16; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_16 = !lean_is_exclusive(x_14);
if (x_16 == 0)
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_ctor_get(x_14, 0);
lean_dec(x_17);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_14, 0, x_18);
return x_14;
}
else
{
lean_object* x_19; lean_object* x_20; lean_object* x_21; 
x_19 = lean_ctor_get(x_14, 1);
lean_inc(x_19);
lean_dec(x_14);
x_20 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_21 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_21, 0, x_20);
lean_ctor_set(x_21, 1, x_19);
return x_21;
}
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_22 = lean_ctor_get(x_14, 1);
lean_inc(x_22);
lean_dec(x_14);
x_23 = lean_ctor_get(x_15, 0);
lean_inc(x_23);
lean_dec(x_15);
x_24 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_25 = l_Std_BitVec_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_22);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint8_t x_40; 
x_34 = lean_ctor_get(x_25, 1);
x_35 = lean_ctor_get(x_25, 0);
lean_dec(x_35);
x_36 = lean_ctor_get(x_26, 0);
lean_inc(x_36);
lean_dec(x_26);
x_37 = lean_ctor_get(x_23, 0);
lean_inc(x_37);
x_38 = lean_ctor_get(x_23, 1);
lean_inc(x_38);
lean_dec(x_23);
x_39 = lean_ctor_get(x_36, 0);
lean_inc(x_39);
x_40 = lean_nat_dec_eq(x_37, x_39);
if (x_40 == 0)
{
lean_object* x_41; 
lean_dec(x_39);
lean_dec(x_38);
lean_dec(x_37);
lean_dec(x_36);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_41 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_41);
return x_25;
}
else
{
lean_object* x_42; lean_object* x_43; lean_object* x_44; uint8_t x_45; 
lean_free_object(x_25);
x_42 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_43 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_42, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_34);
x_44 = lean_ctor_get(x_43, 0);
lean_inc(x_44);
x_45 = lean_unbox(x_44);
lean_dec(x_44);
if (x_45 == 0)
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; 
lean_dec(x_39);
lean_dec(x_2);
x_46 = lean_ctor_get(x_43, 1);
lean_inc(x_46);
lean_dec(x_43);
x_47 = lean_box(0);
x_48 = l_Std_BitVec_reduceDiv___lambda__1(x_36, x_38, x_37, x_47, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_46);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_38);
lean_dec(x_36);
return x_48;
}
else
{
lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; lean_object* x_69; lean_object* x_70; lean_object* x_71; lean_object* x_72; lean_object* x_73; lean_object* x_74; lean_object* x_75; lean_object* x_76; lean_object* x_77; lean_object* x_78; lean_object* x_79; lean_object* x_80; lean_object* x_81; lean_object* x_82; lean_object* x_83; lean_object* x_84; lean_object* x_85; lean_object* x_86; lean_object* x_87; lean_object* x_88; lean_object* x_89; 
x_49 = lean_ctor_get(x_43, 1);
lean_inc(x_49);
lean_dec(x_43);
x_50 = l_Lean_MessageData_ofName(x_2);
x_51 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_52 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_52, 0, x_51);
lean_ctor_set(x_52, 1, x_50);
x_53 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_54 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_54, 0, x_52);
lean_ctor_set(x_54, 1, x_53);
lean_inc(x_38);
x_55 = l_Std_BitVec_toHex(x_37, x_38);
x_56 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_56, 0, x_55);
x_57 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_58 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_58, 0, x_57);
lean_ctor_set(x_58, 1, x_56);
x_59 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_60 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_60, 0, x_58);
lean_ctor_set(x_60, 1, x_59);
lean_inc(x_37);
x_61 = l_Nat_repr(x_37);
x_62 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_62, 0, x_61);
x_63 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_63, 0, x_60);
lean_ctor_set(x_63, 1, x_62);
x_64 = l_Std_Format_defWidth;
x_65 = lean_unsigned_to_nat(0u);
x_66 = lean_format_pretty(x_63, x_64, x_65, x_65);
x_67 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_68, 0, x_67);
x_69 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_69, 0, x_54);
lean_ctor_set(x_69, 1, x_68);
x_70 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_71 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
x_72 = lean_ctor_get(x_36, 1);
lean_inc(x_72);
x_73 = l_Std_BitVec_toHex(x_39, x_72);
x_74 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_74, 0, x_73);
x_75 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_75, 0, x_57);
lean_ctor_set(x_75, 1, x_74);
x_76 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_76, 0, x_75);
lean_ctor_set(x_76, 1, x_59);
x_77 = l_Nat_repr(x_39);
x_78 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_78, 0, x_77);
x_79 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_79, 0, x_76);
lean_ctor_set(x_79, 1, x_78);
x_80 = lean_format_pretty(x_79, x_64, x_65, x_65);
x_81 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_81, 0, x_80);
x_82 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_82, 0, x_81);
x_83 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_83, 0, x_71);
lean_ctor_set(x_83, 1, x_82);
x_84 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_85 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_85, 0, x_83);
lean_ctor_set(x_85, 1, x_84);
x_86 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_42, x_85, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_49);
x_87 = lean_ctor_get(x_86, 0);
lean_inc(x_87);
x_88 = lean_ctor_get(x_86, 1);
lean_inc(x_88);
lean_dec(x_86);
x_89 = l_Std_BitVec_reduceDiv___lambda__1(x_36, x_38, x_37, x_87, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_88);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_87);
lean_dec(x_38);
lean_dec(x_36);
return x_89;
}
}
}
else
{
lean_object* x_90; lean_object* x_91; lean_object* x_92; lean_object* x_93; lean_object* x_94; uint8_t x_95; 
x_90 = lean_ctor_get(x_25, 1);
lean_inc(x_90);
lean_dec(x_25);
x_91 = lean_ctor_get(x_26, 0);
lean_inc(x_91);
lean_dec(x_26);
x_92 = lean_ctor_get(x_23, 0);
lean_inc(x_92);
x_93 = lean_ctor_get(x_23, 1);
lean_inc(x_93);
lean_dec(x_23);
x_94 = lean_ctor_get(x_91, 0);
lean_inc(x_94);
x_95 = lean_nat_dec_eq(x_92, x_94);
if (x_95 == 0)
{
lean_object* x_96; lean_object* x_97; 
lean_dec(x_94);
lean_dec(x_93);
lean_dec(x_92);
lean_dec(x_91);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_96 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_97 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_97, 0, x_96);
lean_ctor_set(x_97, 1, x_90);
return x_97;
}
else
{
lean_object* x_98; lean_object* x_99; lean_object* x_100; uint8_t x_101; 
x_98 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_99 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_98, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_90);
x_100 = lean_ctor_get(x_99, 0);
lean_inc(x_100);
x_101 = lean_unbox(x_100);
lean_dec(x_100);
if (x_101 == 0)
{
lean_object* x_102; lean_object* x_103; lean_object* x_104; 
lean_dec(x_94);
lean_dec(x_2);
x_102 = lean_ctor_get(x_99, 1);
lean_inc(x_102);
lean_dec(x_99);
x_103 = lean_box(0);
x_104 = l_Std_BitVec_reduceDiv___lambda__1(x_91, x_93, x_92, x_103, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_102);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_93);
lean_dec(x_91);
return x_104;
}
else
{
lean_object* x_105; lean_object* x_106; lean_object* x_107; lean_object* x_108; lean_object* x_109; lean_object* x_110; lean_object* x_111; lean_object* x_112; lean_object* x_113; lean_object* x_114; lean_object* x_115; lean_object* x_116; lean_object* x_117; lean_object* x_118; lean_object* x_119; lean_object* x_120; lean_object* x_121; lean_object* x_122; lean_object* x_123; lean_object* x_124; lean_object* x_125; lean_object* x_126; lean_object* x_127; lean_object* x_128; lean_object* x_129; lean_object* x_130; lean_object* x_131; lean_object* x_132; lean_object* x_133; lean_object* x_134; lean_object* x_135; lean_object* x_136; lean_object* x_137; lean_object* x_138; lean_object* x_139; lean_object* x_140; lean_object* x_141; lean_object* x_142; lean_object* x_143; lean_object* x_144; lean_object* x_145; 
x_105 = lean_ctor_get(x_99, 1);
lean_inc(x_105);
lean_dec(x_99);
x_106 = l_Lean_MessageData_ofName(x_2);
x_107 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_108 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_108, 0, x_107);
lean_ctor_set(x_108, 1, x_106);
x_109 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_110 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_110, 0, x_108);
lean_ctor_set(x_110, 1, x_109);
lean_inc(x_93);
x_111 = l_Std_BitVec_toHex(x_92, x_93);
x_112 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_112, 0, x_111);
x_113 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_114 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_114, 0, x_113);
lean_ctor_set(x_114, 1, x_112);
x_115 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_116 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_116, 0, x_114);
lean_ctor_set(x_116, 1, x_115);
lean_inc(x_92);
x_117 = l_Nat_repr(x_92);
x_118 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_118, 0, x_117);
x_119 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_119, 0, x_116);
lean_ctor_set(x_119, 1, x_118);
x_120 = l_Std_Format_defWidth;
x_121 = lean_unsigned_to_nat(0u);
x_122 = lean_format_pretty(x_119, x_120, x_121, x_121);
x_123 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_123, 0, x_122);
x_124 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_124, 0, x_123);
x_125 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_125, 0, x_110);
lean_ctor_set(x_125, 1, x_124);
x_126 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_127 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_127, 0, x_125);
lean_ctor_set(x_127, 1, x_126);
x_128 = lean_ctor_get(x_91, 1);
lean_inc(x_128);
x_129 = l_Std_BitVec_toHex(x_94, x_128);
x_130 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_130, 0, x_129);
x_131 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_131, 0, x_113);
lean_ctor_set(x_131, 1, x_130);
x_132 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_132, 0, x_131);
lean_ctor_set(x_132, 1, x_115);
x_133 = l_Nat_repr(x_94);
x_134 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_134, 0, x_133);
x_135 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_135, 0, x_132);
lean_ctor_set(x_135, 1, x_134);
x_136 = lean_format_pretty(x_135, x_120, x_121, x_121);
x_137 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_137, 0, x_136);
x_138 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_138, 0, x_137);
x_139 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_139, 0, x_127);
lean_ctor_set(x_139, 1, x_138);
x_140 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_141 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_141, 0, x_139);
lean_ctor_set(x_141, 1, x_140);
x_142 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_98, x_141, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_105);
x_143 = lean_ctor_get(x_142, 0);
lean_inc(x_143);
x_144 = lean_ctor_get(x_142, 1);
lean_inc(x_144);
lean_dec(x_142);
x_145 = l_Std_BitVec_reduceDiv___lambda__1(x_91, x_93, x_92, x_143, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_144);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_143);
lean_dec(x_93);
lean_dec(x_91);
return x_145;
}
}
}
}
}
else
{
uint8_t x_146; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_146 = !lean_is_exclusive(x_25);
if (x_146 == 0)
{
return x_25;
}
else
{
lean_object* x_147; lean_object* x_148; lean_object* x_149; 
x_147 = lean_ctor_get(x_25, 0);
x_148 = lean_ctor_get(x_25, 1);
lean_inc(x_148);
lean_inc(x_147);
lean_dec(x_25);
x_149 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_149, 0, x_147);
lean_ctor_set(x_149, 1, x_148);
return x_149;
}
}
}
}
else
{
uint8_t x_150; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_150 = !lean_is_exclusive(x_14);
if (x_150 == 0)
{
return x_14;
}
else
{
lean_object* x_151; lean_object* x_152; lean_object* x_153; 
x_151 = lean_ctor_get(x_14, 0);
x_152 = lean_ctor_get(x_14, 1);
lean_inc(x_152);
lean_inc(x_151);
lean_dec(x_14);
x_153 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_153, 0, x_151);
lean_ctor_set(x_153, 1, x_152);
return x_153;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceDiv___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("HDiv", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceDiv___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("hDiv", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceDiv___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceDiv___closed__1;
x_2 = l_Std_BitVec_reduceDiv___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceDiv(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceDiv___closed__3;
x_11 = lean_unsigned_to_nat(6u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceDiv___lambda__2(x_1, x_10, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceDiv___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; 
x_13 = l_Std_BitVec_reduceDiv___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
return x_13;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceDiv", 9);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceDiv___closed__3;
x_2 = lean_unsigned_to_nat(6u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__6;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__8;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__10() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__9;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__11() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__10;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__12() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__11;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__13() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__12;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__14() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceDiv), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__13;
x_4 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__14;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1725_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__14;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1727_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__14;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMod___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; lean_object* x_14; lean_object* x_15; lean_object* x_16; lean_object* x_17; uint32_t x_18; uint8_t x_19; lean_object* x_20; lean_object* x_21; lean_object* x_22; 
x_13 = lean_ctor_get(x_1, 1);
x_14 = lean_nat_mod(x_2, x_13);
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_3);
lean_ctor_set(x_15, 1, x_14);
x_16 = l_Std_BitVec_Literal_toExpr(x_15);
x_17 = lean_box(0);
x_18 = 0;
x_19 = 1;
x_20 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_20, 0, x_16);
lean_ctor_set(x_20, 1, x_17);
lean_ctor_set_uint32(x_20, sizeof(void*)*2, x_18);
lean_ctor_set_uint8(x_20, sizeof(void*)*2 + 4, x_19);
x_21 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_21, 0, x_20);
x_22 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_22, 0, x_21);
lean_ctor_set(x_22, 1, x_12);
return x_22;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMod___lambda__2(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; lean_object* x_14; 
lean_dec(x_3);
lean_inc(x_1);
x_12 = l_Lean_Expr_appFn_x21(x_1);
x_13 = l_Lean_Expr_appArg_x21(x_12);
lean_dec(x_12);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_14 = l_Std_BitVec_fromExpr_x3f(x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_14) == 0)
{
lean_object* x_15; 
x_15 = lean_ctor_get(x_14, 0);
lean_inc(x_15);
if (lean_obj_tag(x_15) == 0)
{
uint8_t x_16; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_16 = !lean_is_exclusive(x_14);
if (x_16 == 0)
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_ctor_get(x_14, 0);
lean_dec(x_17);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_14, 0, x_18);
return x_14;
}
else
{
lean_object* x_19; lean_object* x_20; lean_object* x_21; 
x_19 = lean_ctor_get(x_14, 1);
lean_inc(x_19);
lean_dec(x_14);
x_20 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_21 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_21, 0, x_20);
lean_ctor_set(x_21, 1, x_19);
return x_21;
}
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_22 = lean_ctor_get(x_14, 1);
lean_inc(x_22);
lean_dec(x_14);
x_23 = lean_ctor_get(x_15, 0);
lean_inc(x_23);
lean_dec(x_15);
x_24 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_25 = l_Std_BitVec_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_22);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint8_t x_40; 
x_34 = lean_ctor_get(x_25, 1);
x_35 = lean_ctor_get(x_25, 0);
lean_dec(x_35);
x_36 = lean_ctor_get(x_26, 0);
lean_inc(x_36);
lean_dec(x_26);
x_37 = lean_ctor_get(x_23, 0);
lean_inc(x_37);
x_38 = lean_ctor_get(x_23, 1);
lean_inc(x_38);
lean_dec(x_23);
x_39 = lean_ctor_get(x_36, 0);
lean_inc(x_39);
x_40 = lean_nat_dec_eq(x_37, x_39);
if (x_40 == 0)
{
lean_object* x_41; 
lean_dec(x_39);
lean_dec(x_38);
lean_dec(x_37);
lean_dec(x_36);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_41 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_41);
return x_25;
}
else
{
lean_object* x_42; lean_object* x_43; lean_object* x_44; uint8_t x_45; 
lean_free_object(x_25);
x_42 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_43 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_42, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_34);
x_44 = lean_ctor_get(x_43, 0);
lean_inc(x_44);
x_45 = lean_unbox(x_44);
lean_dec(x_44);
if (x_45 == 0)
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; 
lean_dec(x_39);
lean_dec(x_2);
x_46 = lean_ctor_get(x_43, 1);
lean_inc(x_46);
lean_dec(x_43);
x_47 = lean_box(0);
x_48 = l_Std_BitVec_reduceMod___lambda__1(x_36, x_38, x_37, x_47, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_46);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_38);
lean_dec(x_36);
return x_48;
}
else
{
lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; lean_object* x_69; lean_object* x_70; lean_object* x_71; lean_object* x_72; lean_object* x_73; lean_object* x_74; lean_object* x_75; lean_object* x_76; lean_object* x_77; lean_object* x_78; lean_object* x_79; lean_object* x_80; lean_object* x_81; lean_object* x_82; lean_object* x_83; lean_object* x_84; lean_object* x_85; lean_object* x_86; lean_object* x_87; lean_object* x_88; lean_object* x_89; 
x_49 = lean_ctor_get(x_43, 1);
lean_inc(x_49);
lean_dec(x_43);
x_50 = l_Lean_MessageData_ofName(x_2);
x_51 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_52 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_52, 0, x_51);
lean_ctor_set(x_52, 1, x_50);
x_53 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_54 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_54, 0, x_52);
lean_ctor_set(x_54, 1, x_53);
lean_inc(x_38);
x_55 = l_Std_BitVec_toHex(x_37, x_38);
x_56 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_56, 0, x_55);
x_57 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_58 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_58, 0, x_57);
lean_ctor_set(x_58, 1, x_56);
x_59 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_60 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_60, 0, x_58);
lean_ctor_set(x_60, 1, x_59);
lean_inc(x_37);
x_61 = l_Nat_repr(x_37);
x_62 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_62, 0, x_61);
x_63 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_63, 0, x_60);
lean_ctor_set(x_63, 1, x_62);
x_64 = l_Std_Format_defWidth;
x_65 = lean_unsigned_to_nat(0u);
x_66 = lean_format_pretty(x_63, x_64, x_65, x_65);
x_67 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_68, 0, x_67);
x_69 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_69, 0, x_54);
lean_ctor_set(x_69, 1, x_68);
x_70 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_71 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
x_72 = lean_ctor_get(x_36, 1);
lean_inc(x_72);
x_73 = l_Std_BitVec_toHex(x_39, x_72);
x_74 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_74, 0, x_73);
x_75 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_75, 0, x_57);
lean_ctor_set(x_75, 1, x_74);
x_76 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_76, 0, x_75);
lean_ctor_set(x_76, 1, x_59);
x_77 = l_Nat_repr(x_39);
x_78 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_78, 0, x_77);
x_79 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_79, 0, x_76);
lean_ctor_set(x_79, 1, x_78);
x_80 = lean_format_pretty(x_79, x_64, x_65, x_65);
x_81 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_81, 0, x_80);
x_82 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_82, 0, x_81);
x_83 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_83, 0, x_71);
lean_ctor_set(x_83, 1, x_82);
x_84 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_85 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_85, 0, x_83);
lean_ctor_set(x_85, 1, x_84);
x_86 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_42, x_85, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_49);
x_87 = lean_ctor_get(x_86, 0);
lean_inc(x_87);
x_88 = lean_ctor_get(x_86, 1);
lean_inc(x_88);
lean_dec(x_86);
x_89 = l_Std_BitVec_reduceMod___lambda__1(x_36, x_38, x_37, x_87, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_88);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_87);
lean_dec(x_38);
lean_dec(x_36);
return x_89;
}
}
}
else
{
lean_object* x_90; lean_object* x_91; lean_object* x_92; lean_object* x_93; lean_object* x_94; uint8_t x_95; 
x_90 = lean_ctor_get(x_25, 1);
lean_inc(x_90);
lean_dec(x_25);
x_91 = lean_ctor_get(x_26, 0);
lean_inc(x_91);
lean_dec(x_26);
x_92 = lean_ctor_get(x_23, 0);
lean_inc(x_92);
x_93 = lean_ctor_get(x_23, 1);
lean_inc(x_93);
lean_dec(x_23);
x_94 = lean_ctor_get(x_91, 0);
lean_inc(x_94);
x_95 = lean_nat_dec_eq(x_92, x_94);
if (x_95 == 0)
{
lean_object* x_96; lean_object* x_97; 
lean_dec(x_94);
lean_dec(x_93);
lean_dec(x_92);
lean_dec(x_91);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_96 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_97 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_97, 0, x_96);
lean_ctor_set(x_97, 1, x_90);
return x_97;
}
else
{
lean_object* x_98; lean_object* x_99; lean_object* x_100; uint8_t x_101; 
x_98 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_99 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_98, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_90);
x_100 = lean_ctor_get(x_99, 0);
lean_inc(x_100);
x_101 = lean_unbox(x_100);
lean_dec(x_100);
if (x_101 == 0)
{
lean_object* x_102; lean_object* x_103; lean_object* x_104; 
lean_dec(x_94);
lean_dec(x_2);
x_102 = lean_ctor_get(x_99, 1);
lean_inc(x_102);
lean_dec(x_99);
x_103 = lean_box(0);
x_104 = l_Std_BitVec_reduceMod___lambda__1(x_91, x_93, x_92, x_103, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_102);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_93);
lean_dec(x_91);
return x_104;
}
else
{
lean_object* x_105; lean_object* x_106; lean_object* x_107; lean_object* x_108; lean_object* x_109; lean_object* x_110; lean_object* x_111; lean_object* x_112; lean_object* x_113; lean_object* x_114; lean_object* x_115; lean_object* x_116; lean_object* x_117; lean_object* x_118; lean_object* x_119; lean_object* x_120; lean_object* x_121; lean_object* x_122; lean_object* x_123; lean_object* x_124; lean_object* x_125; lean_object* x_126; lean_object* x_127; lean_object* x_128; lean_object* x_129; lean_object* x_130; lean_object* x_131; lean_object* x_132; lean_object* x_133; lean_object* x_134; lean_object* x_135; lean_object* x_136; lean_object* x_137; lean_object* x_138; lean_object* x_139; lean_object* x_140; lean_object* x_141; lean_object* x_142; lean_object* x_143; lean_object* x_144; lean_object* x_145; 
x_105 = lean_ctor_get(x_99, 1);
lean_inc(x_105);
lean_dec(x_99);
x_106 = l_Lean_MessageData_ofName(x_2);
x_107 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_108 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_108, 0, x_107);
lean_ctor_set(x_108, 1, x_106);
x_109 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_110 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_110, 0, x_108);
lean_ctor_set(x_110, 1, x_109);
lean_inc(x_93);
x_111 = l_Std_BitVec_toHex(x_92, x_93);
x_112 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_112, 0, x_111);
x_113 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_114 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_114, 0, x_113);
lean_ctor_set(x_114, 1, x_112);
x_115 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_116 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_116, 0, x_114);
lean_ctor_set(x_116, 1, x_115);
lean_inc(x_92);
x_117 = l_Nat_repr(x_92);
x_118 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_118, 0, x_117);
x_119 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_119, 0, x_116);
lean_ctor_set(x_119, 1, x_118);
x_120 = l_Std_Format_defWidth;
x_121 = lean_unsigned_to_nat(0u);
x_122 = lean_format_pretty(x_119, x_120, x_121, x_121);
x_123 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_123, 0, x_122);
x_124 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_124, 0, x_123);
x_125 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_125, 0, x_110);
lean_ctor_set(x_125, 1, x_124);
x_126 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_127 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_127, 0, x_125);
lean_ctor_set(x_127, 1, x_126);
x_128 = lean_ctor_get(x_91, 1);
lean_inc(x_128);
x_129 = l_Std_BitVec_toHex(x_94, x_128);
x_130 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_130, 0, x_129);
x_131 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_131, 0, x_113);
lean_ctor_set(x_131, 1, x_130);
x_132 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_132, 0, x_131);
lean_ctor_set(x_132, 1, x_115);
x_133 = l_Nat_repr(x_94);
x_134 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_134, 0, x_133);
x_135 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_135, 0, x_132);
lean_ctor_set(x_135, 1, x_134);
x_136 = lean_format_pretty(x_135, x_120, x_121, x_121);
x_137 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_137, 0, x_136);
x_138 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_138, 0, x_137);
x_139 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_139, 0, x_127);
lean_ctor_set(x_139, 1, x_138);
x_140 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_141 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_141, 0, x_139);
lean_ctor_set(x_141, 1, x_140);
x_142 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_98, x_141, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_105);
x_143 = lean_ctor_get(x_142, 0);
lean_inc(x_143);
x_144 = lean_ctor_get(x_142, 1);
lean_inc(x_144);
lean_dec(x_142);
x_145 = l_Std_BitVec_reduceMod___lambda__1(x_91, x_93, x_92, x_143, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_144);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_143);
lean_dec(x_93);
lean_dec(x_91);
return x_145;
}
}
}
}
}
else
{
uint8_t x_146; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_146 = !lean_is_exclusive(x_25);
if (x_146 == 0)
{
return x_25;
}
else
{
lean_object* x_147; lean_object* x_148; lean_object* x_149; 
x_147 = lean_ctor_get(x_25, 0);
x_148 = lean_ctor_get(x_25, 1);
lean_inc(x_148);
lean_inc(x_147);
lean_dec(x_25);
x_149 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_149, 0, x_147);
lean_ctor_set(x_149, 1, x_148);
return x_149;
}
}
}
}
else
{
uint8_t x_150; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_150 = !lean_is_exclusive(x_14);
if (x_150 == 0)
{
return x_14;
}
else
{
lean_object* x_151; lean_object* x_152; lean_object* x_153; 
x_151 = lean_ctor_get(x_14, 0);
x_152 = lean_ctor_get(x_14, 1);
lean_inc(x_152);
lean_inc(x_151);
lean_dec(x_14);
x_153 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_153, 0, x_151);
lean_ctor_set(x_153, 1, x_152);
return x_153;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceMod___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("HMod", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceMod___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("hMod", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceMod___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceMod___closed__1;
x_2 = l_Std_BitVec_reduceMod___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMod(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceMod___closed__3;
x_11 = lean_unsigned_to_nat(6u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceMod___lambda__2(x_1, x_10, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceMod___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; 
x_13 = l_Std_BitVec_reduceMod___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
return x_13;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceMod", 9);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceMod___closed__3;
x_2 = lean_unsigned_to_nat(6u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__6;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__8;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__10() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__9;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__11() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__10;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__12() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__11;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__13() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__12;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__14() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceMod), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__13;
x_4 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__14;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1767_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__14;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1769_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__14;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
static lean_object* _init_l_Std_BitVec_reduceUMod___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("umod", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceUMod___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceUMod___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUMod(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceUMod___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceMod___lambda__2(x_1, x_10, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceUMod", 10);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceUMod___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = lean_unsigned_to_nat(4u);
x_2 = lean_mk_empty_array_with_capacity(x_1);
return x_2;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__9() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceUMod), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__8;
x_4 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__9;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1793_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__9;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1795_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__9;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
static lean_object* _init_l_Std_BitVec_reduceUDiv___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("udiv", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceUDiv___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceUDiv___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUDiv(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceUDiv___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceDiv___lambda__2(x_1, x_10, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceUDiv", 10);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceUDiv___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceUDiv), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1819_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1821_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTUDiv___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; lean_object* x_14; lean_object* x_15; lean_object* x_16; lean_object* x_17; uint32_t x_18; uint8_t x_19; lean_object* x_20; lean_object* x_21; lean_object* x_22; 
x_13 = lean_ctor_get(x_1, 1);
x_14 = l_Std_BitVec_smtUDiv(x_2, x_3, x_13);
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_2);
lean_ctor_set(x_15, 1, x_14);
x_16 = l_Std_BitVec_Literal_toExpr(x_15);
x_17 = lean_box(0);
x_18 = 0;
x_19 = 1;
x_20 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_20, 0, x_16);
lean_ctor_set(x_20, 1, x_17);
lean_ctor_set_uint32(x_20, sizeof(void*)*2, x_18);
lean_ctor_set_uint8(x_20, sizeof(void*)*2 + 4, x_19);
x_21 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_21, 0, x_20);
x_22 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_22, 0, x_21);
lean_ctor_set(x_22, 1, x_12);
return x_22;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTUDiv___lambda__2(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; lean_object* x_14; 
lean_dec(x_3);
lean_inc(x_1);
x_12 = l_Lean_Expr_appFn_x21(x_1);
x_13 = l_Lean_Expr_appArg_x21(x_12);
lean_dec(x_12);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_14 = l_Std_BitVec_fromExpr_x3f(x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_14) == 0)
{
lean_object* x_15; 
x_15 = lean_ctor_get(x_14, 0);
lean_inc(x_15);
if (lean_obj_tag(x_15) == 0)
{
uint8_t x_16; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_16 = !lean_is_exclusive(x_14);
if (x_16 == 0)
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_ctor_get(x_14, 0);
lean_dec(x_17);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_14, 0, x_18);
return x_14;
}
else
{
lean_object* x_19; lean_object* x_20; lean_object* x_21; 
x_19 = lean_ctor_get(x_14, 1);
lean_inc(x_19);
lean_dec(x_14);
x_20 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_21 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_21, 0, x_20);
lean_ctor_set(x_21, 1, x_19);
return x_21;
}
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_22 = lean_ctor_get(x_14, 1);
lean_inc(x_22);
lean_dec(x_14);
x_23 = lean_ctor_get(x_15, 0);
lean_inc(x_23);
lean_dec(x_15);
x_24 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_25 = l_Std_BitVec_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_22);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint8_t x_40; 
x_34 = lean_ctor_get(x_25, 1);
x_35 = lean_ctor_get(x_25, 0);
lean_dec(x_35);
x_36 = lean_ctor_get(x_26, 0);
lean_inc(x_36);
lean_dec(x_26);
x_37 = lean_ctor_get(x_23, 0);
lean_inc(x_37);
x_38 = lean_ctor_get(x_23, 1);
lean_inc(x_38);
lean_dec(x_23);
x_39 = lean_ctor_get(x_36, 0);
lean_inc(x_39);
x_40 = lean_nat_dec_eq(x_37, x_39);
if (x_40 == 0)
{
lean_object* x_41; 
lean_dec(x_39);
lean_dec(x_38);
lean_dec(x_37);
lean_dec(x_36);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_41 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_41);
return x_25;
}
else
{
lean_object* x_42; lean_object* x_43; lean_object* x_44; uint8_t x_45; 
lean_free_object(x_25);
x_42 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_43 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_42, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_34);
x_44 = lean_ctor_get(x_43, 0);
lean_inc(x_44);
x_45 = lean_unbox(x_44);
lean_dec(x_44);
if (x_45 == 0)
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; 
lean_dec(x_39);
lean_dec(x_2);
x_46 = lean_ctor_get(x_43, 1);
lean_inc(x_46);
lean_dec(x_43);
x_47 = lean_box(0);
x_48 = l_Std_BitVec_reduceSMTUDiv___lambda__1(x_36, x_37, x_38, x_47, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_46);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_38);
lean_dec(x_36);
return x_48;
}
else
{
lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; lean_object* x_69; lean_object* x_70; lean_object* x_71; lean_object* x_72; lean_object* x_73; lean_object* x_74; lean_object* x_75; lean_object* x_76; lean_object* x_77; lean_object* x_78; lean_object* x_79; lean_object* x_80; lean_object* x_81; lean_object* x_82; lean_object* x_83; lean_object* x_84; lean_object* x_85; lean_object* x_86; lean_object* x_87; lean_object* x_88; lean_object* x_89; 
x_49 = lean_ctor_get(x_43, 1);
lean_inc(x_49);
lean_dec(x_43);
x_50 = l_Lean_MessageData_ofName(x_2);
x_51 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_52 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_52, 0, x_51);
lean_ctor_set(x_52, 1, x_50);
x_53 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_54 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_54, 0, x_52);
lean_ctor_set(x_54, 1, x_53);
lean_inc(x_38);
x_55 = l_Std_BitVec_toHex(x_37, x_38);
x_56 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_56, 0, x_55);
x_57 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_58 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_58, 0, x_57);
lean_ctor_set(x_58, 1, x_56);
x_59 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_60 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_60, 0, x_58);
lean_ctor_set(x_60, 1, x_59);
lean_inc(x_37);
x_61 = l_Nat_repr(x_37);
x_62 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_62, 0, x_61);
x_63 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_63, 0, x_60);
lean_ctor_set(x_63, 1, x_62);
x_64 = l_Std_Format_defWidth;
x_65 = lean_unsigned_to_nat(0u);
x_66 = lean_format_pretty(x_63, x_64, x_65, x_65);
x_67 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_68, 0, x_67);
x_69 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_69, 0, x_54);
lean_ctor_set(x_69, 1, x_68);
x_70 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_71 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
x_72 = lean_ctor_get(x_36, 1);
lean_inc(x_72);
x_73 = l_Std_BitVec_toHex(x_39, x_72);
x_74 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_74, 0, x_73);
x_75 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_75, 0, x_57);
lean_ctor_set(x_75, 1, x_74);
x_76 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_76, 0, x_75);
lean_ctor_set(x_76, 1, x_59);
x_77 = l_Nat_repr(x_39);
x_78 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_78, 0, x_77);
x_79 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_79, 0, x_76);
lean_ctor_set(x_79, 1, x_78);
x_80 = lean_format_pretty(x_79, x_64, x_65, x_65);
x_81 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_81, 0, x_80);
x_82 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_82, 0, x_81);
x_83 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_83, 0, x_71);
lean_ctor_set(x_83, 1, x_82);
x_84 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_85 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_85, 0, x_83);
lean_ctor_set(x_85, 1, x_84);
x_86 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_42, x_85, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_49);
x_87 = lean_ctor_get(x_86, 0);
lean_inc(x_87);
x_88 = lean_ctor_get(x_86, 1);
lean_inc(x_88);
lean_dec(x_86);
x_89 = l_Std_BitVec_reduceSMTUDiv___lambda__1(x_36, x_37, x_38, x_87, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_88);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_87);
lean_dec(x_38);
lean_dec(x_36);
return x_89;
}
}
}
else
{
lean_object* x_90; lean_object* x_91; lean_object* x_92; lean_object* x_93; lean_object* x_94; uint8_t x_95; 
x_90 = lean_ctor_get(x_25, 1);
lean_inc(x_90);
lean_dec(x_25);
x_91 = lean_ctor_get(x_26, 0);
lean_inc(x_91);
lean_dec(x_26);
x_92 = lean_ctor_get(x_23, 0);
lean_inc(x_92);
x_93 = lean_ctor_get(x_23, 1);
lean_inc(x_93);
lean_dec(x_23);
x_94 = lean_ctor_get(x_91, 0);
lean_inc(x_94);
x_95 = lean_nat_dec_eq(x_92, x_94);
if (x_95 == 0)
{
lean_object* x_96; lean_object* x_97; 
lean_dec(x_94);
lean_dec(x_93);
lean_dec(x_92);
lean_dec(x_91);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_96 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_97 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_97, 0, x_96);
lean_ctor_set(x_97, 1, x_90);
return x_97;
}
else
{
lean_object* x_98; lean_object* x_99; lean_object* x_100; uint8_t x_101; 
x_98 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_99 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_98, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_90);
x_100 = lean_ctor_get(x_99, 0);
lean_inc(x_100);
x_101 = lean_unbox(x_100);
lean_dec(x_100);
if (x_101 == 0)
{
lean_object* x_102; lean_object* x_103; lean_object* x_104; 
lean_dec(x_94);
lean_dec(x_2);
x_102 = lean_ctor_get(x_99, 1);
lean_inc(x_102);
lean_dec(x_99);
x_103 = lean_box(0);
x_104 = l_Std_BitVec_reduceSMTUDiv___lambda__1(x_91, x_92, x_93, x_103, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_102);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_93);
lean_dec(x_91);
return x_104;
}
else
{
lean_object* x_105; lean_object* x_106; lean_object* x_107; lean_object* x_108; lean_object* x_109; lean_object* x_110; lean_object* x_111; lean_object* x_112; lean_object* x_113; lean_object* x_114; lean_object* x_115; lean_object* x_116; lean_object* x_117; lean_object* x_118; lean_object* x_119; lean_object* x_120; lean_object* x_121; lean_object* x_122; lean_object* x_123; lean_object* x_124; lean_object* x_125; lean_object* x_126; lean_object* x_127; lean_object* x_128; lean_object* x_129; lean_object* x_130; lean_object* x_131; lean_object* x_132; lean_object* x_133; lean_object* x_134; lean_object* x_135; lean_object* x_136; lean_object* x_137; lean_object* x_138; lean_object* x_139; lean_object* x_140; lean_object* x_141; lean_object* x_142; lean_object* x_143; lean_object* x_144; lean_object* x_145; 
x_105 = lean_ctor_get(x_99, 1);
lean_inc(x_105);
lean_dec(x_99);
x_106 = l_Lean_MessageData_ofName(x_2);
x_107 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_108 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_108, 0, x_107);
lean_ctor_set(x_108, 1, x_106);
x_109 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_110 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_110, 0, x_108);
lean_ctor_set(x_110, 1, x_109);
lean_inc(x_93);
x_111 = l_Std_BitVec_toHex(x_92, x_93);
x_112 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_112, 0, x_111);
x_113 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_114 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_114, 0, x_113);
lean_ctor_set(x_114, 1, x_112);
x_115 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_116 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_116, 0, x_114);
lean_ctor_set(x_116, 1, x_115);
lean_inc(x_92);
x_117 = l_Nat_repr(x_92);
x_118 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_118, 0, x_117);
x_119 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_119, 0, x_116);
lean_ctor_set(x_119, 1, x_118);
x_120 = l_Std_Format_defWidth;
x_121 = lean_unsigned_to_nat(0u);
x_122 = lean_format_pretty(x_119, x_120, x_121, x_121);
x_123 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_123, 0, x_122);
x_124 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_124, 0, x_123);
x_125 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_125, 0, x_110);
lean_ctor_set(x_125, 1, x_124);
x_126 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_127 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_127, 0, x_125);
lean_ctor_set(x_127, 1, x_126);
x_128 = lean_ctor_get(x_91, 1);
lean_inc(x_128);
x_129 = l_Std_BitVec_toHex(x_94, x_128);
x_130 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_130, 0, x_129);
x_131 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_131, 0, x_113);
lean_ctor_set(x_131, 1, x_130);
x_132 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_132, 0, x_131);
lean_ctor_set(x_132, 1, x_115);
x_133 = l_Nat_repr(x_94);
x_134 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_134, 0, x_133);
x_135 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_135, 0, x_132);
lean_ctor_set(x_135, 1, x_134);
x_136 = lean_format_pretty(x_135, x_120, x_121, x_121);
x_137 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_137, 0, x_136);
x_138 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_138, 0, x_137);
x_139 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_139, 0, x_127);
lean_ctor_set(x_139, 1, x_138);
x_140 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_141 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_141, 0, x_139);
lean_ctor_set(x_141, 1, x_140);
x_142 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_98, x_141, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_105);
x_143 = lean_ctor_get(x_142, 0);
lean_inc(x_143);
x_144 = lean_ctor_get(x_142, 1);
lean_inc(x_144);
lean_dec(x_142);
x_145 = l_Std_BitVec_reduceSMTUDiv___lambda__1(x_91, x_92, x_93, x_143, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_144);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_143);
lean_dec(x_93);
lean_dec(x_91);
return x_145;
}
}
}
}
}
else
{
uint8_t x_146; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_146 = !lean_is_exclusive(x_25);
if (x_146 == 0)
{
return x_25;
}
else
{
lean_object* x_147; lean_object* x_148; lean_object* x_149; 
x_147 = lean_ctor_get(x_25, 0);
x_148 = lean_ctor_get(x_25, 1);
lean_inc(x_148);
lean_inc(x_147);
lean_dec(x_25);
x_149 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_149, 0, x_147);
lean_ctor_set(x_149, 1, x_148);
return x_149;
}
}
}
}
else
{
uint8_t x_150; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_150 = !lean_is_exclusive(x_14);
if (x_150 == 0)
{
return x_14;
}
else
{
lean_object* x_151; lean_object* x_152; lean_object* x_153; 
x_151 = lean_ctor_get(x_14, 0);
x_152 = lean_ctor_get(x_14, 1);
lean_inc(x_152);
lean_inc(x_151);
lean_dec(x_14);
x_153 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_153, 0, x_151);
lean_ctor_set(x_153, 1, x_152);
return x_153;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceSMTUDiv___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("smtUDiv", 7);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceSMTUDiv___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceSMTUDiv___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTUDiv(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceSMTUDiv___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceSMTUDiv___lambda__2(x_1, x_10, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTUDiv___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; 
x_13 = l_Std_BitVec_reduceSMTUDiv___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_1);
return x_13;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceSMTUDiv", 13);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceSMTUDiv___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceSMTUDiv), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1845_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1847_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMod___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; lean_object* x_14; lean_object* x_15; lean_object* x_16; lean_object* x_17; uint32_t x_18; uint8_t x_19; lean_object* x_20; lean_object* x_21; lean_object* x_22; 
x_13 = lean_ctor_get(x_1, 1);
x_14 = l_Std_BitVec_smod(x_2, x_3, x_13);
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_2);
lean_ctor_set(x_15, 1, x_14);
x_16 = l_Std_BitVec_Literal_toExpr(x_15);
x_17 = lean_box(0);
x_18 = 0;
x_19 = 1;
x_20 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_20, 0, x_16);
lean_ctor_set(x_20, 1, x_17);
lean_ctor_set_uint32(x_20, sizeof(void*)*2, x_18);
lean_ctor_set_uint8(x_20, sizeof(void*)*2 + 4, x_19);
x_21 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_21, 0, x_20);
x_22 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_22, 0, x_21);
lean_ctor_set(x_22, 1, x_12);
return x_22;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMod___lambda__2(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; lean_object* x_14; 
lean_dec(x_3);
lean_inc(x_1);
x_12 = l_Lean_Expr_appFn_x21(x_1);
x_13 = l_Lean_Expr_appArg_x21(x_12);
lean_dec(x_12);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_14 = l_Std_BitVec_fromExpr_x3f(x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_14) == 0)
{
lean_object* x_15; 
x_15 = lean_ctor_get(x_14, 0);
lean_inc(x_15);
if (lean_obj_tag(x_15) == 0)
{
uint8_t x_16; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_16 = !lean_is_exclusive(x_14);
if (x_16 == 0)
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_ctor_get(x_14, 0);
lean_dec(x_17);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_14, 0, x_18);
return x_14;
}
else
{
lean_object* x_19; lean_object* x_20; lean_object* x_21; 
x_19 = lean_ctor_get(x_14, 1);
lean_inc(x_19);
lean_dec(x_14);
x_20 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_21 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_21, 0, x_20);
lean_ctor_set(x_21, 1, x_19);
return x_21;
}
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_22 = lean_ctor_get(x_14, 1);
lean_inc(x_22);
lean_dec(x_14);
x_23 = lean_ctor_get(x_15, 0);
lean_inc(x_23);
lean_dec(x_15);
x_24 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_25 = l_Std_BitVec_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_22);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint8_t x_40; 
x_34 = lean_ctor_get(x_25, 1);
x_35 = lean_ctor_get(x_25, 0);
lean_dec(x_35);
x_36 = lean_ctor_get(x_26, 0);
lean_inc(x_36);
lean_dec(x_26);
x_37 = lean_ctor_get(x_23, 0);
lean_inc(x_37);
x_38 = lean_ctor_get(x_23, 1);
lean_inc(x_38);
lean_dec(x_23);
x_39 = lean_ctor_get(x_36, 0);
lean_inc(x_39);
x_40 = lean_nat_dec_eq(x_37, x_39);
if (x_40 == 0)
{
lean_object* x_41; 
lean_dec(x_39);
lean_dec(x_38);
lean_dec(x_37);
lean_dec(x_36);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_41 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_41);
return x_25;
}
else
{
lean_object* x_42; lean_object* x_43; lean_object* x_44; uint8_t x_45; 
lean_free_object(x_25);
x_42 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_43 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_42, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_34);
x_44 = lean_ctor_get(x_43, 0);
lean_inc(x_44);
x_45 = lean_unbox(x_44);
lean_dec(x_44);
if (x_45 == 0)
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; 
lean_dec(x_39);
lean_dec(x_2);
x_46 = lean_ctor_get(x_43, 1);
lean_inc(x_46);
lean_dec(x_43);
x_47 = lean_box(0);
x_48 = l_Std_BitVec_reduceSMod___lambda__1(x_36, x_37, x_38, x_47, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_46);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_38);
lean_dec(x_36);
return x_48;
}
else
{
lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; lean_object* x_69; lean_object* x_70; lean_object* x_71; lean_object* x_72; lean_object* x_73; lean_object* x_74; lean_object* x_75; lean_object* x_76; lean_object* x_77; lean_object* x_78; lean_object* x_79; lean_object* x_80; lean_object* x_81; lean_object* x_82; lean_object* x_83; lean_object* x_84; lean_object* x_85; lean_object* x_86; lean_object* x_87; lean_object* x_88; lean_object* x_89; 
x_49 = lean_ctor_get(x_43, 1);
lean_inc(x_49);
lean_dec(x_43);
x_50 = l_Lean_MessageData_ofName(x_2);
x_51 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_52 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_52, 0, x_51);
lean_ctor_set(x_52, 1, x_50);
x_53 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_54 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_54, 0, x_52);
lean_ctor_set(x_54, 1, x_53);
lean_inc(x_38);
x_55 = l_Std_BitVec_toHex(x_37, x_38);
x_56 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_56, 0, x_55);
x_57 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_58 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_58, 0, x_57);
lean_ctor_set(x_58, 1, x_56);
x_59 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_60 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_60, 0, x_58);
lean_ctor_set(x_60, 1, x_59);
lean_inc(x_37);
x_61 = l_Nat_repr(x_37);
x_62 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_62, 0, x_61);
x_63 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_63, 0, x_60);
lean_ctor_set(x_63, 1, x_62);
x_64 = l_Std_Format_defWidth;
x_65 = lean_unsigned_to_nat(0u);
x_66 = lean_format_pretty(x_63, x_64, x_65, x_65);
x_67 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_68, 0, x_67);
x_69 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_69, 0, x_54);
lean_ctor_set(x_69, 1, x_68);
x_70 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_71 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
x_72 = lean_ctor_get(x_36, 1);
lean_inc(x_72);
x_73 = l_Std_BitVec_toHex(x_39, x_72);
x_74 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_74, 0, x_73);
x_75 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_75, 0, x_57);
lean_ctor_set(x_75, 1, x_74);
x_76 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_76, 0, x_75);
lean_ctor_set(x_76, 1, x_59);
x_77 = l_Nat_repr(x_39);
x_78 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_78, 0, x_77);
x_79 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_79, 0, x_76);
lean_ctor_set(x_79, 1, x_78);
x_80 = lean_format_pretty(x_79, x_64, x_65, x_65);
x_81 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_81, 0, x_80);
x_82 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_82, 0, x_81);
x_83 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_83, 0, x_71);
lean_ctor_set(x_83, 1, x_82);
x_84 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_85 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_85, 0, x_83);
lean_ctor_set(x_85, 1, x_84);
x_86 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_42, x_85, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_49);
x_87 = lean_ctor_get(x_86, 0);
lean_inc(x_87);
x_88 = lean_ctor_get(x_86, 1);
lean_inc(x_88);
lean_dec(x_86);
x_89 = l_Std_BitVec_reduceSMod___lambda__1(x_36, x_37, x_38, x_87, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_88);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_87);
lean_dec(x_38);
lean_dec(x_36);
return x_89;
}
}
}
else
{
lean_object* x_90; lean_object* x_91; lean_object* x_92; lean_object* x_93; lean_object* x_94; uint8_t x_95; 
x_90 = lean_ctor_get(x_25, 1);
lean_inc(x_90);
lean_dec(x_25);
x_91 = lean_ctor_get(x_26, 0);
lean_inc(x_91);
lean_dec(x_26);
x_92 = lean_ctor_get(x_23, 0);
lean_inc(x_92);
x_93 = lean_ctor_get(x_23, 1);
lean_inc(x_93);
lean_dec(x_23);
x_94 = lean_ctor_get(x_91, 0);
lean_inc(x_94);
x_95 = lean_nat_dec_eq(x_92, x_94);
if (x_95 == 0)
{
lean_object* x_96; lean_object* x_97; 
lean_dec(x_94);
lean_dec(x_93);
lean_dec(x_92);
lean_dec(x_91);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_96 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_97 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_97, 0, x_96);
lean_ctor_set(x_97, 1, x_90);
return x_97;
}
else
{
lean_object* x_98; lean_object* x_99; lean_object* x_100; uint8_t x_101; 
x_98 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_99 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_98, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_90);
x_100 = lean_ctor_get(x_99, 0);
lean_inc(x_100);
x_101 = lean_unbox(x_100);
lean_dec(x_100);
if (x_101 == 0)
{
lean_object* x_102; lean_object* x_103; lean_object* x_104; 
lean_dec(x_94);
lean_dec(x_2);
x_102 = lean_ctor_get(x_99, 1);
lean_inc(x_102);
lean_dec(x_99);
x_103 = lean_box(0);
x_104 = l_Std_BitVec_reduceSMod___lambda__1(x_91, x_92, x_93, x_103, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_102);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_93);
lean_dec(x_91);
return x_104;
}
else
{
lean_object* x_105; lean_object* x_106; lean_object* x_107; lean_object* x_108; lean_object* x_109; lean_object* x_110; lean_object* x_111; lean_object* x_112; lean_object* x_113; lean_object* x_114; lean_object* x_115; lean_object* x_116; lean_object* x_117; lean_object* x_118; lean_object* x_119; lean_object* x_120; lean_object* x_121; lean_object* x_122; lean_object* x_123; lean_object* x_124; lean_object* x_125; lean_object* x_126; lean_object* x_127; lean_object* x_128; lean_object* x_129; lean_object* x_130; lean_object* x_131; lean_object* x_132; lean_object* x_133; lean_object* x_134; lean_object* x_135; lean_object* x_136; lean_object* x_137; lean_object* x_138; lean_object* x_139; lean_object* x_140; lean_object* x_141; lean_object* x_142; lean_object* x_143; lean_object* x_144; lean_object* x_145; 
x_105 = lean_ctor_get(x_99, 1);
lean_inc(x_105);
lean_dec(x_99);
x_106 = l_Lean_MessageData_ofName(x_2);
x_107 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_108 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_108, 0, x_107);
lean_ctor_set(x_108, 1, x_106);
x_109 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_110 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_110, 0, x_108);
lean_ctor_set(x_110, 1, x_109);
lean_inc(x_93);
x_111 = l_Std_BitVec_toHex(x_92, x_93);
x_112 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_112, 0, x_111);
x_113 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_114 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_114, 0, x_113);
lean_ctor_set(x_114, 1, x_112);
x_115 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_116 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_116, 0, x_114);
lean_ctor_set(x_116, 1, x_115);
lean_inc(x_92);
x_117 = l_Nat_repr(x_92);
x_118 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_118, 0, x_117);
x_119 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_119, 0, x_116);
lean_ctor_set(x_119, 1, x_118);
x_120 = l_Std_Format_defWidth;
x_121 = lean_unsigned_to_nat(0u);
x_122 = lean_format_pretty(x_119, x_120, x_121, x_121);
x_123 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_123, 0, x_122);
x_124 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_124, 0, x_123);
x_125 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_125, 0, x_110);
lean_ctor_set(x_125, 1, x_124);
x_126 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_127 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_127, 0, x_125);
lean_ctor_set(x_127, 1, x_126);
x_128 = lean_ctor_get(x_91, 1);
lean_inc(x_128);
x_129 = l_Std_BitVec_toHex(x_94, x_128);
x_130 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_130, 0, x_129);
x_131 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_131, 0, x_113);
lean_ctor_set(x_131, 1, x_130);
x_132 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_132, 0, x_131);
lean_ctor_set(x_132, 1, x_115);
x_133 = l_Nat_repr(x_94);
x_134 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_134, 0, x_133);
x_135 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_135, 0, x_132);
lean_ctor_set(x_135, 1, x_134);
x_136 = lean_format_pretty(x_135, x_120, x_121, x_121);
x_137 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_137, 0, x_136);
x_138 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_138, 0, x_137);
x_139 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_139, 0, x_127);
lean_ctor_set(x_139, 1, x_138);
x_140 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_141 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_141, 0, x_139);
lean_ctor_set(x_141, 1, x_140);
x_142 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_98, x_141, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_105);
x_143 = lean_ctor_get(x_142, 0);
lean_inc(x_143);
x_144 = lean_ctor_get(x_142, 1);
lean_inc(x_144);
lean_dec(x_142);
x_145 = l_Std_BitVec_reduceSMod___lambda__1(x_91, x_92, x_93, x_143, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_144);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_143);
lean_dec(x_93);
lean_dec(x_91);
return x_145;
}
}
}
}
}
else
{
uint8_t x_146; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_146 = !lean_is_exclusive(x_25);
if (x_146 == 0)
{
return x_25;
}
else
{
lean_object* x_147; lean_object* x_148; lean_object* x_149; 
x_147 = lean_ctor_get(x_25, 0);
x_148 = lean_ctor_get(x_25, 1);
lean_inc(x_148);
lean_inc(x_147);
lean_dec(x_25);
x_149 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_149, 0, x_147);
lean_ctor_set(x_149, 1, x_148);
return x_149;
}
}
}
}
else
{
uint8_t x_150; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_150 = !lean_is_exclusive(x_14);
if (x_150 == 0)
{
return x_14;
}
else
{
lean_object* x_151; lean_object* x_152; lean_object* x_153; 
x_151 = lean_ctor_get(x_14, 0);
x_152 = lean_ctor_get(x_14, 1);
lean_inc(x_152);
lean_inc(x_151);
lean_dec(x_14);
x_153 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_153, 0, x_151);
lean_ctor_set(x_153, 1, x_152);
return x_153;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceSMod___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("smod", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceSMod___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceSMod___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMod(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceSMod___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceSMod___lambda__2(x_1, x_10, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMod___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; 
x_13 = l_Std_BitVec_reduceSMod___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_1);
return x_13;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceSMod", 10);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceSMod___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceSMod), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1871_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1873_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSRem___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; lean_object* x_14; lean_object* x_15; lean_object* x_16; lean_object* x_17; uint32_t x_18; uint8_t x_19; lean_object* x_20; lean_object* x_21; lean_object* x_22; 
x_13 = lean_ctor_get(x_1, 1);
x_14 = l_Std_BitVec_srem(x_2, x_3, x_13);
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_2);
lean_ctor_set(x_15, 1, x_14);
x_16 = l_Std_BitVec_Literal_toExpr(x_15);
x_17 = lean_box(0);
x_18 = 0;
x_19 = 1;
x_20 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_20, 0, x_16);
lean_ctor_set(x_20, 1, x_17);
lean_ctor_set_uint32(x_20, sizeof(void*)*2, x_18);
lean_ctor_set_uint8(x_20, sizeof(void*)*2 + 4, x_19);
x_21 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_21, 0, x_20);
x_22 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_22, 0, x_21);
lean_ctor_set(x_22, 1, x_12);
return x_22;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSRem___lambda__2(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; lean_object* x_14; 
lean_dec(x_3);
lean_inc(x_1);
x_12 = l_Lean_Expr_appFn_x21(x_1);
x_13 = l_Lean_Expr_appArg_x21(x_12);
lean_dec(x_12);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_14 = l_Std_BitVec_fromExpr_x3f(x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_14) == 0)
{
lean_object* x_15; 
x_15 = lean_ctor_get(x_14, 0);
lean_inc(x_15);
if (lean_obj_tag(x_15) == 0)
{
uint8_t x_16; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_16 = !lean_is_exclusive(x_14);
if (x_16 == 0)
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_ctor_get(x_14, 0);
lean_dec(x_17);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_14, 0, x_18);
return x_14;
}
else
{
lean_object* x_19; lean_object* x_20; lean_object* x_21; 
x_19 = lean_ctor_get(x_14, 1);
lean_inc(x_19);
lean_dec(x_14);
x_20 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_21 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_21, 0, x_20);
lean_ctor_set(x_21, 1, x_19);
return x_21;
}
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_22 = lean_ctor_get(x_14, 1);
lean_inc(x_22);
lean_dec(x_14);
x_23 = lean_ctor_get(x_15, 0);
lean_inc(x_23);
lean_dec(x_15);
x_24 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_25 = l_Std_BitVec_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_22);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint8_t x_40; 
x_34 = lean_ctor_get(x_25, 1);
x_35 = lean_ctor_get(x_25, 0);
lean_dec(x_35);
x_36 = lean_ctor_get(x_26, 0);
lean_inc(x_36);
lean_dec(x_26);
x_37 = lean_ctor_get(x_23, 0);
lean_inc(x_37);
x_38 = lean_ctor_get(x_23, 1);
lean_inc(x_38);
lean_dec(x_23);
x_39 = lean_ctor_get(x_36, 0);
lean_inc(x_39);
x_40 = lean_nat_dec_eq(x_37, x_39);
if (x_40 == 0)
{
lean_object* x_41; 
lean_dec(x_39);
lean_dec(x_38);
lean_dec(x_37);
lean_dec(x_36);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_41 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_41);
return x_25;
}
else
{
lean_object* x_42; lean_object* x_43; lean_object* x_44; uint8_t x_45; 
lean_free_object(x_25);
x_42 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_43 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_42, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_34);
x_44 = lean_ctor_get(x_43, 0);
lean_inc(x_44);
x_45 = lean_unbox(x_44);
lean_dec(x_44);
if (x_45 == 0)
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; 
lean_dec(x_39);
lean_dec(x_2);
x_46 = lean_ctor_get(x_43, 1);
lean_inc(x_46);
lean_dec(x_43);
x_47 = lean_box(0);
x_48 = l_Std_BitVec_reduceSRem___lambda__1(x_36, x_37, x_38, x_47, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_46);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_38);
lean_dec(x_36);
return x_48;
}
else
{
lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; lean_object* x_69; lean_object* x_70; lean_object* x_71; lean_object* x_72; lean_object* x_73; lean_object* x_74; lean_object* x_75; lean_object* x_76; lean_object* x_77; lean_object* x_78; lean_object* x_79; lean_object* x_80; lean_object* x_81; lean_object* x_82; lean_object* x_83; lean_object* x_84; lean_object* x_85; lean_object* x_86; lean_object* x_87; lean_object* x_88; lean_object* x_89; 
x_49 = lean_ctor_get(x_43, 1);
lean_inc(x_49);
lean_dec(x_43);
x_50 = l_Lean_MessageData_ofName(x_2);
x_51 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_52 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_52, 0, x_51);
lean_ctor_set(x_52, 1, x_50);
x_53 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_54 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_54, 0, x_52);
lean_ctor_set(x_54, 1, x_53);
lean_inc(x_38);
x_55 = l_Std_BitVec_toHex(x_37, x_38);
x_56 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_56, 0, x_55);
x_57 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_58 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_58, 0, x_57);
lean_ctor_set(x_58, 1, x_56);
x_59 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_60 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_60, 0, x_58);
lean_ctor_set(x_60, 1, x_59);
lean_inc(x_37);
x_61 = l_Nat_repr(x_37);
x_62 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_62, 0, x_61);
x_63 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_63, 0, x_60);
lean_ctor_set(x_63, 1, x_62);
x_64 = l_Std_Format_defWidth;
x_65 = lean_unsigned_to_nat(0u);
x_66 = lean_format_pretty(x_63, x_64, x_65, x_65);
x_67 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_68, 0, x_67);
x_69 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_69, 0, x_54);
lean_ctor_set(x_69, 1, x_68);
x_70 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_71 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
x_72 = lean_ctor_get(x_36, 1);
lean_inc(x_72);
x_73 = l_Std_BitVec_toHex(x_39, x_72);
x_74 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_74, 0, x_73);
x_75 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_75, 0, x_57);
lean_ctor_set(x_75, 1, x_74);
x_76 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_76, 0, x_75);
lean_ctor_set(x_76, 1, x_59);
x_77 = l_Nat_repr(x_39);
x_78 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_78, 0, x_77);
x_79 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_79, 0, x_76);
lean_ctor_set(x_79, 1, x_78);
x_80 = lean_format_pretty(x_79, x_64, x_65, x_65);
x_81 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_81, 0, x_80);
x_82 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_82, 0, x_81);
x_83 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_83, 0, x_71);
lean_ctor_set(x_83, 1, x_82);
x_84 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_85 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_85, 0, x_83);
lean_ctor_set(x_85, 1, x_84);
x_86 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_42, x_85, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_49);
x_87 = lean_ctor_get(x_86, 0);
lean_inc(x_87);
x_88 = lean_ctor_get(x_86, 1);
lean_inc(x_88);
lean_dec(x_86);
x_89 = l_Std_BitVec_reduceSRem___lambda__1(x_36, x_37, x_38, x_87, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_88);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_87);
lean_dec(x_38);
lean_dec(x_36);
return x_89;
}
}
}
else
{
lean_object* x_90; lean_object* x_91; lean_object* x_92; lean_object* x_93; lean_object* x_94; uint8_t x_95; 
x_90 = lean_ctor_get(x_25, 1);
lean_inc(x_90);
lean_dec(x_25);
x_91 = lean_ctor_get(x_26, 0);
lean_inc(x_91);
lean_dec(x_26);
x_92 = lean_ctor_get(x_23, 0);
lean_inc(x_92);
x_93 = lean_ctor_get(x_23, 1);
lean_inc(x_93);
lean_dec(x_23);
x_94 = lean_ctor_get(x_91, 0);
lean_inc(x_94);
x_95 = lean_nat_dec_eq(x_92, x_94);
if (x_95 == 0)
{
lean_object* x_96; lean_object* x_97; 
lean_dec(x_94);
lean_dec(x_93);
lean_dec(x_92);
lean_dec(x_91);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_96 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_97 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_97, 0, x_96);
lean_ctor_set(x_97, 1, x_90);
return x_97;
}
else
{
lean_object* x_98; lean_object* x_99; lean_object* x_100; uint8_t x_101; 
x_98 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_99 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_98, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_90);
x_100 = lean_ctor_get(x_99, 0);
lean_inc(x_100);
x_101 = lean_unbox(x_100);
lean_dec(x_100);
if (x_101 == 0)
{
lean_object* x_102; lean_object* x_103; lean_object* x_104; 
lean_dec(x_94);
lean_dec(x_2);
x_102 = lean_ctor_get(x_99, 1);
lean_inc(x_102);
lean_dec(x_99);
x_103 = lean_box(0);
x_104 = l_Std_BitVec_reduceSRem___lambda__1(x_91, x_92, x_93, x_103, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_102);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_93);
lean_dec(x_91);
return x_104;
}
else
{
lean_object* x_105; lean_object* x_106; lean_object* x_107; lean_object* x_108; lean_object* x_109; lean_object* x_110; lean_object* x_111; lean_object* x_112; lean_object* x_113; lean_object* x_114; lean_object* x_115; lean_object* x_116; lean_object* x_117; lean_object* x_118; lean_object* x_119; lean_object* x_120; lean_object* x_121; lean_object* x_122; lean_object* x_123; lean_object* x_124; lean_object* x_125; lean_object* x_126; lean_object* x_127; lean_object* x_128; lean_object* x_129; lean_object* x_130; lean_object* x_131; lean_object* x_132; lean_object* x_133; lean_object* x_134; lean_object* x_135; lean_object* x_136; lean_object* x_137; lean_object* x_138; lean_object* x_139; lean_object* x_140; lean_object* x_141; lean_object* x_142; lean_object* x_143; lean_object* x_144; lean_object* x_145; 
x_105 = lean_ctor_get(x_99, 1);
lean_inc(x_105);
lean_dec(x_99);
x_106 = l_Lean_MessageData_ofName(x_2);
x_107 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_108 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_108, 0, x_107);
lean_ctor_set(x_108, 1, x_106);
x_109 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_110 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_110, 0, x_108);
lean_ctor_set(x_110, 1, x_109);
lean_inc(x_93);
x_111 = l_Std_BitVec_toHex(x_92, x_93);
x_112 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_112, 0, x_111);
x_113 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_114 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_114, 0, x_113);
lean_ctor_set(x_114, 1, x_112);
x_115 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_116 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_116, 0, x_114);
lean_ctor_set(x_116, 1, x_115);
lean_inc(x_92);
x_117 = l_Nat_repr(x_92);
x_118 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_118, 0, x_117);
x_119 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_119, 0, x_116);
lean_ctor_set(x_119, 1, x_118);
x_120 = l_Std_Format_defWidth;
x_121 = lean_unsigned_to_nat(0u);
x_122 = lean_format_pretty(x_119, x_120, x_121, x_121);
x_123 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_123, 0, x_122);
x_124 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_124, 0, x_123);
x_125 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_125, 0, x_110);
lean_ctor_set(x_125, 1, x_124);
x_126 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_127 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_127, 0, x_125);
lean_ctor_set(x_127, 1, x_126);
x_128 = lean_ctor_get(x_91, 1);
lean_inc(x_128);
x_129 = l_Std_BitVec_toHex(x_94, x_128);
x_130 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_130, 0, x_129);
x_131 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_131, 0, x_113);
lean_ctor_set(x_131, 1, x_130);
x_132 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_132, 0, x_131);
lean_ctor_set(x_132, 1, x_115);
x_133 = l_Nat_repr(x_94);
x_134 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_134, 0, x_133);
x_135 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_135, 0, x_132);
lean_ctor_set(x_135, 1, x_134);
x_136 = lean_format_pretty(x_135, x_120, x_121, x_121);
x_137 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_137, 0, x_136);
x_138 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_138, 0, x_137);
x_139 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_139, 0, x_127);
lean_ctor_set(x_139, 1, x_138);
x_140 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_141 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_141, 0, x_139);
lean_ctor_set(x_141, 1, x_140);
x_142 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_98, x_141, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_105);
x_143 = lean_ctor_get(x_142, 0);
lean_inc(x_143);
x_144 = lean_ctor_get(x_142, 1);
lean_inc(x_144);
lean_dec(x_142);
x_145 = l_Std_BitVec_reduceSRem___lambda__1(x_91, x_92, x_93, x_143, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_144);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_143);
lean_dec(x_93);
lean_dec(x_91);
return x_145;
}
}
}
}
}
else
{
uint8_t x_146; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_146 = !lean_is_exclusive(x_25);
if (x_146 == 0)
{
return x_25;
}
else
{
lean_object* x_147; lean_object* x_148; lean_object* x_149; 
x_147 = lean_ctor_get(x_25, 0);
x_148 = lean_ctor_get(x_25, 1);
lean_inc(x_148);
lean_inc(x_147);
lean_dec(x_25);
x_149 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_149, 0, x_147);
lean_ctor_set(x_149, 1, x_148);
return x_149;
}
}
}
}
else
{
uint8_t x_150; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_150 = !lean_is_exclusive(x_14);
if (x_150 == 0)
{
return x_14;
}
else
{
lean_object* x_151; lean_object* x_152; lean_object* x_153; 
x_151 = lean_ctor_get(x_14, 0);
x_152 = lean_ctor_get(x_14, 1);
lean_inc(x_152);
lean_inc(x_151);
lean_dec(x_14);
x_153 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_153, 0, x_151);
lean_ctor_set(x_153, 1, x_152);
return x_153;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceSRem___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("srem", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceSRem___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceSRem___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSRem(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceSRem___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceSRem___lambda__2(x_1, x_10, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSRem___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; 
x_13 = l_Std_BitVec_reduceSRem___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_1);
return x_13;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceSRem", 10);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceSRem___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceSRem), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1897_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1899_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSDiv___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; lean_object* x_14; lean_object* x_15; lean_object* x_16; lean_object* x_17; uint32_t x_18; uint8_t x_19; lean_object* x_20; lean_object* x_21; lean_object* x_22; 
x_13 = lean_ctor_get(x_1, 1);
x_14 = l_Std_BitVec_sdiv(x_2, x_3, x_13);
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_2);
lean_ctor_set(x_15, 1, x_14);
x_16 = l_Std_BitVec_Literal_toExpr(x_15);
x_17 = lean_box(0);
x_18 = 0;
x_19 = 1;
x_20 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_20, 0, x_16);
lean_ctor_set(x_20, 1, x_17);
lean_ctor_set_uint32(x_20, sizeof(void*)*2, x_18);
lean_ctor_set_uint8(x_20, sizeof(void*)*2 + 4, x_19);
x_21 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_21, 0, x_20);
x_22 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_22, 0, x_21);
lean_ctor_set(x_22, 1, x_12);
return x_22;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSDiv___lambda__2(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; lean_object* x_14; 
lean_dec(x_3);
lean_inc(x_1);
x_12 = l_Lean_Expr_appFn_x21(x_1);
x_13 = l_Lean_Expr_appArg_x21(x_12);
lean_dec(x_12);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_14 = l_Std_BitVec_fromExpr_x3f(x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_14) == 0)
{
lean_object* x_15; 
x_15 = lean_ctor_get(x_14, 0);
lean_inc(x_15);
if (lean_obj_tag(x_15) == 0)
{
uint8_t x_16; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_16 = !lean_is_exclusive(x_14);
if (x_16 == 0)
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_ctor_get(x_14, 0);
lean_dec(x_17);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_14, 0, x_18);
return x_14;
}
else
{
lean_object* x_19; lean_object* x_20; lean_object* x_21; 
x_19 = lean_ctor_get(x_14, 1);
lean_inc(x_19);
lean_dec(x_14);
x_20 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_21 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_21, 0, x_20);
lean_ctor_set(x_21, 1, x_19);
return x_21;
}
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_22 = lean_ctor_get(x_14, 1);
lean_inc(x_22);
lean_dec(x_14);
x_23 = lean_ctor_get(x_15, 0);
lean_inc(x_23);
lean_dec(x_15);
x_24 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_25 = l_Std_BitVec_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_22);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint8_t x_40; 
x_34 = lean_ctor_get(x_25, 1);
x_35 = lean_ctor_get(x_25, 0);
lean_dec(x_35);
x_36 = lean_ctor_get(x_26, 0);
lean_inc(x_36);
lean_dec(x_26);
x_37 = lean_ctor_get(x_23, 0);
lean_inc(x_37);
x_38 = lean_ctor_get(x_23, 1);
lean_inc(x_38);
lean_dec(x_23);
x_39 = lean_ctor_get(x_36, 0);
lean_inc(x_39);
x_40 = lean_nat_dec_eq(x_37, x_39);
if (x_40 == 0)
{
lean_object* x_41; 
lean_dec(x_39);
lean_dec(x_38);
lean_dec(x_37);
lean_dec(x_36);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_41 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_41);
return x_25;
}
else
{
lean_object* x_42; lean_object* x_43; lean_object* x_44; uint8_t x_45; 
lean_free_object(x_25);
x_42 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_43 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_42, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_34);
x_44 = lean_ctor_get(x_43, 0);
lean_inc(x_44);
x_45 = lean_unbox(x_44);
lean_dec(x_44);
if (x_45 == 0)
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; 
lean_dec(x_39);
lean_dec(x_2);
x_46 = lean_ctor_get(x_43, 1);
lean_inc(x_46);
lean_dec(x_43);
x_47 = lean_box(0);
x_48 = l_Std_BitVec_reduceSDiv___lambda__1(x_36, x_37, x_38, x_47, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_46);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_38);
lean_dec(x_36);
return x_48;
}
else
{
lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; lean_object* x_69; lean_object* x_70; lean_object* x_71; lean_object* x_72; lean_object* x_73; lean_object* x_74; lean_object* x_75; lean_object* x_76; lean_object* x_77; lean_object* x_78; lean_object* x_79; lean_object* x_80; lean_object* x_81; lean_object* x_82; lean_object* x_83; lean_object* x_84; lean_object* x_85; lean_object* x_86; lean_object* x_87; lean_object* x_88; lean_object* x_89; 
x_49 = lean_ctor_get(x_43, 1);
lean_inc(x_49);
lean_dec(x_43);
x_50 = l_Lean_MessageData_ofName(x_2);
x_51 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_52 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_52, 0, x_51);
lean_ctor_set(x_52, 1, x_50);
x_53 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_54 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_54, 0, x_52);
lean_ctor_set(x_54, 1, x_53);
lean_inc(x_38);
x_55 = l_Std_BitVec_toHex(x_37, x_38);
x_56 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_56, 0, x_55);
x_57 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_58 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_58, 0, x_57);
lean_ctor_set(x_58, 1, x_56);
x_59 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_60 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_60, 0, x_58);
lean_ctor_set(x_60, 1, x_59);
lean_inc(x_37);
x_61 = l_Nat_repr(x_37);
x_62 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_62, 0, x_61);
x_63 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_63, 0, x_60);
lean_ctor_set(x_63, 1, x_62);
x_64 = l_Std_Format_defWidth;
x_65 = lean_unsigned_to_nat(0u);
x_66 = lean_format_pretty(x_63, x_64, x_65, x_65);
x_67 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_68, 0, x_67);
x_69 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_69, 0, x_54);
lean_ctor_set(x_69, 1, x_68);
x_70 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_71 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
x_72 = lean_ctor_get(x_36, 1);
lean_inc(x_72);
x_73 = l_Std_BitVec_toHex(x_39, x_72);
x_74 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_74, 0, x_73);
x_75 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_75, 0, x_57);
lean_ctor_set(x_75, 1, x_74);
x_76 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_76, 0, x_75);
lean_ctor_set(x_76, 1, x_59);
x_77 = l_Nat_repr(x_39);
x_78 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_78, 0, x_77);
x_79 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_79, 0, x_76);
lean_ctor_set(x_79, 1, x_78);
x_80 = lean_format_pretty(x_79, x_64, x_65, x_65);
x_81 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_81, 0, x_80);
x_82 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_82, 0, x_81);
x_83 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_83, 0, x_71);
lean_ctor_set(x_83, 1, x_82);
x_84 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_85 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_85, 0, x_83);
lean_ctor_set(x_85, 1, x_84);
x_86 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_42, x_85, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_49);
x_87 = lean_ctor_get(x_86, 0);
lean_inc(x_87);
x_88 = lean_ctor_get(x_86, 1);
lean_inc(x_88);
lean_dec(x_86);
x_89 = l_Std_BitVec_reduceSDiv___lambda__1(x_36, x_37, x_38, x_87, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_88);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_87);
lean_dec(x_38);
lean_dec(x_36);
return x_89;
}
}
}
else
{
lean_object* x_90; lean_object* x_91; lean_object* x_92; lean_object* x_93; lean_object* x_94; uint8_t x_95; 
x_90 = lean_ctor_get(x_25, 1);
lean_inc(x_90);
lean_dec(x_25);
x_91 = lean_ctor_get(x_26, 0);
lean_inc(x_91);
lean_dec(x_26);
x_92 = lean_ctor_get(x_23, 0);
lean_inc(x_92);
x_93 = lean_ctor_get(x_23, 1);
lean_inc(x_93);
lean_dec(x_23);
x_94 = lean_ctor_get(x_91, 0);
lean_inc(x_94);
x_95 = lean_nat_dec_eq(x_92, x_94);
if (x_95 == 0)
{
lean_object* x_96; lean_object* x_97; 
lean_dec(x_94);
lean_dec(x_93);
lean_dec(x_92);
lean_dec(x_91);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_96 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_97 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_97, 0, x_96);
lean_ctor_set(x_97, 1, x_90);
return x_97;
}
else
{
lean_object* x_98; lean_object* x_99; lean_object* x_100; uint8_t x_101; 
x_98 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_99 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_98, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_90);
x_100 = lean_ctor_get(x_99, 0);
lean_inc(x_100);
x_101 = lean_unbox(x_100);
lean_dec(x_100);
if (x_101 == 0)
{
lean_object* x_102; lean_object* x_103; lean_object* x_104; 
lean_dec(x_94);
lean_dec(x_2);
x_102 = lean_ctor_get(x_99, 1);
lean_inc(x_102);
lean_dec(x_99);
x_103 = lean_box(0);
x_104 = l_Std_BitVec_reduceSDiv___lambda__1(x_91, x_92, x_93, x_103, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_102);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_93);
lean_dec(x_91);
return x_104;
}
else
{
lean_object* x_105; lean_object* x_106; lean_object* x_107; lean_object* x_108; lean_object* x_109; lean_object* x_110; lean_object* x_111; lean_object* x_112; lean_object* x_113; lean_object* x_114; lean_object* x_115; lean_object* x_116; lean_object* x_117; lean_object* x_118; lean_object* x_119; lean_object* x_120; lean_object* x_121; lean_object* x_122; lean_object* x_123; lean_object* x_124; lean_object* x_125; lean_object* x_126; lean_object* x_127; lean_object* x_128; lean_object* x_129; lean_object* x_130; lean_object* x_131; lean_object* x_132; lean_object* x_133; lean_object* x_134; lean_object* x_135; lean_object* x_136; lean_object* x_137; lean_object* x_138; lean_object* x_139; lean_object* x_140; lean_object* x_141; lean_object* x_142; lean_object* x_143; lean_object* x_144; lean_object* x_145; 
x_105 = lean_ctor_get(x_99, 1);
lean_inc(x_105);
lean_dec(x_99);
x_106 = l_Lean_MessageData_ofName(x_2);
x_107 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_108 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_108, 0, x_107);
lean_ctor_set(x_108, 1, x_106);
x_109 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_110 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_110, 0, x_108);
lean_ctor_set(x_110, 1, x_109);
lean_inc(x_93);
x_111 = l_Std_BitVec_toHex(x_92, x_93);
x_112 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_112, 0, x_111);
x_113 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_114 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_114, 0, x_113);
lean_ctor_set(x_114, 1, x_112);
x_115 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_116 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_116, 0, x_114);
lean_ctor_set(x_116, 1, x_115);
lean_inc(x_92);
x_117 = l_Nat_repr(x_92);
x_118 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_118, 0, x_117);
x_119 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_119, 0, x_116);
lean_ctor_set(x_119, 1, x_118);
x_120 = l_Std_Format_defWidth;
x_121 = lean_unsigned_to_nat(0u);
x_122 = lean_format_pretty(x_119, x_120, x_121, x_121);
x_123 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_123, 0, x_122);
x_124 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_124, 0, x_123);
x_125 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_125, 0, x_110);
lean_ctor_set(x_125, 1, x_124);
x_126 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_127 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_127, 0, x_125);
lean_ctor_set(x_127, 1, x_126);
x_128 = lean_ctor_get(x_91, 1);
lean_inc(x_128);
x_129 = l_Std_BitVec_toHex(x_94, x_128);
x_130 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_130, 0, x_129);
x_131 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_131, 0, x_113);
lean_ctor_set(x_131, 1, x_130);
x_132 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_132, 0, x_131);
lean_ctor_set(x_132, 1, x_115);
x_133 = l_Nat_repr(x_94);
x_134 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_134, 0, x_133);
x_135 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_135, 0, x_132);
lean_ctor_set(x_135, 1, x_134);
x_136 = lean_format_pretty(x_135, x_120, x_121, x_121);
x_137 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_137, 0, x_136);
x_138 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_138, 0, x_137);
x_139 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_139, 0, x_127);
lean_ctor_set(x_139, 1, x_138);
x_140 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_141 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_141, 0, x_139);
lean_ctor_set(x_141, 1, x_140);
x_142 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_98, x_141, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_105);
x_143 = lean_ctor_get(x_142, 0);
lean_inc(x_143);
x_144 = lean_ctor_get(x_142, 1);
lean_inc(x_144);
lean_dec(x_142);
x_145 = l_Std_BitVec_reduceSDiv___lambda__1(x_91, x_92, x_93, x_143, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_144);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_143);
lean_dec(x_93);
lean_dec(x_91);
return x_145;
}
}
}
}
}
else
{
uint8_t x_146; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_146 = !lean_is_exclusive(x_25);
if (x_146 == 0)
{
return x_25;
}
else
{
lean_object* x_147; lean_object* x_148; lean_object* x_149; 
x_147 = lean_ctor_get(x_25, 0);
x_148 = lean_ctor_get(x_25, 1);
lean_inc(x_148);
lean_inc(x_147);
lean_dec(x_25);
x_149 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_149, 0, x_147);
lean_ctor_set(x_149, 1, x_148);
return x_149;
}
}
}
}
else
{
uint8_t x_150; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_150 = !lean_is_exclusive(x_14);
if (x_150 == 0)
{
return x_14;
}
else
{
lean_object* x_151; lean_object* x_152; lean_object* x_153; 
x_151 = lean_ctor_get(x_14, 0);
x_152 = lean_ctor_get(x_14, 1);
lean_inc(x_152);
lean_inc(x_151);
lean_dec(x_14);
x_153 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_153, 0, x_151);
lean_ctor_set(x_153, 1, x_152);
return x_153;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceSDiv___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("sdiv", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceSDiv___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceSDiv___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSDiv(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceSDiv___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceSDiv___lambda__2(x_1, x_10, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSDiv___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; 
x_13 = l_Std_BitVec_reduceSDiv___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_1);
return x_13;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceSDiv", 10);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceSDiv___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceSDiv), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1923_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1925_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTSDiv___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; lean_object* x_14; lean_object* x_15; lean_object* x_16; lean_object* x_17; uint32_t x_18; uint8_t x_19; lean_object* x_20; lean_object* x_21; lean_object* x_22; 
x_13 = lean_ctor_get(x_1, 1);
x_14 = l_Std_BitVec_smtSDiv(x_2, x_3, x_13);
x_15 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_15, 0, x_2);
lean_ctor_set(x_15, 1, x_14);
x_16 = l_Std_BitVec_Literal_toExpr(x_15);
x_17 = lean_box(0);
x_18 = 0;
x_19 = 1;
x_20 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_20, 0, x_16);
lean_ctor_set(x_20, 1, x_17);
lean_ctor_set_uint32(x_20, sizeof(void*)*2, x_18);
lean_ctor_set_uint8(x_20, sizeof(void*)*2 + 4, x_19);
x_21 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_21, 0, x_20);
x_22 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_22, 0, x_21);
lean_ctor_set(x_22, 1, x_12);
return x_22;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTSDiv___lambda__2(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11) {
_start:
{
lean_object* x_12; lean_object* x_13; lean_object* x_14; 
lean_dec(x_3);
lean_inc(x_1);
x_12 = l_Lean_Expr_appFn_x21(x_1);
x_13 = l_Lean_Expr_appArg_x21(x_12);
lean_dec(x_12);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_14 = l_Std_BitVec_fromExpr_x3f(x_13, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11);
if (lean_obj_tag(x_14) == 0)
{
lean_object* x_15; 
x_15 = lean_ctor_get(x_14, 0);
lean_inc(x_15);
if (lean_obj_tag(x_15) == 0)
{
uint8_t x_16; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_16 = !lean_is_exclusive(x_14);
if (x_16 == 0)
{
lean_object* x_17; lean_object* x_18; 
x_17 = lean_ctor_get(x_14, 0);
lean_dec(x_17);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_14, 0, x_18);
return x_14;
}
else
{
lean_object* x_19; lean_object* x_20; lean_object* x_21; 
x_19 = lean_ctor_get(x_14, 1);
lean_inc(x_19);
lean_dec(x_14);
x_20 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_21 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_21, 0, x_20);
lean_ctor_set(x_21, 1, x_19);
return x_21;
}
}
else
{
lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_22 = lean_ctor_get(x_14, 1);
lean_inc(x_22);
lean_dec(x_14);
x_23 = lean_ctor_get(x_15, 0);
lean_inc(x_23);
lean_dec(x_15);
x_24 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
lean_inc(x_10);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_25 = l_Std_BitVec_fromExpr_x3f(x_24, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_22);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint8_t x_40; 
x_34 = lean_ctor_get(x_25, 1);
x_35 = lean_ctor_get(x_25, 0);
lean_dec(x_35);
x_36 = lean_ctor_get(x_26, 0);
lean_inc(x_36);
lean_dec(x_26);
x_37 = lean_ctor_get(x_23, 0);
lean_inc(x_37);
x_38 = lean_ctor_get(x_23, 1);
lean_inc(x_38);
lean_dec(x_23);
x_39 = lean_ctor_get(x_36, 0);
lean_inc(x_39);
x_40 = lean_nat_dec_eq(x_37, x_39);
if (x_40 == 0)
{
lean_object* x_41; 
lean_dec(x_39);
lean_dec(x_38);
lean_dec(x_37);
lean_dec(x_36);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_41 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_41);
return x_25;
}
else
{
lean_object* x_42; lean_object* x_43; lean_object* x_44; uint8_t x_45; 
lean_free_object(x_25);
x_42 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_43 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_42, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_34);
x_44 = lean_ctor_get(x_43, 0);
lean_inc(x_44);
x_45 = lean_unbox(x_44);
lean_dec(x_44);
if (x_45 == 0)
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; 
lean_dec(x_39);
lean_dec(x_2);
x_46 = lean_ctor_get(x_43, 1);
lean_inc(x_46);
lean_dec(x_43);
x_47 = lean_box(0);
x_48 = l_Std_BitVec_reduceSMTSDiv___lambda__1(x_36, x_37, x_38, x_47, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_46);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_38);
lean_dec(x_36);
return x_48;
}
else
{
lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; lean_object* x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; lean_object* x_69; lean_object* x_70; lean_object* x_71; lean_object* x_72; lean_object* x_73; lean_object* x_74; lean_object* x_75; lean_object* x_76; lean_object* x_77; lean_object* x_78; lean_object* x_79; lean_object* x_80; lean_object* x_81; lean_object* x_82; lean_object* x_83; lean_object* x_84; lean_object* x_85; lean_object* x_86; lean_object* x_87; lean_object* x_88; lean_object* x_89; 
x_49 = lean_ctor_get(x_43, 1);
lean_inc(x_49);
lean_dec(x_43);
x_50 = l_Lean_MessageData_ofName(x_2);
x_51 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_52 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_52, 0, x_51);
lean_ctor_set(x_52, 1, x_50);
x_53 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_54 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_54, 0, x_52);
lean_ctor_set(x_54, 1, x_53);
lean_inc(x_38);
x_55 = l_Std_BitVec_toHex(x_37, x_38);
x_56 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_56, 0, x_55);
x_57 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_58 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_58, 0, x_57);
lean_ctor_set(x_58, 1, x_56);
x_59 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_60 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_60, 0, x_58);
lean_ctor_set(x_60, 1, x_59);
lean_inc(x_37);
x_61 = l_Nat_repr(x_37);
x_62 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_62, 0, x_61);
x_63 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_63, 0, x_60);
lean_ctor_set(x_63, 1, x_62);
x_64 = l_Std_Format_defWidth;
x_65 = lean_unsigned_to_nat(0u);
x_66 = lean_format_pretty(x_63, x_64, x_65, x_65);
x_67 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_68, 0, x_67);
x_69 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_69, 0, x_54);
lean_ctor_set(x_69, 1, x_68);
x_70 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_71 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
x_72 = lean_ctor_get(x_36, 1);
lean_inc(x_72);
x_73 = l_Std_BitVec_toHex(x_39, x_72);
x_74 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_74, 0, x_73);
x_75 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_75, 0, x_57);
lean_ctor_set(x_75, 1, x_74);
x_76 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_76, 0, x_75);
lean_ctor_set(x_76, 1, x_59);
x_77 = l_Nat_repr(x_39);
x_78 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_78, 0, x_77);
x_79 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_79, 0, x_76);
lean_ctor_set(x_79, 1, x_78);
x_80 = lean_format_pretty(x_79, x_64, x_65, x_65);
x_81 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_81, 0, x_80);
x_82 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_82, 0, x_81);
x_83 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_83, 0, x_71);
lean_ctor_set(x_83, 1, x_82);
x_84 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_85 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_85, 0, x_83);
lean_ctor_set(x_85, 1, x_84);
x_86 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_42, x_85, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_49);
x_87 = lean_ctor_get(x_86, 0);
lean_inc(x_87);
x_88 = lean_ctor_get(x_86, 1);
lean_inc(x_88);
lean_dec(x_86);
x_89 = l_Std_BitVec_reduceSMTSDiv___lambda__1(x_36, x_37, x_38, x_87, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_88);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_87);
lean_dec(x_38);
lean_dec(x_36);
return x_89;
}
}
}
else
{
lean_object* x_90; lean_object* x_91; lean_object* x_92; lean_object* x_93; lean_object* x_94; uint8_t x_95; 
x_90 = lean_ctor_get(x_25, 1);
lean_inc(x_90);
lean_dec(x_25);
x_91 = lean_ctor_get(x_26, 0);
lean_inc(x_91);
lean_dec(x_26);
x_92 = lean_ctor_get(x_23, 0);
lean_inc(x_92);
x_93 = lean_ctor_get(x_23, 1);
lean_inc(x_93);
lean_dec(x_23);
x_94 = lean_ctor_get(x_91, 0);
lean_inc(x_94);
x_95 = lean_nat_dec_eq(x_92, x_94);
if (x_95 == 0)
{
lean_object* x_96; lean_object* x_97; 
lean_dec(x_94);
lean_dec(x_93);
lean_dec(x_92);
lean_dec(x_91);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_96 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_97 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_97, 0, x_96);
lean_ctor_set(x_97, 1, x_90);
return x_97;
}
else
{
lean_object* x_98; lean_object* x_99; lean_object* x_100; uint8_t x_101; 
x_98 = l_Std_BitVec_reduceBin___lambda__2___closed__3;
x_99 = l_Lean_isTracingEnabledFor___at_Lean_Meta_Simp_congrArgs___spec__1(x_98, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_90);
x_100 = lean_ctor_get(x_99, 0);
lean_inc(x_100);
x_101 = lean_unbox(x_100);
lean_dec(x_100);
if (x_101 == 0)
{
lean_object* x_102; lean_object* x_103; lean_object* x_104; 
lean_dec(x_94);
lean_dec(x_2);
x_102 = lean_ctor_get(x_99, 1);
lean_inc(x_102);
lean_dec(x_99);
x_103 = lean_box(0);
x_104 = l_Std_BitVec_reduceSMTSDiv___lambda__1(x_91, x_92, x_93, x_103, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_102);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_93);
lean_dec(x_91);
return x_104;
}
else
{
lean_object* x_105; lean_object* x_106; lean_object* x_107; lean_object* x_108; lean_object* x_109; lean_object* x_110; lean_object* x_111; lean_object* x_112; lean_object* x_113; lean_object* x_114; lean_object* x_115; lean_object* x_116; lean_object* x_117; lean_object* x_118; lean_object* x_119; lean_object* x_120; lean_object* x_121; lean_object* x_122; lean_object* x_123; lean_object* x_124; lean_object* x_125; lean_object* x_126; lean_object* x_127; lean_object* x_128; lean_object* x_129; lean_object* x_130; lean_object* x_131; lean_object* x_132; lean_object* x_133; lean_object* x_134; lean_object* x_135; lean_object* x_136; lean_object* x_137; lean_object* x_138; lean_object* x_139; lean_object* x_140; lean_object* x_141; lean_object* x_142; lean_object* x_143; lean_object* x_144; lean_object* x_145; 
x_105 = lean_ctor_get(x_99, 1);
lean_inc(x_105);
lean_dec(x_99);
x_106 = l_Lean_MessageData_ofName(x_2);
x_107 = l_Std_BitVec_reduceBin___lambda__2___closed__5;
x_108 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_108, 0, x_107);
lean_ctor_set(x_108, 1, x_106);
x_109 = l_Std_BitVec_reduceBin___lambda__2___closed__7;
x_110 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_110, 0, x_108);
lean_ctor_set(x_110, 1, x_109);
lean_inc(x_93);
x_111 = l_Std_BitVec_toHex(x_92, x_93);
x_112 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_112, 0, x_111);
x_113 = l_Std_BitVec_reduceBin___lambda__2___closed__9;
x_114 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_114, 0, x_113);
lean_ctor_set(x_114, 1, x_112);
x_115 = l_Std_BitVec_reduceBin___lambda__2___closed__11;
x_116 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_116, 0, x_114);
lean_ctor_set(x_116, 1, x_115);
lean_inc(x_92);
x_117 = l_Nat_repr(x_92);
x_118 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_118, 0, x_117);
x_119 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_119, 0, x_116);
lean_ctor_set(x_119, 1, x_118);
x_120 = l_Std_Format_defWidth;
x_121 = lean_unsigned_to_nat(0u);
x_122 = lean_format_pretty(x_119, x_120, x_121, x_121);
x_123 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_123, 0, x_122);
x_124 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_124, 0, x_123);
x_125 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_125, 0, x_110);
lean_ctor_set(x_125, 1, x_124);
x_126 = l_Std_BitVec_reduceBin___lambda__2___closed__13;
x_127 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_127, 0, x_125);
lean_ctor_set(x_127, 1, x_126);
x_128 = lean_ctor_get(x_91, 1);
lean_inc(x_128);
x_129 = l_Std_BitVec_toHex(x_94, x_128);
x_130 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_130, 0, x_129);
x_131 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_131, 0, x_113);
lean_ctor_set(x_131, 1, x_130);
x_132 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_132, 0, x_131);
lean_ctor_set(x_132, 1, x_115);
x_133 = l_Nat_repr(x_94);
x_134 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_134, 0, x_133);
x_135 = lean_alloc_ctor(5, 2, 0);
lean_ctor_set(x_135, 0, x_132);
lean_ctor_set(x_135, 1, x_134);
x_136 = lean_format_pretty(x_135, x_120, x_121, x_121);
x_137 = lean_alloc_ctor(3, 1, 0);
lean_ctor_set(x_137, 0, x_136);
x_138 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_138, 0, x_137);
x_139 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_139, 0, x_127);
lean_ctor_set(x_139, 1, x_138);
x_140 = l_Std_BitVec_reduceBin___lambda__2___closed__15;
x_141 = lean_alloc_ctor(7, 2, 0);
lean_ctor_set(x_141, 0, x_139);
lean_ctor_set(x_141, 1, x_140);
x_142 = l_Lean_addTrace___at_Lean_Meta_Simp_congrArgs___spec__2(x_98, x_141, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_105);
x_143 = lean_ctor_get(x_142, 0);
lean_inc(x_143);
x_144 = lean_ctor_get(x_142, 1);
lean_inc(x_144);
lean_dec(x_142);
x_145 = l_Std_BitVec_reduceSMTSDiv___lambda__1(x_91, x_92, x_93, x_143, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_144);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_143);
lean_dec(x_93);
lean_dec(x_91);
return x_145;
}
}
}
}
}
else
{
uint8_t x_146; 
lean_dec(x_23);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
x_146 = !lean_is_exclusive(x_25);
if (x_146 == 0)
{
return x_25;
}
else
{
lean_object* x_147; lean_object* x_148; lean_object* x_149; 
x_147 = lean_ctor_get(x_25, 0);
x_148 = lean_ctor_get(x_25, 1);
lean_inc(x_148);
lean_inc(x_147);
lean_dec(x_25);
x_149 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_149, 0, x_147);
lean_ctor_set(x_149, 1, x_148);
return x_149;
}
}
}
}
else
{
uint8_t x_150; 
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_2);
lean_dec(x_1);
x_150 = !lean_is_exclusive(x_14);
if (x_150 == 0)
{
return x_14;
}
else
{
lean_object* x_151; lean_object* x_152; lean_object* x_153; 
x_151 = lean_ctor_get(x_14, 0);
x_152 = lean_ctor_get(x_14, 1);
lean_inc(x_152);
lean_inc(x_151);
lean_dec(x_14);
x_153 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_153, 0, x_151);
lean_ctor_set(x_153, 1, x_152);
return x_153;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceSMTSDiv___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("smtSDiv", 7);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceSMTSDiv___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceSMTSDiv___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTSDiv(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceSMTSDiv___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceSMTSDiv___lambda__2(x_1, x_10, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSMTSDiv___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10, lean_object* x_11, lean_object* x_12) {
_start:
{
lean_object* x_13; 
x_13 = l_Std_BitVec_reduceSMTSDiv___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12);
lean_dec(x_11);
lean_dec(x_10);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_1);
return x_13;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceSMTSDiv", 13);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceSMTSDiv___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceSMTSDiv), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1949_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1951_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetLsb___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_24 = l_Nat_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
lean_dec(x_6);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; uint8_t x_36; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = lean_ctor_get(x_22, 1);
lean_inc(x_35);
lean_dec(x_22);
x_36 = l_Nat_testBit(x_35, x_34);
lean_dec(x_34);
lean_dec(x_35);
if (x_36 == 0)
{
lean_object* x_37; 
x_37 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
lean_ctor_set(x_24, 0, x_37);
return x_24;
}
else
{
lean_object* x_38; 
x_38 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
lean_ctor_set(x_24, 0, x_38);
return x_24;
}
}
else
{
lean_object* x_39; lean_object* x_40; lean_object* x_41; uint8_t x_42; 
x_39 = lean_ctor_get(x_24, 1);
lean_inc(x_39);
lean_dec(x_24);
x_40 = lean_ctor_get(x_25, 0);
lean_inc(x_40);
lean_dec(x_25);
x_41 = lean_ctor_get(x_22, 1);
lean_inc(x_41);
lean_dec(x_22);
x_42 = l_Nat_testBit(x_41, x_40);
lean_dec(x_40);
lean_dec(x_41);
if (x_42 == 0)
{
lean_object* x_43; lean_object* x_44; 
x_43 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
x_44 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_44, 0, x_43);
lean_ctor_set(x_44, 1, x_39);
return x_44;
}
else
{
lean_object* x_45; lean_object* x_46; 
x_45 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
x_46 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_46, 0, x_45);
lean_ctor_set(x_46, 1, x_39);
return x_46;
}
}
}
}
else
{
uint8_t x_47; 
lean_dec(x_22);
x_47 = !lean_is_exclusive(x_24);
if (x_47 == 0)
{
return x_24;
}
else
{
lean_object* x_48; lean_object* x_49; lean_object* x_50; 
x_48 = lean_ctor_get(x_24, 0);
x_49 = lean_ctor_get(x_24, 1);
lean_inc(x_49);
lean_inc(x_48);
lean_dec(x_24);
x_50 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_50, 0, x_48);
lean_ctor_set(x_50, 1, x_49);
return x_50;
}
}
}
}
else
{
uint8_t x_51; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_51 = !lean_is_exclusive(x_13);
if (x_51 == 0)
{
return x_13;
}
else
{
lean_object* x_52; lean_object* x_53; lean_object* x_54; 
x_52 = lean_ctor_get(x_13, 0);
x_53 = lean_ctor_get(x_13, 1);
lean_inc(x_53);
lean_inc(x_52);
lean_dec(x_13);
x_54 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_54, 0, x_52);
lean_ctor_set(x_54, 1, x_53);
return x_54;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceGetLsb___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("getLsb", 6);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceGetLsb___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceGetLsb___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetLsb(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceGetLsb___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceGetLsb___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetLsb___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceGetLsb___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceGetLsb", 12);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceGetLsb___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceGetLsb), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1970_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1972_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetMsb___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_24 = l_Nat_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
lean_dec(x_6);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; uint8_t x_37; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = lean_ctor_get(x_22, 0);
lean_inc(x_35);
x_36 = lean_ctor_get(x_22, 1);
lean_inc(x_36);
lean_dec(x_22);
x_37 = lean_nat_dec_lt(x_34, x_35);
if (x_37 == 0)
{
lean_object* x_38; 
lean_dec(x_36);
lean_dec(x_35);
lean_dec(x_34);
x_38 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
lean_ctor_set(x_24, 0, x_38);
return x_24;
}
else
{
lean_object* x_39; lean_object* x_40; lean_object* x_41; uint8_t x_42; 
x_39 = lean_unsigned_to_nat(1u);
x_40 = lean_nat_sub(x_35, x_39);
lean_dec(x_35);
x_41 = lean_nat_sub(x_40, x_34);
lean_dec(x_34);
lean_dec(x_40);
x_42 = l_Nat_testBit(x_36, x_41);
lean_dec(x_41);
lean_dec(x_36);
if (x_42 == 0)
{
lean_object* x_43; 
x_43 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
lean_ctor_set(x_24, 0, x_43);
return x_24;
}
else
{
lean_object* x_44; 
x_44 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
lean_ctor_set(x_24, 0, x_44);
return x_24;
}
}
}
else
{
lean_object* x_45; lean_object* x_46; lean_object* x_47; lean_object* x_48; uint8_t x_49; 
x_45 = lean_ctor_get(x_24, 1);
lean_inc(x_45);
lean_dec(x_24);
x_46 = lean_ctor_get(x_25, 0);
lean_inc(x_46);
lean_dec(x_25);
x_47 = lean_ctor_get(x_22, 0);
lean_inc(x_47);
x_48 = lean_ctor_get(x_22, 1);
lean_inc(x_48);
lean_dec(x_22);
x_49 = lean_nat_dec_lt(x_46, x_47);
if (x_49 == 0)
{
lean_object* x_50; lean_object* x_51; 
lean_dec(x_48);
lean_dec(x_47);
lean_dec(x_46);
x_50 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
x_51 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_51, 0, x_50);
lean_ctor_set(x_51, 1, x_45);
return x_51;
}
else
{
lean_object* x_52; lean_object* x_53; lean_object* x_54; uint8_t x_55; 
x_52 = lean_unsigned_to_nat(1u);
x_53 = lean_nat_sub(x_47, x_52);
lean_dec(x_47);
x_54 = lean_nat_sub(x_53, x_46);
lean_dec(x_46);
lean_dec(x_53);
x_55 = l_Nat_testBit(x_48, x_54);
lean_dec(x_54);
lean_dec(x_48);
if (x_55 == 0)
{
lean_object* x_56; lean_object* x_57; 
x_56 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
x_57 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_57, 0, x_56);
lean_ctor_set(x_57, 1, x_45);
return x_57;
}
else
{
lean_object* x_58; lean_object* x_59; 
x_58 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
x_59 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_59, 0, x_58);
lean_ctor_set(x_59, 1, x_45);
return x_59;
}
}
}
}
}
else
{
uint8_t x_60; 
lean_dec(x_22);
x_60 = !lean_is_exclusive(x_24);
if (x_60 == 0)
{
return x_24;
}
else
{
lean_object* x_61; lean_object* x_62; lean_object* x_63; 
x_61 = lean_ctor_get(x_24, 0);
x_62 = lean_ctor_get(x_24, 1);
lean_inc(x_62);
lean_inc(x_61);
lean_dec(x_24);
x_63 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_63, 0, x_61);
lean_ctor_set(x_63, 1, x_62);
return x_63;
}
}
}
}
else
{
uint8_t x_64; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_64 = !lean_is_exclusive(x_13);
if (x_64 == 0)
{
return x_13;
}
else
{
lean_object* x_65; lean_object* x_66; lean_object* x_67; 
x_65 = lean_ctor_get(x_13, 0);
x_66 = lean_ctor_get(x_13, 1);
lean_inc(x_66);
lean_inc(x_65);
lean_dec(x_13);
x_67 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_67, 0, x_65);
lean_ctor_set(x_67, 1, x_66);
return x_67;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceGetMsb___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("getMsb", 6);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceGetMsb___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceGetMsb___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetMsb(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceGetMsb___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceGetMsb___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGetMsb___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceGetMsb___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceGetMsb", 12);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceGetMsb___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceGetMsb), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1991_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1993_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShiftLeft___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_24 = l_Nat_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
lean_dec(x_6);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; uint8_t x_35; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = !lean_is_exclusive(x_22);
if (x_35 == 0)
{
lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; lean_object* x_40; uint32_t x_41; uint8_t x_42; lean_object* x_43; lean_object* x_44; 
x_36 = lean_ctor_get(x_22, 0);
x_37 = lean_ctor_get(x_22, 1);
x_38 = l_Std_BitVec_shiftLeft(x_36, x_37, x_34);
lean_dec(x_34);
lean_dec(x_37);
lean_ctor_set(x_22, 1, x_38);
x_39 = l_Std_BitVec_Literal_toExpr(x_22);
x_40 = lean_box(0);
x_41 = 0;
x_42 = 1;
x_43 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_43, 0, x_39);
lean_ctor_set(x_43, 1, x_40);
lean_ctor_set_uint32(x_43, sizeof(void*)*2, x_41);
lean_ctor_set_uint8(x_43, sizeof(void*)*2 + 4, x_42);
x_44 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_44, 0, x_43);
lean_ctor_set(x_24, 0, x_44);
return x_24;
}
else
{
lean_object* x_45; lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; uint32_t x_51; uint8_t x_52; lean_object* x_53; lean_object* x_54; 
x_45 = lean_ctor_get(x_22, 0);
x_46 = lean_ctor_get(x_22, 1);
lean_inc(x_46);
lean_inc(x_45);
lean_dec(x_22);
x_47 = l_Std_BitVec_shiftLeft(x_45, x_46, x_34);
lean_dec(x_34);
lean_dec(x_46);
x_48 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_48, 0, x_45);
lean_ctor_set(x_48, 1, x_47);
x_49 = l_Std_BitVec_Literal_toExpr(x_48);
x_50 = lean_box(0);
x_51 = 0;
x_52 = 1;
x_53 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_53, 0, x_49);
lean_ctor_set(x_53, 1, x_50);
lean_ctor_set_uint32(x_53, sizeof(void*)*2, x_51);
lean_ctor_set_uint8(x_53, sizeof(void*)*2 + 4, x_52);
x_54 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_54, 0, x_53);
lean_ctor_set(x_24, 0, x_54);
return x_24;
}
}
else
{
lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; uint32_t x_64; uint8_t x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; 
x_55 = lean_ctor_get(x_24, 1);
lean_inc(x_55);
lean_dec(x_24);
x_56 = lean_ctor_get(x_25, 0);
lean_inc(x_56);
lean_dec(x_25);
x_57 = lean_ctor_get(x_22, 0);
lean_inc(x_57);
x_58 = lean_ctor_get(x_22, 1);
lean_inc(x_58);
if (lean_is_exclusive(x_22)) {
 lean_ctor_release(x_22, 0);
 lean_ctor_release(x_22, 1);
 x_59 = x_22;
} else {
 lean_dec_ref(x_22);
 x_59 = lean_box(0);
}
x_60 = l_Std_BitVec_shiftLeft(x_57, x_58, x_56);
lean_dec(x_56);
lean_dec(x_58);
if (lean_is_scalar(x_59)) {
 x_61 = lean_alloc_ctor(0, 2, 0);
} else {
 x_61 = x_59;
}
lean_ctor_set(x_61, 0, x_57);
lean_ctor_set(x_61, 1, x_60);
x_62 = l_Std_BitVec_Literal_toExpr(x_61);
x_63 = lean_box(0);
x_64 = 0;
x_65 = 1;
x_66 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_66, 0, x_62);
lean_ctor_set(x_66, 1, x_63);
lean_ctor_set_uint32(x_66, sizeof(void*)*2, x_64);
lean_ctor_set_uint8(x_66, sizeof(void*)*2 + 4, x_65);
x_67 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_68, 0, x_67);
lean_ctor_set(x_68, 1, x_55);
return x_68;
}
}
}
else
{
uint8_t x_69; 
lean_dec(x_22);
x_69 = !lean_is_exclusive(x_24);
if (x_69 == 0)
{
return x_24;
}
else
{
lean_object* x_70; lean_object* x_71; lean_object* x_72; 
x_70 = lean_ctor_get(x_24, 0);
x_71 = lean_ctor_get(x_24, 1);
lean_inc(x_71);
lean_inc(x_70);
lean_dec(x_24);
x_72 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_72, 0, x_70);
lean_ctor_set(x_72, 1, x_71);
return x_72;
}
}
}
}
else
{
uint8_t x_73; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_73 = !lean_is_exclusive(x_13);
if (x_73 == 0)
{
return x_13;
}
else
{
lean_object* x_74; lean_object* x_75; lean_object* x_76; 
x_74 = lean_ctor_get(x_13, 0);
x_75 = lean_ctor_get(x_13, 1);
lean_inc(x_75);
lean_inc(x_74);
lean_dec(x_13);
x_76 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_76, 0, x_74);
lean_ctor_set(x_76, 1, x_75);
return x_76;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceShiftLeft___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("shiftLeft", 9);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceShiftLeft___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceShiftLeft___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShiftLeft(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceShiftLeft___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceShiftLeft___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShiftLeft___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceShiftLeft___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceShiftLeft", 15);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceShiftLeft___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceShiftLeft), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2013_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2015_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUShiftRight___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_24 = l_Nat_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
lean_dec(x_6);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; uint8_t x_35; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = !lean_is_exclusive(x_22);
if (x_35 == 0)
{
lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; uint32_t x_40; uint8_t x_41; lean_object* x_42; lean_object* x_43; 
x_36 = lean_ctor_get(x_22, 1);
x_37 = lean_nat_shiftr(x_36, x_34);
lean_dec(x_34);
lean_dec(x_36);
lean_ctor_set(x_22, 1, x_37);
x_38 = l_Std_BitVec_Literal_toExpr(x_22);
x_39 = lean_box(0);
x_40 = 0;
x_41 = 1;
x_42 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_42, 0, x_38);
lean_ctor_set(x_42, 1, x_39);
lean_ctor_set_uint32(x_42, sizeof(void*)*2, x_40);
lean_ctor_set_uint8(x_42, sizeof(void*)*2 + 4, x_41);
x_43 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_43, 0, x_42);
lean_ctor_set(x_24, 0, x_43);
return x_24;
}
else
{
lean_object* x_44; lean_object* x_45; lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; uint32_t x_50; uint8_t x_51; lean_object* x_52; lean_object* x_53; 
x_44 = lean_ctor_get(x_22, 0);
x_45 = lean_ctor_get(x_22, 1);
lean_inc(x_45);
lean_inc(x_44);
lean_dec(x_22);
x_46 = lean_nat_shiftr(x_45, x_34);
lean_dec(x_34);
lean_dec(x_45);
x_47 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_47, 0, x_44);
lean_ctor_set(x_47, 1, x_46);
x_48 = l_Std_BitVec_Literal_toExpr(x_47);
x_49 = lean_box(0);
x_50 = 0;
x_51 = 1;
x_52 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_52, 0, x_48);
lean_ctor_set(x_52, 1, x_49);
lean_ctor_set_uint32(x_52, sizeof(void*)*2, x_50);
lean_ctor_set_uint8(x_52, sizeof(void*)*2 + 4, x_51);
x_53 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_53, 0, x_52);
lean_ctor_set(x_24, 0, x_53);
return x_24;
}
}
else
{
lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; uint32_t x_63; uint8_t x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; 
x_54 = lean_ctor_get(x_24, 1);
lean_inc(x_54);
lean_dec(x_24);
x_55 = lean_ctor_get(x_25, 0);
lean_inc(x_55);
lean_dec(x_25);
x_56 = lean_ctor_get(x_22, 0);
lean_inc(x_56);
x_57 = lean_ctor_get(x_22, 1);
lean_inc(x_57);
if (lean_is_exclusive(x_22)) {
 lean_ctor_release(x_22, 0);
 lean_ctor_release(x_22, 1);
 x_58 = x_22;
} else {
 lean_dec_ref(x_22);
 x_58 = lean_box(0);
}
x_59 = lean_nat_shiftr(x_57, x_55);
lean_dec(x_55);
lean_dec(x_57);
if (lean_is_scalar(x_58)) {
 x_60 = lean_alloc_ctor(0, 2, 0);
} else {
 x_60 = x_58;
}
lean_ctor_set(x_60, 0, x_56);
lean_ctor_set(x_60, 1, x_59);
x_61 = l_Std_BitVec_Literal_toExpr(x_60);
x_62 = lean_box(0);
x_63 = 0;
x_64 = 1;
x_65 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_65, 0, x_61);
lean_ctor_set(x_65, 1, x_62);
lean_ctor_set_uint32(x_65, sizeof(void*)*2, x_63);
lean_ctor_set_uint8(x_65, sizeof(void*)*2 + 4, x_64);
x_66 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_66, 0, x_65);
x_67 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_67, 0, x_66);
lean_ctor_set(x_67, 1, x_54);
return x_67;
}
}
}
else
{
uint8_t x_68; 
lean_dec(x_22);
x_68 = !lean_is_exclusive(x_24);
if (x_68 == 0)
{
return x_24;
}
else
{
lean_object* x_69; lean_object* x_70; lean_object* x_71; 
x_69 = lean_ctor_get(x_24, 0);
x_70 = lean_ctor_get(x_24, 1);
lean_inc(x_70);
lean_inc(x_69);
lean_dec(x_24);
x_71 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
return x_71;
}
}
}
}
else
{
uint8_t x_72; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_72 = !lean_is_exclusive(x_13);
if (x_72 == 0)
{
return x_13;
}
else
{
lean_object* x_73; lean_object* x_74; lean_object* x_75; 
x_73 = lean_ctor_get(x_13, 0);
x_74 = lean_ctor_get(x_13, 1);
lean_inc(x_74);
lean_inc(x_73);
lean_dec(x_13);
x_75 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_75, 0, x_73);
lean_ctor_set(x_75, 1, x_74);
return x_75;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceUShiftRight___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("ushiftRight", 11);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceUShiftRight___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceUShiftRight___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUShiftRight(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceUShiftRight___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceUShiftRight___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceUShiftRight___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceUShiftRight___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceUShiftRight", 17);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceUShiftRight___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceUShiftRight), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2035_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2037_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSShiftRight___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_24 = l_Nat_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
lean_dec(x_6);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; uint8_t x_35; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = !lean_is_exclusive(x_22);
if (x_35 == 0)
{
lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; lean_object* x_40; uint32_t x_41; uint8_t x_42; lean_object* x_43; lean_object* x_44; 
x_36 = lean_ctor_get(x_22, 0);
x_37 = lean_ctor_get(x_22, 1);
x_38 = l_Std_BitVec_sshiftRight(x_36, x_37, x_34);
lean_dec(x_34);
lean_ctor_set(x_22, 1, x_38);
x_39 = l_Std_BitVec_Literal_toExpr(x_22);
x_40 = lean_box(0);
x_41 = 0;
x_42 = 1;
x_43 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_43, 0, x_39);
lean_ctor_set(x_43, 1, x_40);
lean_ctor_set_uint32(x_43, sizeof(void*)*2, x_41);
lean_ctor_set_uint8(x_43, sizeof(void*)*2 + 4, x_42);
x_44 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_44, 0, x_43);
lean_ctor_set(x_24, 0, x_44);
return x_24;
}
else
{
lean_object* x_45; lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; uint32_t x_51; uint8_t x_52; lean_object* x_53; lean_object* x_54; 
x_45 = lean_ctor_get(x_22, 0);
x_46 = lean_ctor_get(x_22, 1);
lean_inc(x_46);
lean_inc(x_45);
lean_dec(x_22);
x_47 = l_Std_BitVec_sshiftRight(x_45, x_46, x_34);
lean_dec(x_34);
x_48 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_48, 0, x_45);
lean_ctor_set(x_48, 1, x_47);
x_49 = l_Std_BitVec_Literal_toExpr(x_48);
x_50 = lean_box(0);
x_51 = 0;
x_52 = 1;
x_53 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_53, 0, x_49);
lean_ctor_set(x_53, 1, x_50);
lean_ctor_set_uint32(x_53, sizeof(void*)*2, x_51);
lean_ctor_set_uint8(x_53, sizeof(void*)*2 + 4, x_52);
x_54 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_54, 0, x_53);
lean_ctor_set(x_24, 0, x_54);
return x_24;
}
}
else
{
lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; uint32_t x_64; uint8_t x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; 
x_55 = lean_ctor_get(x_24, 1);
lean_inc(x_55);
lean_dec(x_24);
x_56 = lean_ctor_get(x_25, 0);
lean_inc(x_56);
lean_dec(x_25);
x_57 = lean_ctor_get(x_22, 0);
lean_inc(x_57);
x_58 = lean_ctor_get(x_22, 1);
lean_inc(x_58);
if (lean_is_exclusive(x_22)) {
 lean_ctor_release(x_22, 0);
 lean_ctor_release(x_22, 1);
 x_59 = x_22;
} else {
 lean_dec_ref(x_22);
 x_59 = lean_box(0);
}
x_60 = l_Std_BitVec_sshiftRight(x_57, x_58, x_56);
lean_dec(x_56);
if (lean_is_scalar(x_59)) {
 x_61 = lean_alloc_ctor(0, 2, 0);
} else {
 x_61 = x_59;
}
lean_ctor_set(x_61, 0, x_57);
lean_ctor_set(x_61, 1, x_60);
x_62 = l_Std_BitVec_Literal_toExpr(x_61);
x_63 = lean_box(0);
x_64 = 0;
x_65 = 1;
x_66 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_66, 0, x_62);
lean_ctor_set(x_66, 1, x_63);
lean_ctor_set_uint32(x_66, sizeof(void*)*2, x_64);
lean_ctor_set_uint8(x_66, sizeof(void*)*2 + 4, x_65);
x_67 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_68, 0, x_67);
lean_ctor_set(x_68, 1, x_55);
return x_68;
}
}
}
else
{
uint8_t x_69; 
lean_dec(x_22);
x_69 = !lean_is_exclusive(x_24);
if (x_69 == 0)
{
return x_24;
}
else
{
lean_object* x_70; lean_object* x_71; lean_object* x_72; 
x_70 = lean_ctor_get(x_24, 0);
x_71 = lean_ctor_get(x_24, 1);
lean_inc(x_71);
lean_inc(x_70);
lean_dec(x_24);
x_72 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_72, 0, x_70);
lean_ctor_set(x_72, 1, x_71);
return x_72;
}
}
}
}
else
{
uint8_t x_73; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_73 = !lean_is_exclusive(x_13);
if (x_73 == 0)
{
return x_13;
}
else
{
lean_object* x_74; lean_object* x_75; lean_object* x_76; 
x_74 = lean_ctor_get(x_13, 0);
x_75 = lean_ctor_get(x_13, 1);
lean_inc(x_75);
lean_inc(x_74);
lean_dec(x_13);
x_76 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_76, 0, x_74);
lean_ctor_set(x_76, 1, x_75);
return x_76;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceSShiftRight___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("sshiftRight", 11);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceSShiftRight___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceSShiftRight___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSShiftRight(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceSShiftRight___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceSShiftRight___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSShiftRight___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceSShiftRight___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceSShiftRight", 17);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceSShiftRight___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceSShiftRight), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2057_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2059_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
static lean_object* _init_l_Std_BitVec_reduceHShiftLeft___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("HShiftLeft", 10);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceHShiftLeft___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("hShiftLeft", 10);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceHShiftLeft___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceHShiftLeft___closed__1;
x_2 = l_Std_BitVec_reduceHShiftLeft___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceHShiftLeft(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceHShiftLeft___closed__3;
x_11 = lean_unsigned_to_nat(6u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceShiftLeft___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceHShiftLeft___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; 
x_10 = l_Std_BitVec_reduceHShiftLeft(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_10;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceHShiftLeft", 16);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceHShiftLeft___closed__3;
x_2 = lean_unsigned_to_nat(6u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = lean_unsigned_to_nat(8u);
x_2 = lean_mk_empty_array_with_capacity(x_1);
return x_2;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__7;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__8;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__10() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__9;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__11() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__10;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__12() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__11;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__13() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceHShiftLeft___boxed), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__12;
x_4 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__13;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2099_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__13;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2101_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__13;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
static lean_object* _init_l_Std_BitVec_reduceHShiftRight___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("HShiftRight", 11);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceHShiftRight___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("hShiftRight", 11);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceHShiftRight___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceHShiftRight___closed__1;
x_2 = l_Std_BitVec_reduceHShiftRight___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceHShiftRight(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceHShiftRight___closed__3;
x_11 = lean_unsigned_to_nat(6u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceUShiftRight___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceHShiftRight___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; 
x_10 = l_Std_BitVec_reduceHShiftRight(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_10;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceHShiftRight", 17);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceHShiftRight___closed__3;
x_2 = lean_unsigned_to_nat(6u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__6;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__8;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__10() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__9;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__11() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__10;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__12() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceHShiftRight___boxed), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__11;
x_4 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__12;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2141_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__12;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2143_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__12;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceRotateLeft___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_24 = l_Nat_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
lean_dec(x_6);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; uint8_t x_35; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = !lean_is_exclusive(x_22);
if (x_35 == 0)
{
lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; lean_object* x_40; uint32_t x_41; uint8_t x_42; lean_object* x_43; lean_object* x_44; 
x_36 = lean_ctor_get(x_22, 0);
x_37 = lean_ctor_get(x_22, 1);
x_38 = l_Std_BitVec_rotateLeft(x_36, x_37, x_34);
lean_dec(x_34);
lean_dec(x_37);
lean_ctor_set(x_22, 1, x_38);
x_39 = l_Std_BitVec_Literal_toExpr(x_22);
x_40 = lean_box(0);
x_41 = 0;
x_42 = 1;
x_43 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_43, 0, x_39);
lean_ctor_set(x_43, 1, x_40);
lean_ctor_set_uint32(x_43, sizeof(void*)*2, x_41);
lean_ctor_set_uint8(x_43, sizeof(void*)*2 + 4, x_42);
x_44 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_44, 0, x_43);
lean_ctor_set(x_24, 0, x_44);
return x_24;
}
else
{
lean_object* x_45; lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; uint32_t x_51; uint8_t x_52; lean_object* x_53; lean_object* x_54; 
x_45 = lean_ctor_get(x_22, 0);
x_46 = lean_ctor_get(x_22, 1);
lean_inc(x_46);
lean_inc(x_45);
lean_dec(x_22);
x_47 = l_Std_BitVec_rotateLeft(x_45, x_46, x_34);
lean_dec(x_34);
lean_dec(x_46);
x_48 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_48, 0, x_45);
lean_ctor_set(x_48, 1, x_47);
x_49 = l_Std_BitVec_Literal_toExpr(x_48);
x_50 = lean_box(0);
x_51 = 0;
x_52 = 1;
x_53 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_53, 0, x_49);
lean_ctor_set(x_53, 1, x_50);
lean_ctor_set_uint32(x_53, sizeof(void*)*2, x_51);
lean_ctor_set_uint8(x_53, sizeof(void*)*2 + 4, x_52);
x_54 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_54, 0, x_53);
lean_ctor_set(x_24, 0, x_54);
return x_24;
}
}
else
{
lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; uint32_t x_64; uint8_t x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; 
x_55 = lean_ctor_get(x_24, 1);
lean_inc(x_55);
lean_dec(x_24);
x_56 = lean_ctor_get(x_25, 0);
lean_inc(x_56);
lean_dec(x_25);
x_57 = lean_ctor_get(x_22, 0);
lean_inc(x_57);
x_58 = lean_ctor_get(x_22, 1);
lean_inc(x_58);
if (lean_is_exclusive(x_22)) {
 lean_ctor_release(x_22, 0);
 lean_ctor_release(x_22, 1);
 x_59 = x_22;
} else {
 lean_dec_ref(x_22);
 x_59 = lean_box(0);
}
x_60 = l_Std_BitVec_rotateLeft(x_57, x_58, x_56);
lean_dec(x_56);
lean_dec(x_58);
if (lean_is_scalar(x_59)) {
 x_61 = lean_alloc_ctor(0, 2, 0);
} else {
 x_61 = x_59;
}
lean_ctor_set(x_61, 0, x_57);
lean_ctor_set(x_61, 1, x_60);
x_62 = l_Std_BitVec_Literal_toExpr(x_61);
x_63 = lean_box(0);
x_64 = 0;
x_65 = 1;
x_66 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_66, 0, x_62);
lean_ctor_set(x_66, 1, x_63);
lean_ctor_set_uint32(x_66, sizeof(void*)*2, x_64);
lean_ctor_set_uint8(x_66, sizeof(void*)*2 + 4, x_65);
x_67 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_68, 0, x_67);
lean_ctor_set(x_68, 1, x_55);
return x_68;
}
}
}
else
{
uint8_t x_69; 
lean_dec(x_22);
x_69 = !lean_is_exclusive(x_24);
if (x_69 == 0)
{
return x_24;
}
else
{
lean_object* x_70; lean_object* x_71; lean_object* x_72; 
x_70 = lean_ctor_get(x_24, 0);
x_71 = lean_ctor_get(x_24, 1);
lean_inc(x_71);
lean_inc(x_70);
lean_dec(x_24);
x_72 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_72, 0, x_70);
lean_ctor_set(x_72, 1, x_71);
return x_72;
}
}
}
}
else
{
uint8_t x_73; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_73 = !lean_is_exclusive(x_13);
if (x_73 == 0)
{
return x_13;
}
else
{
lean_object* x_74; lean_object* x_75; lean_object* x_76; 
x_74 = lean_ctor_get(x_13, 0);
x_75 = lean_ctor_get(x_13, 1);
lean_inc(x_75);
lean_inc(x_74);
lean_dec(x_13);
x_76 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_76, 0, x_74);
lean_ctor_set(x_76, 1, x_75);
return x_76;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceRotateLeft___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("rotateLeft", 10);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceRotateLeft___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceRotateLeft___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceRotateLeft(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceRotateLeft___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceRotateLeft___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceRotateLeft___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceRotateLeft___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceRotateLeft", 16);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceRotateLeft___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceRotateLeft), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2163_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2165_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceRotateRight___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_24 = l_Nat_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
lean_dec(x_6);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; uint8_t x_35; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = !lean_is_exclusive(x_22);
if (x_35 == 0)
{
lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; lean_object* x_40; uint32_t x_41; uint8_t x_42; lean_object* x_43; lean_object* x_44; 
x_36 = lean_ctor_get(x_22, 0);
x_37 = lean_ctor_get(x_22, 1);
x_38 = l_Std_BitVec_rotateRight(x_36, x_37, x_34);
lean_dec(x_34);
lean_dec(x_37);
lean_ctor_set(x_22, 1, x_38);
x_39 = l_Std_BitVec_Literal_toExpr(x_22);
x_40 = lean_box(0);
x_41 = 0;
x_42 = 1;
x_43 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_43, 0, x_39);
lean_ctor_set(x_43, 1, x_40);
lean_ctor_set_uint32(x_43, sizeof(void*)*2, x_41);
lean_ctor_set_uint8(x_43, sizeof(void*)*2 + 4, x_42);
x_44 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_44, 0, x_43);
lean_ctor_set(x_24, 0, x_44);
return x_24;
}
else
{
lean_object* x_45; lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; uint32_t x_51; uint8_t x_52; lean_object* x_53; lean_object* x_54; 
x_45 = lean_ctor_get(x_22, 0);
x_46 = lean_ctor_get(x_22, 1);
lean_inc(x_46);
lean_inc(x_45);
lean_dec(x_22);
x_47 = l_Std_BitVec_rotateRight(x_45, x_46, x_34);
lean_dec(x_34);
lean_dec(x_46);
x_48 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_48, 0, x_45);
lean_ctor_set(x_48, 1, x_47);
x_49 = l_Std_BitVec_Literal_toExpr(x_48);
x_50 = lean_box(0);
x_51 = 0;
x_52 = 1;
x_53 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_53, 0, x_49);
lean_ctor_set(x_53, 1, x_50);
lean_ctor_set_uint32(x_53, sizeof(void*)*2, x_51);
lean_ctor_set_uint8(x_53, sizeof(void*)*2 + 4, x_52);
x_54 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_54, 0, x_53);
lean_ctor_set(x_24, 0, x_54);
return x_24;
}
}
else
{
lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; uint32_t x_64; uint8_t x_65; lean_object* x_66; lean_object* x_67; lean_object* x_68; 
x_55 = lean_ctor_get(x_24, 1);
lean_inc(x_55);
lean_dec(x_24);
x_56 = lean_ctor_get(x_25, 0);
lean_inc(x_56);
lean_dec(x_25);
x_57 = lean_ctor_get(x_22, 0);
lean_inc(x_57);
x_58 = lean_ctor_get(x_22, 1);
lean_inc(x_58);
if (lean_is_exclusive(x_22)) {
 lean_ctor_release(x_22, 0);
 lean_ctor_release(x_22, 1);
 x_59 = x_22;
} else {
 lean_dec_ref(x_22);
 x_59 = lean_box(0);
}
x_60 = l_Std_BitVec_rotateRight(x_57, x_58, x_56);
lean_dec(x_56);
lean_dec(x_58);
if (lean_is_scalar(x_59)) {
 x_61 = lean_alloc_ctor(0, 2, 0);
} else {
 x_61 = x_59;
}
lean_ctor_set(x_61, 0, x_57);
lean_ctor_set(x_61, 1, x_60);
x_62 = l_Std_BitVec_Literal_toExpr(x_61);
x_63 = lean_box(0);
x_64 = 0;
x_65 = 1;
x_66 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_66, 0, x_62);
lean_ctor_set(x_66, 1, x_63);
lean_ctor_set_uint32(x_66, sizeof(void*)*2, x_64);
lean_ctor_set_uint8(x_66, sizeof(void*)*2 + 4, x_65);
x_67 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_67, 0, x_66);
x_68 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_68, 0, x_67);
lean_ctor_set(x_68, 1, x_55);
return x_68;
}
}
}
else
{
uint8_t x_69; 
lean_dec(x_22);
x_69 = !lean_is_exclusive(x_24);
if (x_69 == 0)
{
return x_24;
}
else
{
lean_object* x_70; lean_object* x_71; lean_object* x_72; 
x_70 = lean_ctor_get(x_24, 0);
x_71 = lean_ctor_get(x_24, 1);
lean_inc(x_71);
lean_inc(x_70);
lean_dec(x_24);
x_72 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_72, 0, x_70);
lean_ctor_set(x_72, 1, x_71);
return x_72;
}
}
}
}
else
{
uint8_t x_73; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_73 = !lean_is_exclusive(x_13);
if (x_73 == 0)
{
return x_13;
}
else
{
lean_object* x_74; lean_object* x_75; lean_object* x_76; 
x_74 = lean_ctor_get(x_13, 0);
x_75 = lean_ctor_get(x_13, 1);
lean_inc(x_75);
lean_inc(x_74);
lean_dec(x_13);
x_76 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_76, 0, x_74);
lean_ctor_set(x_76, 1, x_75);
return x_76;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceRotateRight___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("rotateRight", 11);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceRotateRight___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceRotateRight___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceRotateRight(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceRotateRight___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceRotateRight___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceRotateRight___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceRotateRight___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceRotateRight", 17);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceRotateRight___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceRotateRight), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2185_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2187_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAppend___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_24 = l_Std_BitVec_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; lean_object* x_40; lean_object* x_41; lean_object* x_42; lean_object* x_43; uint32_t x_44; uint8_t x_45; lean_object* x_46; lean_object* x_47; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = lean_ctor_get(x_22, 0);
lean_inc(x_35);
x_36 = lean_ctor_get(x_34, 0);
lean_inc(x_36);
x_37 = lean_nat_add(x_35, x_36);
lean_dec(x_35);
x_38 = lean_ctor_get(x_22, 1);
lean_inc(x_38);
lean_dec(x_22);
x_39 = lean_ctor_get(x_34, 1);
lean_inc(x_39);
lean_dec(x_34);
x_40 = l_Std_BitVec_append___rarg(x_36, x_38, x_39);
lean_dec(x_39);
lean_dec(x_38);
lean_dec(x_36);
x_41 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_41, 0, x_37);
lean_ctor_set(x_41, 1, x_40);
x_42 = l_Std_BitVec_Literal_toExpr(x_41);
x_43 = lean_box(0);
x_44 = 0;
x_45 = 1;
x_46 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_46, 0, x_42);
lean_ctor_set(x_46, 1, x_43);
lean_ctor_set_uint32(x_46, sizeof(void*)*2, x_44);
lean_ctor_set_uint8(x_46, sizeof(void*)*2 + 4, x_45);
x_47 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_47, 0, x_46);
lean_ctor_set(x_24, 0, x_47);
return x_24;
}
else
{
lean_object* x_48; lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; lean_object* x_58; uint32_t x_59; uint8_t x_60; lean_object* x_61; lean_object* x_62; lean_object* x_63; 
x_48 = lean_ctor_get(x_24, 1);
lean_inc(x_48);
lean_dec(x_24);
x_49 = lean_ctor_get(x_25, 0);
lean_inc(x_49);
lean_dec(x_25);
x_50 = lean_ctor_get(x_22, 0);
lean_inc(x_50);
x_51 = lean_ctor_get(x_49, 0);
lean_inc(x_51);
x_52 = lean_nat_add(x_50, x_51);
lean_dec(x_50);
x_53 = lean_ctor_get(x_22, 1);
lean_inc(x_53);
lean_dec(x_22);
x_54 = lean_ctor_get(x_49, 1);
lean_inc(x_54);
lean_dec(x_49);
x_55 = l_Std_BitVec_append___rarg(x_51, x_53, x_54);
lean_dec(x_54);
lean_dec(x_53);
lean_dec(x_51);
x_56 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_56, 0, x_52);
lean_ctor_set(x_56, 1, x_55);
x_57 = l_Std_BitVec_Literal_toExpr(x_56);
x_58 = lean_box(0);
x_59 = 0;
x_60 = 1;
x_61 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_61, 0, x_57);
lean_ctor_set(x_61, 1, x_58);
lean_ctor_set_uint32(x_61, sizeof(void*)*2, x_59);
lean_ctor_set_uint8(x_61, sizeof(void*)*2 + 4, x_60);
x_62 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_62, 0, x_61);
x_63 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_63, 0, x_62);
lean_ctor_set(x_63, 1, x_48);
return x_63;
}
}
}
else
{
uint8_t x_64; 
lean_dec(x_22);
x_64 = !lean_is_exclusive(x_24);
if (x_64 == 0)
{
return x_24;
}
else
{
lean_object* x_65; lean_object* x_66; lean_object* x_67; 
x_65 = lean_ctor_get(x_24, 0);
x_66 = lean_ctor_get(x_24, 1);
lean_inc(x_66);
lean_inc(x_65);
lean_dec(x_24);
x_67 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_67, 0, x_65);
lean_ctor_set(x_67, 1, x_66);
return x_67;
}
}
}
}
else
{
uint8_t x_68; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_68 = !lean_is_exclusive(x_13);
if (x_68 == 0)
{
return x_13;
}
else
{
lean_object* x_69; lean_object* x_70; lean_object* x_71; 
x_69 = lean_ctor_get(x_13, 0);
x_70 = lean_ctor_get(x_13, 1);
lean_inc(x_70);
lean_inc(x_69);
lean_dec(x_13);
x_71 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
return x_71;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceAppend___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("HAppend", 7);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceAppend___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("hAppend", 7);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceAppend___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceAppend___closed__1;
x_2 = l_Std_BitVec_reduceAppend___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAppend(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceAppend___closed__3;
x_11 = lean_unsigned_to_nat(6u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceAppend___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAppend___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceAppend___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceAppend", 12);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceAppend___closed__3;
x_2 = lean_unsigned_to_nat(6u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__6;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__8;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__10() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__9;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__11() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__10;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__12() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceAppend), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__11;
x_4 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__12;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2352_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__12;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2354_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__12;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceCast___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; 
x_11 = l_Lean_Expr_appArg_x21(x_1);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_12 = l_Std_BitVec_fromExpr_x3f(x_11, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_12) == 0)
{
lean_object* x_13; 
x_13 = lean_ctor_get(x_12, 0);
lean_inc(x_13);
if (lean_obj_tag(x_13) == 0)
{
uint8_t x_14; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_14 = !lean_is_exclusive(x_12);
if (x_14 == 0)
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_ctor_get(x_12, 0);
lean_dec(x_15);
x_16 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_12, 0, x_16);
return x_12;
}
else
{
lean_object* x_17; lean_object* x_18; lean_object* x_19; 
x_17 = lean_ctor_get(x_12, 1);
lean_inc(x_17);
lean_dec(x_12);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_19 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_19, 0, x_18);
lean_ctor_set(x_19, 1, x_17);
return x_19;
}
}
else
{
lean_object* x_20; lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_20 = lean_ctor_get(x_12, 1);
lean_inc(x_20);
lean_dec(x_12);
x_21 = lean_ctor_get(x_13, 0);
lean_inc(x_21);
lean_dec(x_13);
x_22 = l_Lean_Expr_appFn_x21(x_1);
x_23 = l_Lean_Expr_appFn_x21(x_22);
x_24 = l_Lean_Expr_appArg_x21(x_23);
lean_dec(x_23);
x_25 = l_Nat_fromExpr_x3f(x_24, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_20);
lean_dec(x_6);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_21);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; lean_object* x_40; uint32_t x_41; uint8_t x_42; lean_object* x_43; lean_object* x_44; 
x_34 = lean_ctor_get(x_25, 0);
lean_dec(x_34);
x_35 = lean_ctor_get(x_26, 0);
lean_inc(x_35);
lean_dec(x_26);
x_36 = lean_ctor_get(x_21, 1);
lean_inc(x_36);
lean_dec(x_21);
x_37 = l_Std_BitVec_ofNat(x_35, x_36);
lean_dec(x_36);
x_38 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_38, 0, x_35);
lean_ctor_set(x_38, 1, x_37);
x_39 = l_Std_BitVec_Literal_toExpr(x_38);
x_40 = lean_box(0);
x_41 = 0;
x_42 = 1;
x_43 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_43, 0, x_39);
lean_ctor_set(x_43, 1, x_40);
lean_ctor_set_uint32(x_43, sizeof(void*)*2, x_41);
lean_ctor_set_uint8(x_43, sizeof(void*)*2 + 4, x_42);
x_44 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_44, 0, x_43);
lean_ctor_set(x_25, 0, x_44);
return x_25;
}
else
{
lean_object* x_45; lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; lean_object* x_51; uint32_t x_52; uint8_t x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; 
x_45 = lean_ctor_get(x_25, 1);
lean_inc(x_45);
lean_dec(x_25);
x_46 = lean_ctor_get(x_26, 0);
lean_inc(x_46);
lean_dec(x_26);
x_47 = lean_ctor_get(x_21, 1);
lean_inc(x_47);
lean_dec(x_21);
x_48 = l_Std_BitVec_ofNat(x_46, x_47);
lean_dec(x_47);
x_49 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_49, 0, x_46);
lean_ctor_set(x_49, 1, x_48);
x_50 = l_Std_BitVec_Literal_toExpr(x_49);
x_51 = lean_box(0);
x_52 = 0;
x_53 = 1;
x_54 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_54, 0, x_50);
lean_ctor_set(x_54, 1, x_51);
lean_ctor_set_uint32(x_54, sizeof(void*)*2, x_52);
lean_ctor_set_uint8(x_54, sizeof(void*)*2 + 4, x_53);
x_55 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_55, 0, x_54);
x_56 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_56, 0, x_55);
lean_ctor_set(x_56, 1, x_45);
return x_56;
}
}
}
else
{
uint8_t x_57; 
lean_dec(x_21);
x_57 = !lean_is_exclusive(x_25);
if (x_57 == 0)
{
return x_25;
}
else
{
lean_object* x_58; lean_object* x_59; lean_object* x_60; 
x_58 = lean_ctor_get(x_25, 0);
x_59 = lean_ctor_get(x_25, 1);
lean_inc(x_59);
lean_inc(x_58);
lean_dec(x_25);
x_60 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_60, 0, x_58);
lean_ctor_set(x_60, 1, x_59);
return x_60;
}
}
}
}
else
{
uint8_t x_61; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_61 = !lean_is_exclusive(x_12);
if (x_61 == 0)
{
return x_12;
}
else
{
lean_object* x_62; lean_object* x_63; lean_object* x_64; 
x_62 = lean_ctor_get(x_12, 0);
x_63 = lean_ctor_get(x_12, 1);
lean_inc(x_63);
lean_inc(x_62);
lean_dec(x_12);
x_64 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_64, 0, x_62);
lean_ctor_set(x_64, 1, x_63);
return x_64;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceCast___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("cast", 4);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceCast___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceCast___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceCast(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceCast___closed__2;
x_11 = lean_unsigned_to_nat(4u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceCast___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceCast___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceCast___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceCast", 10);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceCast___closed__2;
x_2 = lean_unsigned_to_nat(4u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__5;
x_2 = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__9() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceCast), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__8;
x_4 = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__9;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2507_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__9;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2509_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__9;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToNat___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; 
x_11 = l_Lean_Expr_appArg_x21(x_1);
x_12 = l_Std_BitVec_fromExpr_x3f(x_11, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_12) == 0)
{
lean_object* x_13; 
x_13 = lean_ctor_get(x_12, 0);
lean_inc(x_13);
if (lean_obj_tag(x_13) == 0)
{
uint8_t x_14; 
x_14 = !lean_is_exclusive(x_12);
if (x_14 == 0)
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_ctor_get(x_12, 0);
lean_dec(x_15);
x_16 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_12, 0, x_16);
return x_12;
}
else
{
lean_object* x_17; lean_object* x_18; lean_object* x_19; 
x_17 = lean_ctor_get(x_12, 1);
lean_inc(x_17);
lean_dec(x_12);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_19 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_19, 0, x_18);
lean_ctor_set(x_19, 1, x_17);
return x_19;
}
}
else
{
uint8_t x_20; 
x_20 = !lean_is_exclusive(x_12);
if (x_20 == 0)
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; uint32_t x_26; uint8_t x_27; lean_object* x_28; lean_object* x_29; 
x_21 = lean_ctor_get(x_12, 0);
lean_dec(x_21);
x_22 = lean_ctor_get(x_13, 0);
lean_inc(x_22);
lean_dec(x_13);
x_23 = lean_ctor_get(x_22, 1);
lean_inc(x_23);
lean_dec(x_22);
x_24 = l_Lean_mkNatLit(x_23);
x_25 = lean_box(0);
x_26 = 0;
x_27 = 1;
x_28 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_28, 0, x_24);
lean_ctor_set(x_28, 1, x_25);
lean_ctor_set_uint32(x_28, sizeof(void*)*2, x_26);
lean_ctor_set_uint8(x_28, sizeof(void*)*2 + 4, x_27);
x_29 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_29, 0, x_28);
lean_ctor_set(x_12, 0, x_29);
return x_12;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; lean_object* x_33; lean_object* x_34; uint32_t x_35; uint8_t x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; 
x_30 = lean_ctor_get(x_12, 1);
lean_inc(x_30);
lean_dec(x_12);
x_31 = lean_ctor_get(x_13, 0);
lean_inc(x_31);
lean_dec(x_13);
x_32 = lean_ctor_get(x_31, 1);
lean_inc(x_32);
lean_dec(x_31);
x_33 = l_Lean_mkNatLit(x_32);
x_34 = lean_box(0);
x_35 = 0;
x_36 = 1;
x_37 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_37, 0, x_33);
lean_ctor_set(x_37, 1, x_34);
lean_ctor_set_uint32(x_37, sizeof(void*)*2, x_35);
lean_ctor_set_uint8(x_37, sizeof(void*)*2 + 4, x_36);
x_38 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_38, 0, x_37);
x_39 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_39, 0, x_38);
lean_ctor_set(x_39, 1, x_30);
return x_39;
}
}
}
else
{
uint8_t x_40; 
x_40 = !lean_is_exclusive(x_12);
if (x_40 == 0)
{
return x_12;
}
else
{
lean_object* x_41; lean_object* x_42; lean_object* x_43; 
x_41 = lean_ctor_get(x_12, 0);
x_42 = lean_ctor_get(x_12, 1);
lean_inc(x_42);
lean_inc(x_41);
lean_dec(x_12);
x_43 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_43, 0, x_41);
lean_ctor_set(x_43, 1, x_42);
return x_43;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceToNat___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("toNat", 5);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceToNat___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceToNat___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToNat(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceToNat___closed__2;
x_11 = lean_unsigned_to_nat(2u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceToNat___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToNat___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceToNat___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
return x_11;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToNat___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; 
x_10 = l_Std_BitVec_reduceToNat(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_1);
return x_10;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceToNat", 11);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceToNat___closed__2;
x_2 = lean_unsigned_to_nat(2u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__7() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceToNat___boxed), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__6;
x_4 = l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__7;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2615_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__7;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2617_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__7;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToInt___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; 
x_11 = l_Lean_Expr_appArg_x21(x_1);
x_12 = l_Std_BitVec_fromExpr_x3f(x_11, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_12) == 0)
{
lean_object* x_13; 
x_13 = lean_ctor_get(x_12, 0);
lean_inc(x_13);
if (lean_obj_tag(x_13) == 0)
{
uint8_t x_14; 
x_14 = !lean_is_exclusive(x_12);
if (x_14 == 0)
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_ctor_get(x_12, 0);
lean_dec(x_15);
x_16 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_12, 0, x_16);
return x_12;
}
else
{
lean_object* x_17; lean_object* x_18; lean_object* x_19; 
x_17 = lean_ctor_get(x_12, 1);
lean_inc(x_17);
lean_dec(x_12);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_19 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_19, 0, x_18);
lean_ctor_set(x_19, 1, x_17);
return x_19;
}
}
else
{
uint8_t x_20; 
x_20 = !lean_is_exclusive(x_12);
if (x_20 == 0)
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; lean_object* x_26; lean_object* x_27; uint32_t x_28; uint8_t x_29; lean_object* x_30; lean_object* x_31; 
x_21 = lean_ctor_get(x_12, 0);
lean_dec(x_21);
x_22 = lean_ctor_get(x_13, 0);
lean_inc(x_22);
lean_dec(x_13);
x_23 = lean_ctor_get(x_22, 0);
lean_inc(x_23);
x_24 = lean_ctor_get(x_22, 1);
lean_inc(x_24);
lean_dec(x_22);
x_25 = l_Std_BitVec_toInt(x_23, x_24);
lean_dec(x_23);
x_26 = l_Int_toExpr(x_25);
lean_dec(x_25);
x_27 = lean_box(0);
x_28 = 0;
x_29 = 1;
x_30 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_30, 0, x_26);
lean_ctor_set(x_30, 1, x_27);
lean_ctor_set_uint32(x_30, sizeof(void*)*2, x_28);
lean_ctor_set_uint8(x_30, sizeof(void*)*2 + 4, x_29);
x_31 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_12, 0, x_31);
return x_12;
}
else
{
lean_object* x_32; lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; uint32_t x_39; uint8_t x_40; lean_object* x_41; lean_object* x_42; lean_object* x_43; 
x_32 = lean_ctor_get(x_12, 1);
lean_inc(x_32);
lean_dec(x_12);
x_33 = lean_ctor_get(x_13, 0);
lean_inc(x_33);
lean_dec(x_13);
x_34 = lean_ctor_get(x_33, 0);
lean_inc(x_34);
x_35 = lean_ctor_get(x_33, 1);
lean_inc(x_35);
lean_dec(x_33);
x_36 = l_Std_BitVec_toInt(x_34, x_35);
lean_dec(x_34);
x_37 = l_Int_toExpr(x_36);
lean_dec(x_36);
x_38 = lean_box(0);
x_39 = 0;
x_40 = 1;
x_41 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_41, 0, x_37);
lean_ctor_set(x_41, 1, x_38);
lean_ctor_set_uint32(x_41, sizeof(void*)*2, x_39);
lean_ctor_set_uint8(x_41, sizeof(void*)*2 + 4, x_40);
x_42 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_42, 0, x_41);
x_43 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_43, 0, x_42);
lean_ctor_set(x_43, 1, x_32);
return x_43;
}
}
}
else
{
uint8_t x_44; 
x_44 = !lean_is_exclusive(x_12);
if (x_44 == 0)
{
return x_12;
}
else
{
lean_object* x_45; lean_object* x_46; lean_object* x_47; 
x_45 = lean_ctor_get(x_12, 0);
x_46 = lean_ctor_get(x_12, 1);
lean_inc(x_46);
lean_inc(x_45);
lean_dec(x_12);
x_47 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_47, 0, x_45);
lean_ctor_set(x_47, 1, x_46);
return x_47;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceToInt___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("toInt", 5);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceToInt___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceToInt___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToInt(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceToInt___closed__2;
x_11 = lean_unsigned_to_nat(2u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceToInt___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToInt___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceToInt___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
return x_11;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceToInt___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; 
x_10 = l_Std_BitVec_reduceToInt(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_1);
return x_10;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceToInt", 11);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceToInt___closed__2;
x_2 = lean_unsigned_to_nat(2u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__7() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceToInt___boxed), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__6;
x_4 = l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__7;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2723_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__7;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2725_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__7;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceOfInt___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_13 = l_Nat_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_24 = l_Int_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; uint32_t x_39; uint8_t x_40; lean_object* x_41; lean_object* x_42; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = l_Std_BitVec_ofInt(x_22, x_34);
lean_dec(x_34);
x_36 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_36, 0, x_22);
lean_ctor_set(x_36, 1, x_35);
x_37 = l_Std_BitVec_Literal_toExpr(x_36);
x_38 = lean_box(0);
x_39 = 0;
x_40 = 1;
x_41 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_41, 0, x_37);
lean_ctor_set(x_41, 1, x_38);
lean_ctor_set_uint32(x_41, sizeof(void*)*2, x_39);
lean_ctor_set_uint8(x_41, sizeof(void*)*2 + 4, x_40);
x_42 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_42, 0, x_41);
lean_ctor_set(x_24, 0, x_42);
return x_24;
}
else
{
lean_object* x_43; lean_object* x_44; lean_object* x_45; lean_object* x_46; lean_object* x_47; lean_object* x_48; uint32_t x_49; uint8_t x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; 
x_43 = lean_ctor_get(x_24, 1);
lean_inc(x_43);
lean_dec(x_24);
x_44 = lean_ctor_get(x_25, 0);
lean_inc(x_44);
lean_dec(x_25);
x_45 = l_Std_BitVec_ofInt(x_22, x_44);
lean_dec(x_44);
x_46 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_46, 0, x_22);
lean_ctor_set(x_46, 1, x_45);
x_47 = l_Std_BitVec_Literal_toExpr(x_46);
x_48 = lean_box(0);
x_49 = 0;
x_50 = 1;
x_51 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_51, 0, x_47);
lean_ctor_set(x_51, 1, x_48);
lean_ctor_set_uint32(x_51, sizeof(void*)*2, x_49);
lean_ctor_set_uint8(x_51, sizeof(void*)*2 + 4, x_50);
x_52 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_52, 0, x_51);
x_53 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_53, 0, x_52);
lean_ctor_set(x_53, 1, x_43);
return x_53;
}
}
}
else
{
uint8_t x_54; 
lean_dec(x_22);
x_54 = !lean_is_exclusive(x_24);
if (x_54 == 0)
{
return x_24;
}
else
{
lean_object* x_55; lean_object* x_56; lean_object* x_57; 
x_55 = lean_ctor_get(x_24, 0);
x_56 = lean_ctor_get(x_24, 1);
lean_inc(x_56);
lean_inc(x_55);
lean_dec(x_24);
x_57 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_57, 0, x_55);
lean_ctor_set(x_57, 1, x_56);
return x_57;
}
}
}
}
else
{
uint8_t x_58; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_1);
x_58 = !lean_is_exclusive(x_13);
if (x_58 == 0)
{
return x_13;
}
else
{
lean_object* x_59; lean_object* x_60; lean_object* x_61; 
x_59 = lean_ctor_get(x_13, 0);
x_60 = lean_ctor_get(x_13, 1);
lean_inc(x_60);
lean_inc(x_59);
lean_dec(x_13);
x_61 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_61, 0, x_59);
lean_ctor_set(x_61, 1, x_60);
return x_61;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceOfInt___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("ofInt", 5);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceOfInt___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceOfInt___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceOfInt(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceOfInt___closed__2;
x_11 = lean_unsigned_to_nat(2u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceOfInt___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceOfInt___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceOfInt___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceOfInt", 11);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceOfInt___closed__2;
x_2 = lean_unsigned_to_nat(2u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__7() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceOfInt), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__6;
x_4 = l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__7;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2880_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__7;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2882_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__7;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceLT___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_24 = l_Std_BitVec_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; uint8_t x_38; 
x_33 = lean_ctor_get(x_24, 1);
x_34 = lean_ctor_get(x_24, 0);
lean_dec(x_34);
x_35 = lean_ctor_get(x_25, 0);
lean_inc(x_35);
lean_dec(x_25);
x_36 = lean_ctor_get(x_22, 0);
lean_inc(x_36);
x_37 = lean_ctor_get(x_35, 0);
lean_inc(x_37);
x_38 = lean_nat_dec_eq(x_36, x_37);
lean_dec(x_37);
lean_dec(x_36);
if (x_38 == 0)
{
lean_object* x_39; 
lean_dec(x_35);
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_39 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_39);
return x_24;
}
else
{
lean_object* x_40; lean_object* x_41; uint8_t x_42; lean_object* x_43; 
lean_free_object(x_24);
x_40 = lean_ctor_get(x_22, 1);
lean_inc(x_40);
lean_dec(x_22);
x_41 = lean_ctor_get(x_35, 1);
lean_inc(x_41);
lean_dec(x_35);
x_42 = lean_nat_dec_lt(x_40, x_41);
lean_dec(x_41);
lean_dec(x_40);
x_43 = l_Lean_Meta_Simp_evalPropStep(x_1, x_42, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_33);
return x_43;
}
}
else
{
lean_object* x_44; lean_object* x_45; lean_object* x_46; lean_object* x_47; uint8_t x_48; 
x_44 = lean_ctor_get(x_24, 1);
lean_inc(x_44);
lean_dec(x_24);
x_45 = lean_ctor_get(x_25, 0);
lean_inc(x_45);
lean_dec(x_25);
x_46 = lean_ctor_get(x_22, 0);
lean_inc(x_46);
x_47 = lean_ctor_get(x_45, 0);
lean_inc(x_47);
x_48 = lean_nat_dec_eq(x_46, x_47);
lean_dec(x_47);
lean_dec(x_46);
if (x_48 == 0)
{
lean_object* x_49; lean_object* x_50; 
lean_dec(x_45);
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_49 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_50 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_50, 0, x_49);
lean_ctor_set(x_50, 1, x_44);
return x_50;
}
else
{
lean_object* x_51; lean_object* x_52; uint8_t x_53; lean_object* x_54; 
x_51 = lean_ctor_get(x_22, 1);
lean_inc(x_51);
lean_dec(x_22);
x_52 = lean_ctor_get(x_45, 1);
lean_inc(x_52);
lean_dec(x_45);
x_53 = lean_nat_dec_lt(x_51, x_52);
lean_dec(x_52);
lean_dec(x_51);
x_54 = l_Lean_Meta_Simp_evalPropStep(x_1, x_53, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_44);
return x_54;
}
}
}
}
else
{
uint8_t x_55; 
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_55 = !lean_is_exclusive(x_24);
if (x_55 == 0)
{
return x_24;
}
else
{
lean_object* x_56; lean_object* x_57; lean_object* x_58; 
x_56 = lean_ctor_get(x_24, 0);
x_57 = lean_ctor_get(x_24, 1);
lean_inc(x_57);
lean_inc(x_56);
lean_dec(x_24);
x_58 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_58, 0, x_56);
lean_ctor_set(x_58, 1, x_57);
return x_58;
}
}
}
}
else
{
uint8_t x_59; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_59 = !lean_is_exclusive(x_13);
if (x_59 == 0)
{
return x_13;
}
else
{
lean_object* x_60; lean_object* x_61; lean_object* x_62; 
x_60 = lean_ctor_get(x_13, 0);
x_61 = lean_ctor_get(x_13, 1);
lean_inc(x_61);
lean_inc(x_60);
lean_dec(x_13);
x_62 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_62, 0, x_60);
lean_ctor_set(x_62, 1, x_61);
return x_62;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceLT___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("LT", 2);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceLT___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("lt", 2);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceLT___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceLT___closed__1;
x_2 = l_Std_BitVec_reduceLT___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceLT(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceLT___closed__3;
x_11 = lean_unsigned_to_nat(4u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceLT___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceLT___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceLT___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceLT", 8);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceLT___closed__3;
x_2 = lean_unsigned_to_nat(4u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = lean_unsigned_to_nat(6u);
x_2 = lean_mk_empty_array_with_capacity(x_1);
return x_2;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__5;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__8;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__10() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__9;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__11() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceLT), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__10;
x_4 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__11;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2923_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__11;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2925_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__11;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceLE___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_24 = l_Std_BitVec_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; uint8_t x_38; 
x_33 = lean_ctor_get(x_24, 1);
x_34 = lean_ctor_get(x_24, 0);
lean_dec(x_34);
x_35 = lean_ctor_get(x_25, 0);
lean_inc(x_35);
lean_dec(x_25);
x_36 = lean_ctor_get(x_22, 0);
lean_inc(x_36);
x_37 = lean_ctor_get(x_35, 0);
lean_inc(x_37);
x_38 = lean_nat_dec_eq(x_36, x_37);
lean_dec(x_37);
lean_dec(x_36);
if (x_38 == 0)
{
lean_object* x_39; 
lean_dec(x_35);
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_39 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_39);
return x_24;
}
else
{
lean_object* x_40; lean_object* x_41; uint8_t x_42; lean_object* x_43; 
lean_free_object(x_24);
x_40 = lean_ctor_get(x_22, 1);
lean_inc(x_40);
lean_dec(x_22);
x_41 = lean_ctor_get(x_35, 1);
lean_inc(x_41);
lean_dec(x_35);
x_42 = lean_nat_dec_le(x_40, x_41);
lean_dec(x_41);
lean_dec(x_40);
x_43 = l_Lean_Meta_Simp_evalPropStep(x_1, x_42, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_33);
return x_43;
}
}
else
{
lean_object* x_44; lean_object* x_45; lean_object* x_46; lean_object* x_47; uint8_t x_48; 
x_44 = lean_ctor_get(x_24, 1);
lean_inc(x_44);
lean_dec(x_24);
x_45 = lean_ctor_get(x_25, 0);
lean_inc(x_45);
lean_dec(x_25);
x_46 = lean_ctor_get(x_22, 0);
lean_inc(x_46);
x_47 = lean_ctor_get(x_45, 0);
lean_inc(x_47);
x_48 = lean_nat_dec_eq(x_46, x_47);
lean_dec(x_47);
lean_dec(x_46);
if (x_48 == 0)
{
lean_object* x_49; lean_object* x_50; 
lean_dec(x_45);
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_49 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_50 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_50, 0, x_49);
lean_ctor_set(x_50, 1, x_44);
return x_50;
}
else
{
lean_object* x_51; lean_object* x_52; uint8_t x_53; lean_object* x_54; 
x_51 = lean_ctor_get(x_22, 1);
lean_inc(x_51);
lean_dec(x_22);
x_52 = lean_ctor_get(x_45, 1);
lean_inc(x_52);
lean_dec(x_45);
x_53 = lean_nat_dec_le(x_51, x_52);
lean_dec(x_52);
lean_dec(x_51);
x_54 = l_Lean_Meta_Simp_evalPropStep(x_1, x_53, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_44);
return x_54;
}
}
}
}
else
{
uint8_t x_55; 
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_55 = !lean_is_exclusive(x_24);
if (x_55 == 0)
{
return x_24;
}
else
{
lean_object* x_56; lean_object* x_57; lean_object* x_58; 
x_56 = lean_ctor_get(x_24, 0);
x_57 = lean_ctor_get(x_24, 1);
lean_inc(x_57);
lean_inc(x_56);
lean_dec(x_24);
x_58 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_58, 0, x_56);
lean_ctor_set(x_58, 1, x_57);
return x_58;
}
}
}
}
else
{
uint8_t x_59; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_59 = !lean_is_exclusive(x_13);
if (x_59 == 0)
{
return x_13;
}
else
{
lean_object* x_60; lean_object* x_61; lean_object* x_62; 
x_60 = lean_ctor_get(x_13, 0);
x_61 = lean_ctor_get(x_13, 1);
lean_inc(x_61);
lean_inc(x_60);
lean_dec(x_13);
x_62 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_62, 0, x_60);
lean_ctor_set(x_62, 1, x_61);
return x_62;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceLE___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("LE", 2);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceLE___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("le", 2);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceLE___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceLE___closed__1;
x_2 = l_Std_BitVec_reduceLE___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceLE(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceLE___closed__3;
x_11 = lean_unsigned_to_nat(4u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceLE___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceLE___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceLE___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceLE", 8);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceLE___closed__3;
x_2 = lean_unsigned_to_nat(4u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__9() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__8;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__10() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceLE), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__9;
x_4 = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__10;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2966_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__10;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2968_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__10;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGT___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_24 = l_Std_BitVec_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; uint8_t x_38; 
x_33 = lean_ctor_get(x_24, 1);
x_34 = lean_ctor_get(x_24, 0);
lean_dec(x_34);
x_35 = lean_ctor_get(x_25, 0);
lean_inc(x_35);
lean_dec(x_25);
x_36 = lean_ctor_get(x_22, 0);
lean_inc(x_36);
x_37 = lean_ctor_get(x_35, 0);
lean_inc(x_37);
x_38 = lean_nat_dec_eq(x_36, x_37);
lean_dec(x_37);
lean_dec(x_36);
if (x_38 == 0)
{
lean_object* x_39; 
lean_dec(x_35);
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_39 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_39);
return x_24;
}
else
{
lean_object* x_40; lean_object* x_41; uint8_t x_42; lean_object* x_43; 
lean_free_object(x_24);
x_40 = lean_ctor_get(x_22, 1);
lean_inc(x_40);
lean_dec(x_22);
x_41 = lean_ctor_get(x_35, 1);
lean_inc(x_41);
lean_dec(x_35);
x_42 = lean_nat_dec_lt(x_41, x_40);
lean_dec(x_40);
lean_dec(x_41);
x_43 = l_Lean_Meta_Simp_evalPropStep(x_1, x_42, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_33);
return x_43;
}
}
else
{
lean_object* x_44; lean_object* x_45; lean_object* x_46; lean_object* x_47; uint8_t x_48; 
x_44 = lean_ctor_get(x_24, 1);
lean_inc(x_44);
lean_dec(x_24);
x_45 = lean_ctor_get(x_25, 0);
lean_inc(x_45);
lean_dec(x_25);
x_46 = lean_ctor_get(x_22, 0);
lean_inc(x_46);
x_47 = lean_ctor_get(x_45, 0);
lean_inc(x_47);
x_48 = lean_nat_dec_eq(x_46, x_47);
lean_dec(x_47);
lean_dec(x_46);
if (x_48 == 0)
{
lean_object* x_49; lean_object* x_50; 
lean_dec(x_45);
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_49 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_50 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_50, 0, x_49);
lean_ctor_set(x_50, 1, x_44);
return x_50;
}
else
{
lean_object* x_51; lean_object* x_52; uint8_t x_53; lean_object* x_54; 
x_51 = lean_ctor_get(x_22, 1);
lean_inc(x_51);
lean_dec(x_22);
x_52 = lean_ctor_get(x_45, 1);
lean_inc(x_52);
lean_dec(x_45);
x_53 = lean_nat_dec_lt(x_52, x_51);
lean_dec(x_51);
lean_dec(x_52);
x_54 = l_Lean_Meta_Simp_evalPropStep(x_1, x_53, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_44);
return x_54;
}
}
}
}
else
{
uint8_t x_55; 
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_55 = !lean_is_exclusive(x_24);
if (x_55 == 0)
{
return x_24;
}
else
{
lean_object* x_56; lean_object* x_57; lean_object* x_58; 
x_56 = lean_ctor_get(x_24, 0);
x_57 = lean_ctor_get(x_24, 1);
lean_inc(x_57);
lean_inc(x_56);
lean_dec(x_24);
x_58 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_58, 0, x_56);
lean_ctor_set(x_58, 1, x_57);
return x_58;
}
}
}
}
else
{
uint8_t x_59; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_59 = !lean_is_exclusive(x_13);
if (x_59 == 0)
{
return x_13;
}
else
{
lean_object* x_60; lean_object* x_61; lean_object* x_62; 
x_60 = lean_ctor_get(x_13, 0);
x_61 = lean_ctor_get(x_13, 1);
lean_inc(x_61);
lean_inc(x_60);
lean_dec(x_13);
x_62 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_62, 0, x_60);
lean_ctor_set(x_62, 1, x_61);
return x_62;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceGT___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("GT", 2);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceGT___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("gt", 2);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceGT___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceGT___closed__1;
x_2 = l_Std_BitVec_reduceGT___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGT(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceGT___closed__3;
x_11 = lean_unsigned_to_nat(4u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceGT___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGT___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceGT___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceGT", 8);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__3() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceGT), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__10;
x_4 = l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__3;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3009_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__3;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3011_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__3;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGE___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_24 = l_Std_BitVec_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; uint8_t x_38; 
x_33 = lean_ctor_get(x_24, 1);
x_34 = lean_ctor_get(x_24, 0);
lean_dec(x_34);
x_35 = lean_ctor_get(x_25, 0);
lean_inc(x_35);
lean_dec(x_25);
x_36 = lean_ctor_get(x_22, 0);
lean_inc(x_36);
x_37 = lean_ctor_get(x_35, 0);
lean_inc(x_37);
x_38 = lean_nat_dec_eq(x_36, x_37);
lean_dec(x_37);
lean_dec(x_36);
if (x_38 == 0)
{
lean_object* x_39; 
lean_dec(x_35);
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_39 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_39);
return x_24;
}
else
{
lean_object* x_40; lean_object* x_41; uint8_t x_42; lean_object* x_43; 
lean_free_object(x_24);
x_40 = lean_ctor_get(x_22, 1);
lean_inc(x_40);
lean_dec(x_22);
x_41 = lean_ctor_get(x_35, 1);
lean_inc(x_41);
lean_dec(x_35);
x_42 = lean_nat_dec_le(x_41, x_40);
lean_dec(x_40);
lean_dec(x_41);
x_43 = l_Lean_Meta_Simp_evalPropStep(x_1, x_42, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_33);
return x_43;
}
}
else
{
lean_object* x_44; lean_object* x_45; lean_object* x_46; lean_object* x_47; uint8_t x_48; 
x_44 = lean_ctor_get(x_24, 1);
lean_inc(x_44);
lean_dec(x_24);
x_45 = lean_ctor_get(x_25, 0);
lean_inc(x_45);
lean_dec(x_25);
x_46 = lean_ctor_get(x_22, 0);
lean_inc(x_46);
x_47 = lean_ctor_get(x_45, 0);
lean_inc(x_47);
x_48 = lean_nat_dec_eq(x_46, x_47);
lean_dec(x_47);
lean_dec(x_46);
if (x_48 == 0)
{
lean_object* x_49; lean_object* x_50; 
lean_dec(x_45);
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_49 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_50 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_50, 0, x_49);
lean_ctor_set(x_50, 1, x_44);
return x_50;
}
else
{
lean_object* x_51; lean_object* x_52; uint8_t x_53; lean_object* x_54; 
x_51 = lean_ctor_get(x_22, 1);
lean_inc(x_51);
lean_dec(x_22);
x_52 = lean_ctor_get(x_45, 1);
lean_inc(x_52);
lean_dec(x_45);
x_53 = lean_nat_dec_le(x_52, x_51);
lean_dec(x_51);
lean_dec(x_52);
x_54 = l_Lean_Meta_Simp_evalPropStep(x_1, x_53, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_44);
return x_54;
}
}
}
}
else
{
uint8_t x_55; 
lean_dec(x_22);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_55 = !lean_is_exclusive(x_24);
if (x_55 == 0)
{
return x_24;
}
else
{
lean_object* x_56; lean_object* x_57; lean_object* x_58; 
x_56 = lean_ctor_get(x_24, 0);
x_57 = lean_ctor_get(x_24, 1);
lean_inc(x_57);
lean_inc(x_56);
lean_dec(x_24);
x_58 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_58, 0, x_56);
lean_ctor_set(x_58, 1, x_57);
return x_58;
}
}
}
}
else
{
uint8_t x_59; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_59 = !lean_is_exclusive(x_13);
if (x_59 == 0)
{
return x_13;
}
else
{
lean_object* x_60; lean_object* x_61; lean_object* x_62; 
x_60 = lean_ctor_get(x_13, 0);
x_61 = lean_ctor_get(x_13, 1);
lean_inc(x_61);
lean_inc(x_60);
lean_dec(x_13);
x_62 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_62, 0, x_60);
lean_ctor_set(x_62, 1, x_61);
return x_62;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceGE___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("GE", 2);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceGE___closed__2() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("ge", 2);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceGE___closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceGE___closed__1;
x_2 = l_Std_BitVec_reduceGE___closed__2;
x_3 = l_Lean_Name_mkStr2(x_1, x_2);
return x_3;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGE(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceGE___closed__3;
x_11 = lean_unsigned_to_nat(4u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceGE___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceGE___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceGE___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceGE", 8);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__3() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceGE), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__9;
x_4 = l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__3;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3052_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__3;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3054_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__3;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceULT___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_24 = l_Std_BitVec_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; uint8_t x_37; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = lean_ctor_get(x_22, 0);
lean_inc(x_35);
x_36 = lean_ctor_get(x_34, 0);
lean_inc(x_36);
x_37 = lean_nat_dec_eq(x_35, x_36);
lean_dec(x_36);
lean_dec(x_35);
if (x_37 == 0)
{
lean_object* x_38; 
lean_dec(x_34);
lean_dec(x_22);
x_38 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_38);
return x_24;
}
else
{
lean_object* x_39; lean_object* x_40; uint8_t x_41; 
x_39 = lean_ctor_get(x_22, 1);
lean_inc(x_39);
lean_dec(x_22);
x_40 = lean_ctor_get(x_34, 1);
lean_inc(x_40);
lean_dec(x_34);
x_41 = lean_nat_dec_lt(x_39, x_40);
lean_dec(x_40);
lean_dec(x_39);
if (x_41 == 0)
{
lean_object* x_42; 
x_42 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
lean_ctor_set(x_24, 0, x_42);
return x_24;
}
else
{
lean_object* x_43; 
x_43 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
lean_ctor_set(x_24, 0, x_43);
return x_24;
}
}
}
else
{
lean_object* x_44; lean_object* x_45; lean_object* x_46; lean_object* x_47; uint8_t x_48; 
x_44 = lean_ctor_get(x_24, 1);
lean_inc(x_44);
lean_dec(x_24);
x_45 = lean_ctor_get(x_25, 0);
lean_inc(x_45);
lean_dec(x_25);
x_46 = lean_ctor_get(x_22, 0);
lean_inc(x_46);
x_47 = lean_ctor_get(x_45, 0);
lean_inc(x_47);
x_48 = lean_nat_dec_eq(x_46, x_47);
lean_dec(x_47);
lean_dec(x_46);
if (x_48 == 0)
{
lean_object* x_49; lean_object* x_50; 
lean_dec(x_45);
lean_dec(x_22);
x_49 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_50 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_50, 0, x_49);
lean_ctor_set(x_50, 1, x_44);
return x_50;
}
else
{
lean_object* x_51; lean_object* x_52; uint8_t x_53; 
x_51 = lean_ctor_get(x_22, 1);
lean_inc(x_51);
lean_dec(x_22);
x_52 = lean_ctor_get(x_45, 1);
lean_inc(x_52);
lean_dec(x_45);
x_53 = lean_nat_dec_lt(x_51, x_52);
lean_dec(x_52);
lean_dec(x_51);
if (x_53 == 0)
{
lean_object* x_54; lean_object* x_55; 
x_54 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
x_55 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_55, 0, x_54);
lean_ctor_set(x_55, 1, x_44);
return x_55;
}
else
{
lean_object* x_56; lean_object* x_57; 
x_56 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
x_57 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_57, 0, x_56);
lean_ctor_set(x_57, 1, x_44);
return x_57;
}
}
}
}
}
else
{
uint8_t x_58; 
lean_dec(x_22);
x_58 = !lean_is_exclusive(x_24);
if (x_58 == 0)
{
return x_24;
}
else
{
lean_object* x_59; lean_object* x_60; lean_object* x_61; 
x_59 = lean_ctor_get(x_24, 0);
x_60 = lean_ctor_get(x_24, 1);
lean_inc(x_60);
lean_inc(x_59);
lean_dec(x_24);
x_61 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_61, 0, x_59);
lean_ctor_set(x_61, 1, x_60);
return x_61;
}
}
}
}
else
{
uint8_t x_62; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_62 = !lean_is_exclusive(x_13);
if (x_62 == 0)
{
return x_13;
}
else
{
lean_object* x_63; lean_object* x_64; lean_object* x_65; 
x_63 = lean_ctor_get(x_13, 0);
x_64 = lean_ctor_get(x_13, 1);
lean_inc(x_64);
lean_inc(x_63);
lean_dec(x_13);
x_65 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_65, 0, x_63);
lean_ctor_set(x_65, 1, x_64);
return x_65;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceULT___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("ult", 3);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceULT___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceULT___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceULT(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceULT___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceULT___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceULT___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceULT___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceULT", 9);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceULT___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceULT), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3075_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3077_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceULE___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_24 = l_Std_BitVec_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; uint8_t x_37; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = lean_ctor_get(x_22, 0);
lean_inc(x_35);
x_36 = lean_ctor_get(x_34, 0);
lean_inc(x_36);
x_37 = lean_nat_dec_eq(x_35, x_36);
lean_dec(x_36);
lean_dec(x_35);
if (x_37 == 0)
{
lean_object* x_38; 
lean_dec(x_34);
lean_dec(x_22);
x_38 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_38);
return x_24;
}
else
{
lean_object* x_39; lean_object* x_40; uint8_t x_41; 
x_39 = lean_ctor_get(x_22, 1);
lean_inc(x_39);
lean_dec(x_22);
x_40 = lean_ctor_get(x_34, 1);
lean_inc(x_40);
lean_dec(x_34);
x_41 = lean_nat_dec_le(x_39, x_40);
lean_dec(x_40);
lean_dec(x_39);
if (x_41 == 0)
{
lean_object* x_42; 
x_42 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
lean_ctor_set(x_24, 0, x_42);
return x_24;
}
else
{
lean_object* x_43; 
x_43 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
lean_ctor_set(x_24, 0, x_43);
return x_24;
}
}
}
else
{
lean_object* x_44; lean_object* x_45; lean_object* x_46; lean_object* x_47; uint8_t x_48; 
x_44 = lean_ctor_get(x_24, 1);
lean_inc(x_44);
lean_dec(x_24);
x_45 = lean_ctor_get(x_25, 0);
lean_inc(x_45);
lean_dec(x_25);
x_46 = lean_ctor_get(x_22, 0);
lean_inc(x_46);
x_47 = lean_ctor_get(x_45, 0);
lean_inc(x_47);
x_48 = lean_nat_dec_eq(x_46, x_47);
lean_dec(x_47);
lean_dec(x_46);
if (x_48 == 0)
{
lean_object* x_49; lean_object* x_50; 
lean_dec(x_45);
lean_dec(x_22);
x_49 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_50 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_50, 0, x_49);
lean_ctor_set(x_50, 1, x_44);
return x_50;
}
else
{
lean_object* x_51; lean_object* x_52; uint8_t x_53; 
x_51 = lean_ctor_get(x_22, 1);
lean_inc(x_51);
lean_dec(x_22);
x_52 = lean_ctor_get(x_45, 1);
lean_inc(x_52);
lean_dec(x_45);
x_53 = lean_nat_dec_le(x_51, x_52);
lean_dec(x_52);
lean_dec(x_51);
if (x_53 == 0)
{
lean_object* x_54; lean_object* x_55; 
x_54 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
x_55 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_55, 0, x_54);
lean_ctor_set(x_55, 1, x_44);
return x_55;
}
else
{
lean_object* x_56; lean_object* x_57; 
x_56 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
x_57 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_57, 0, x_56);
lean_ctor_set(x_57, 1, x_44);
return x_57;
}
}
}
}
}
else
{
uint8_t x_58; 
lean_dec(x_22);
x_58 = !lean_is_exclusive(x_24);
if (x_58 == 0)
{
return x_24;
}
else
{
lean_object* x_59; lean_object* x_60; lean_object* x_61; 
x_59 = lean_ctor_get(x_24, 0);
x_60 = lean_ctor_get(x_24, 1);
lean_inc(x_60);
lean_inc(x_59);
lean_dec(x_24);
x_61 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_61, 0, x_59);
lean_ctor_set(x_61, 1, x_60);
return x_61;
}
}
}
}
else
{
uint8_t x_62; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_62 = !lean_is_exclusive(x_13);
if (x_62 == 0)
{
return x_13;
}
else
{
lean_object* x_63; lean_object* x_64; lean_object* x_65; 
x_63 = lean_ctor_get(x_13, 0);
x_64 = lean_ctor_get(x_13, 1);
lean_inc(x_64);
lean_inc(x_63);
lean_dec(x_13);
x_65 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_65, 0, x_63);
lean_ctor_set(x_65, 1, x_64);
return x_65;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceULE___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("ule", 3);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceULE___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceULE___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceULE(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceULE___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceULE___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceULE___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceULE___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceULE", 9);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceULE___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceULE), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3098_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3100_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSLT___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_24 = l_Std_BitVec_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; uint8_t x_37; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = lean_ctor_get(x_22, 0);
lean_inc(x_35);
x_36 = lean_ctor_get(x_34, 0);
lean_inc(x_36);
x_37 = lean_nat_dec_eq(x_35, x_36);
lean_dec(x_36);
if (x_37 == 0)
{
lean_object* x_38; 
lean_dec(x_35);
lean_dec(x_34);
lean_dec(x_22);
x_38 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_38);
return x_24;
}
else
{
lean_object* x_39; lean_object* x_40; uint8_t x_41; 
x_39 = lean_ctor_get(x_22, 1);
lean_inc(x_39);
lean_dec(x_22);
x_40 = lean_ctor_get(x_34, 1);
lean_inc(x_40);
lean_dec(x_34);
x_41 = l_Std_BitVec_slt(x_35, x_39, x_40);
lean_dec(x_35);
if (x_41 == 0)
{
lean_object* x_42; 
x_42 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
lean_ctor_set(x_24, 0, x_42);
return x_24;
}
else
{
lean_object* x_43; 
x_43 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
lean_ctor_set(x_24, 0, x_43);
return x_24;
}
}
}
else
{
lean_object* x_44; lean_object* x_45; lean_object* x_46; lean_object* x_47; uint8_t x_48; 
x_44 = lean_ctor_get(x_24, 1);
lean_inc(x_44);
lean_dec(x_24);
x_45 = lean_ctor_get(x_25, 0);
lean_inc(x_45);
lean_dec(x_25);
x_46 = lean_ctor_get(x_22, 0);
lean_inc(x_46);
x_47 = lean_ctor_get(x_45, 0);
lean_inc(x_47);
x_48 = lean_nat_dec_eq(x_46, x_47);
lean_dec(x_47);
if (x_48 == 0)
{
lean_object* x_49; lean_object* x_50; 
lean_dec(x_46);
lean_dec(x_45);
lean_dec(x_22);
x_49 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_50 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_50, 0, x_49);
lean_ctor_set(x_50, 1, x_44);
return x_50;
}
else
{
lean_object* x_51; lean_object* x_52; uint8_t x_53; 
x_51 = lean_ctor_get(x_22, 1);
lean_inc(x_51);
lean_dec(x_22);
x_52 = lean_ctor_get(x_45, 1);
lean_inc(x_52);
lean_dec(x_45);
x_53 = l_Std_BitVec_slt(x_46, x_51, x_52);
lean_dec(x_46);
if (x_53 == 0)
{
lean_object* x_54; lean_object* x_55; 
x_54 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
x_55 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_55, 0, x_54);
lean_ctor_set(x_55, 1, x_44);
return x_55;
}
else
{
lean_object* x_56; lean_object* x_57; 
x_56 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
x_57 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_57, 0, x_56);
lean_ctor_set(x_57, 1, x_44);
return x_57;
}
}
}
}
}
else
{
uint8_t x_58; 
lean_dec(x_22);
x_58 = !lean_is_exclusive(x_24);
if (x_58 == 0)
{
return x_24;
}
else
{
lean_object* x_59; lean_object* x_60; lean_object* x_61; 
x_59 = lean_ctor_get(x_24, 0);
x_60 = lean_ctor_get(x_24, 1);
lean_inc(x_60);
lean_inc(x_59);
lean_dec(x_24);
x_61 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_61, 0, x_59);
lean_ctor_set(x_61, 1, x_60);
return x_61;
}
}
}
}
else
{
uint8_t x_62; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_62 = !lean_is_exclusive(x_13);
if (x_62 == 0)
{
return x_13;
}
else
{
lean_object* x_63; lean_object* x_64; lean_object* x_65; 
x_63 = lean_ctor_get(x_13, 0);
x_64 = lean_ctor_get(x_13, 1);
lean_inc(x_64);
lean_inc(x_63);
lean_dec(x_13);
x_65 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_65, 0, x_63);
lean_ctor_set(x_65, 1, x_64);
return x_65;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceSLT___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("slt", 3);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceSLT___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceSLT___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSLT(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceSLT___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceSLT___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSLT___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceSLT___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceSLT", 9);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceSLT___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceSLT), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3121_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3123_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSLE___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_24 = l_Std_BitVec_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; uint8_t x_37; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = lean_ctor_get(x_22, 0);
lean_inc(x_35);
x_36 = lean_ctor_get(x_34, 0);
lean_inc(x_36);
x_37 = lean_nat_dec_eq(x_35, x_36);
lean_dec(x_36);
if (x_37 == 0)
{
lean_object* x_38; 
lean_dec(x_35);
lean_dec(x_34);
lean_dec(x_22);
x_38 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_38);
return x_24;
}
else
{
lean_object* x_39; lean_object* x_40; uint8_t x_41; 
x_39 = lean_ctor_get(x_22, 1);
lean_inc(x_39);
lean_dec(x_22);
x_40 = lean_ctor_get(x_34, 1);
lean_inc(x_40);
lean_dec(x_34);
x_41 = l_Std_BitVec_sle(x_35, x_39, x_40);
lean_dec(x_35);
if (x_41 == 0)
{
lean_object* x_42; 
x_42 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
lean_ctor_set(x_24, 0, x_42);
return x_24;
}
else
{
lean_object* x_43; 
x_43 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
lean_ctor_set(x_24, 0, x_43);
return x_24;
}
}
}
else
{
lean_object* x_44; lean_object* x_45; lean_object* x_46; lean_object* x_47; uint8_t x_48; 
x_44 = lean_ctor_get(x_24, 1);
lean_inc(x_44);
lean_dec(x_24);
x_45 = lean_ctor_get(x_25, 0);
lean_inc(x_45);
lean_dec(x_25);
x_46 = lean_ctor_get(x_22, 0);
lean_inc(x_46);
x_47 = lean_ctor_get(x_45, 0);
lean_inc(x_47);
x_48 = lean_nat_dec_eq(x_46, x_47);
lean_dec(x_47);
if (x_48 == 0)
{
lean_object* x_49; lean_object* x_50; 
lean_dec(x_46);
lean_dec(x_45);
lean_dec(x_22);
x_49 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_50 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_50, 0, x_49);
lean_ctor_set(x_50, 1, x_44);
return x_50;
}
else
{
lean_object* x_51; lean_object* x_52; uint8_t x_53; 
x_51 = lean_ctor_get(x_22, 1);
lean_inc(x_51);
lean_dec(x_22);
x_52 = lean_ctor_get(x_45, 1);
lean_inc(x_52);
lean_dec(x_45);
x_53 = l_Std_BitVec_sle(x_46, x_51, x_52);
lean_dec(x_46);
if (x_53 == 0)
{
lean_object* x_54; lean_object* x_55; 
x_54 = l_Std_BitVec_reduceGetBit___lambda__1___closed__6;
x_55 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_55, 0, x_54);
lean_ctor_set(x_55, 1, x_44);
return x_55;
}
else
{
lean_object* x_56; lean_object* x_57; 
x_56 = l_Std_BitVec_reduceGetBit___lambda__1___closed__11;
x_57 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_57, 0, x_56);
lean_ctor_set(x_57, 1, x_44);
return x_57;
}
}
}
}
}
else
{
uint8_t x_58; 
lean_dec(x_22);
x_58 = !lean_is_exclusive(x_24);
if (x_58 == 0)
{
return x_24;
}
else
{
lean_object* x_59; lean_object* x_60; lean_object* x_61; 
x_59 = lean_ctor_get(x_24, 0);
x_60 = lean_ctor_get(x_24, 1);
lean_inc(x_60);
lean_inc(x_59);
lean_dec(x_24);
x_61 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_61, 0, x_59);
lean_ctor_set(x_61, 1, x_60);
return x_61;
}
}
}
}
else
{
uint8_t x_62; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_62 = !lean_is_exclusive(x_13);
if (x_62 == 0)
{
return x_13;
}
else
{
lean_object* x_63; lean_object* x_64; lean_object* x_65; 
x_63 = lean_ctor_get(x_13, 0);
x_64 = lean_ctor_get(x_13, 1);
lean_inc(x_64);
lean_inc(x_63);
lean_dec(x_13);
x_65 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_65, 0, x_63);
lean_ctor_set(x_65, 1, x_64);
return x_65;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceSLE___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("sle", 3);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceSLE___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceSLE___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSLE(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceSLE___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceSLE___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSLE___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceSLE___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceSLE", 9);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceSLE___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceSLE), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3144_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3146_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceZeroExtend_x27___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; 
x_11 = l_Lean_Expr_appArg_x21(x_1);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_12 = l_Std_BitVec_fromExpr_x3f(x_11, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_12) == 0)
{
lean_object* x_13; 
x_13 = lean_ctor_get(x_12, 0);
lean_inc(x_13);
if (lean_obj_tag(x_13) == 0)
{
uint8_t x_14; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_14 = !lean_is_exclusive(x_12);
if (x_14 == 0)
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_ctor_get(x_12, 0);
lean_dec(x_15);
x_16 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_12, 0, x_16);
return x_12;
}
else
{
lean_object* x_17; lean_object* x_18; lean_object* x_19; 
x_17 = lean_ctor_get(x_12, 1);
lean_inc(x_17);
lean_dec(x_12);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_19 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_19, 0, x_18);
lean_ctor_set(x_19, 1, x_17);
return x_19;
}
}
else
{
lean_object* x_20; lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_20 = lean_ctor_get(x_12, 1);
lean_inc(x_20);
lean_dec(x_12);
x_21 = lean_ctor_get(x_13, 0);
lean_inc(x_21);
lean_dec(x_13);
x_22 = l_Lean_Expr_appFn_x21(x_1);
x_23 = l_Lean_Expr_appFn_x21(x_22);
x_24 = l_Lean_Expr_appArg_x21(x_23);
lean_dec(x_23);
x_25 = l_Nat_fromExpr_x3f(x_24, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_20);
lean_dec(x_6);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_21);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
uint8_t x_33; 
x_33 = !lean_is_exclusive(x_25);
if (x_33 == 0)
{
lean_object* x_34; lean_object* x_35; lean_object* x_36; uint8_t x_37; 
x_34 = lean_ctor_get(x_25, 0);
lean_dec(x_34);
x_35 = lean_ctor_get(x_26, 0);
lean_inc(x_35);
lean_dec(x_26);
x_36 = lean_ctor_get(x_21, 0);
lean_inc(x_36);
x_37 = lean_nat_dec_le(x_36, x_35);
lean_dec(x_36);
if (x_37 == 0)
{
lean_object* x_38; 
lean_dec(x_35);
lean_dec(x_21);
x_38 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_38);
return x_25;
}
else
{
lean_object* x_39; lean_object* x_40; lean_object* x_41; lean_object* x_42; uint32_t x_43; uint8_t x_44; lean_object* x_45; lean_object* x_46; 
x_39 = lean_ctor_get(x_21, 1);
lean_inc(x_39);
lean_dec(x_21);
x_40 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_40, 0, x_35);
lean_ctor_set(x_40, 1, x_39);
x_41 = l_Std_BitVec_Literal_toExpr(x_40);
x_42 = lean_box(0);
x_43 = 0;
x_44 = 1;
x_45 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_45, 0, x_41);
lean_ctor_set(x_45, 1, x_42);
lean_ctor_set_uint32(x_45, sizeof(void*)*2, x_43);
lean_ctor_set_uint8(x_45, sizeof(void*)*2 + 4, x_44);
x_46 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_46, 0, x_45);
lean_ctor_set(x_25, 0, x_46);
return x_25;
}
}
else
{
lean_object* x_47; lean_object* x_48; lean_object* x_49; uint8_t x_50; 
x_47 = lean_ctor_get(x_25, 1);
lean_inc(x_47);
lean_dec(x_25);
x_48 = lean_ctor_get(x_26, 0);
lean_inc(x_48);
lean_dec(x_26);
x_49 = lean_ctor_get(x_21, 0);
lean_inc(x_49);
x_50 = lean_nat_dec_le(x_49, x_48);
lean_dec(x_49);
if (x_50 == 0)
{
lean_object* x_51; lean_object* x_52; 
lean_dec(x_48);
lean_dec(x_21);
x_51 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_52 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_52, 0, x_51);
lean_ctor_set(x_52, 1, x_47);
return x_52;
}
else
{
lean_object* x_53; lean_object* x_54; lean_object* x_55; lean_object* x_56; uint32_t x_57; uint8_t x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; 
x_53 = lean_ctor_get(x_21, 1);
lean_inc(x_53);
lean_dec(x_21);
x_54 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_54, 0, x_48);
lean_ctor_set(x_54, 1, x_53);
x_55 = l_Std_BitVec_Literal_toExpr(x_54);
x_56 = lean_box(0);
x_57 = 0;
x_58 = 1;
x_59 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_59, 0, x_55);
lean_ctor_set(x_59, 1, x_56);
lean_ctor_set_uint32(x_59, sizeof(void*)*2, x_57);
lean_ctor_set_uint8(x_59, sizeof(void*)*2 + 4, x_58);
x_60 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_60, 0, x_59);
x_61 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_61, 0, x_60);
lean_ctor_set(x_61, 1, x_47);
return x_61;
}
}
}
}
else
{
uint8_t x_62; 
lean_dec(x_21);
x_62 = !lean_is_exclusive(x_25);
if (x_62 == 0)
{
return x_25;
}
else
{
lean_object* x_63; lean_object* x_64; lean_object* x_65; 
x_63 = lean_ctor_get(x_25, 0);
x_64 = lean_ctor_get(x_25, 1);
lean_inc(x_64);
lean_inc(x_63);
lean_dec(x_25);
x_65 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_65, 0, x_63);
lean_ctor_set(x_65, 1, x_64);
return x_65;
}
}
}
}
else
{
uint8_t x_66; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_66 = !lean_is_exclusive(x_12);
if (x_66 == 0)
{
return x_12;
}
else
{
lean_object* x_67; lean_object* x_68; lean_object* x_69; 
x_67 = lean_ctor_get(x_12, 0);
x_68 = lean_ctor_get(x_12, 1);
lean_inc(x_68);
lean_inc(x_67);
lean_dec(x_12);
x_69 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_69, 0, x_67);
lean_ctor_set(x_69, 1, x_68);
return x_69;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceZeroExtend_x27___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("zeroExtend'", 11);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceZeroExtend_x27___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceZeroExtend_x27___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceZeroExtend_x27(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceZeroExtend_x27___closed__2;
x_11 = lean_unsigned_to_nat(4u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceZeroExtend_x27___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceZeroExtend_x27___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceZeroExtend_x27___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceZeroExtend'", 17);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceZeroExtend_x27___closed__2;
x_2 = lean_unsigned_to_nat(4u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__5;
x_2 = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__9() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceZeroExtend_x27), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__8;
x_4 = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__9;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3323_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__9;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3325_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__9;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShiftLeftZeroExtend___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; lean_object* x_13; 
lean_inc(x_1);
x_11 = l_Lean_Expr_appFn_x21(x_1);
x_12 = l_Lean_Expr_appArg_x21(x_11);
lean_dec(x_11);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_13 = l_Std_BitVec_fromExpr_x3f(x_12, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_13) == 0)
{
lean_object* x_14; 
x_14 = lean_ctor_get(x_13, 0);
lean_inc(x_14);
if (lean_obj_tag(x_14) == 0)
{
uint8_t x_15; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_15 = !lean_is_exclusive(x_13);
if (x_15 == 0)
{
lean_object* x_16; lean_object* x_17; 
x_16 = lean_ctor_get(x_13, 0);
lean_dec(x_16);
x_17 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_13, 0, x_17);
return x_13;
}
else
{
lean_object* x_18; lean_object* x_19; lean_object* x_20; 
x_18 = lean_ctor_get(x_13, 1);
lean_inc(x_18);
lean_dec(x_13);
x_19 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_20 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_20, 0, x_19);
lean_ctor_set(x_20, 1, x_18);
return x_20;
}
}
else
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_21 = lean_ctor_get(x_13, 1);
lean_inc(x_21);
lean_dec(x_13);
x_22 = lean_ctor_get(x_14, 0);
lean_inc(x_22);
lean_dec(x_14);
x_23 = l_Lean_Expr_appArg_x21(x_1);
lean_dec(x_1);
x_24 = l_Nat_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_21);
lean_dec(x_6);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_22);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; lean_object* x_40; lean_object* x_41; uint32_t x_42; uint8_t x_43; lean_object* x_44; lean_object* x_45; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = lean_ctor_get(x_22, 0);
lean_inc(x_35);
x_36 = lean_nat_add(x_35, x_34);
lean_dec(x_35);
x_37 = lean_ctor_get(x_22, 1);
lean_inc(x_37);
lean_dec(x_22);
x_38 = lean_nat_shiftl(x_37, x_34);
lean_dec(x_34);
lean_dec(x_37);
x_39 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_39, 0, x_36);
lean_ctor_set(x_39, 1, x_38);
x_40 = l_Std_BitVec_Literal_toExpr(x_39);
x_41 = lean_box(0);
x_42 = 0;
x_43 = 1;
x_44 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_44, 0, x_40);
lean_ctor_set(x_44, 1, x_41);
lean_ctor_set_uint32(x_44, sizeof(void*)*2, x_42);
lean_ctor_set_uint8(x_44, sizeof(void*)*2 + 4, x_43);
x_45 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_45, 0, x_44);
lean_ctor_set(x_24, 0, x_45);
return x_24;
}
else
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; uint32_t x_55; uint8_t x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; 
x_46 = lean_ctor_get(x_24, 1);
lean_inc(x_46);
lean_dec(x_24);
x_47 = lean_ctor_get(x_25, 0);
lean_inc(x_47);
lean_dec(x_25);
x_48 = lean_ctor_get(x_22, 0);
lean_inc(x_48);
x_49 = lean_nat_add(x_48, x_47);
lean_dec(x_48);
x_50 = lean_ctor_get(x_22, 1);
lean_inc(x_50);
lean_dec(x_22);
x_51 = lean_nat_shiftl(x_50, x_47);
lean_dec(x_47);
lean_dec(x_50);
x_52 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_52, 0, x_49);
lean_ctor_set(x_52, 1, x_51);
x_53 = l_Std_BitVec_Literal_toExpr(x_52);
x_54 = lean_box(0);
x_55 = 0;
x_56 = 1;
x_57 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_57, 0, x_53);
lean_ctor_set(x_57, 1, x_54);
lean_ctor_set_uint32(x_57, sizeof(void*)*2, x_55);
lean_ctor_set_uint8(x_57, sizeof(void*)*2 + 4, x_56);
x_58 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_58, 0, x_57);
x_59 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_59, 0, x_58);
lean_ctor_set(x_59, 1, x_46);
return x_59;
}
}
}
else
{
uint8_t x_60; 
lean_dec(x_22);
x_60 = !lean_is_exclusive(x_24);
if (x_60 == 0)
{
return x_24;
}
else
{
lean_object* x_61; lean_object* x_62; lean_object* x_63; 
x_61 = lean_ctor_get(x_24, 0);
x_62 = lean_ctor_get(x_24, 1);
lean_inc(x_62);
lean_inc(x_61);
lean_dec(x_24);
x_63 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_63, 0, x_61);
lean_ctor_set(x_63, 1, x_62);
return x_63;
}
}
}
}
else
{
uint8_t x_64; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_64 = !lean_is_exclusive(x_13);
if (x_64 == 0)
{
return x_13;
}
else
{
lean_object* x_65; lean_object* x_66; lean_object* x_67; 
x_65 = lean_ctor_get(x_13, 0);
x_66 = lean_ctor_get(x_13, 1);
lean_inc(x_66);
lean_inc(x_65);
lean_dec(x_13);
x_67 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_67, 0, x_65);
lean_ctor_set(x_67, 1, x_66);
return x_67;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceShiftLeftZeroExtend___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("shiftLeftZeroExtend", 19);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceShiftLeftZeroExtend___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceShiftLeftZeroExtend___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShiftLeftZeroExtend(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceShiftLeftZeroExtend___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceShiftLeftZeroExtend___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceShiftLeftZeroExtend___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceShiftLeftZeroExtend___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceShiftLeftZeroExtend", 25);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceShiftLeftZeroExtend___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceShiftLeftZeroExtend), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3481_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3483_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceExtracLsb_x27___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; 
x_11 = l_Lean_Expr_appArg_x21(x_1);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_12 = l_Std_BitVec_fromExpr_x3f(x_11, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_12) == 0)
{
lean_object* x_13; 
x_13 = lean_ctor_get(x_12, 0);
lean_inc(x_13);
if (lean_obj_tag(x_13) == 0)
{
uint8_t x_14; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_14 = !lean_is_exclusive(x_12);
if (x_14 == 0)
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_ctor_get(x_12, 0);
lean_dec(x_15);
x_16 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_12, 0, x_16);
return x_12;
}
else
{
lean_object* x_17; lean_object* x_18; lean_object* x_19; 
x_17 = lean_ctor_get(x_12, 1);
lean_inc(x_17);
lean_dec(x_12);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_19 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_19, 0, x_18);
lean_ctor_set(x_19, 1, x_17);
return x_19;
}
}
else
{
lean_object* x_20; lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; 
x_20 = lean_ctor_get(x_12, 1);
lean_inc(x_20);
lean_dec(x_12);
x_21 = lean_ctor_get(x_13, 0);
lean_inc(x_21);
lean_dec(x_13);
x_22 = l_Lean_Expr_appFn_x21(x_1);
lean_inc(x_22);
x_23 = l_Lean_Expr_appFn_x21(x_22);
x_24 = l_Lean_Expr_appArg_x21(x_23);
lean_dec(x_23);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
x_25 = l_Nat_fromExpr_x3f(x_24, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_20);
if (lean_obj_tag(x_25) == 0)
{
lean_object* x_26; 
x_26 = lean_ctor_get(x_25, 0);
lean_inc(x_26);
if (lean_obj_tag(x_26) == 0)
{
uint8_t x_27; 
lean_dec(x_22);
lean_dec(x_21);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
x_27 = !lean_is_exclusive(x_25);
if (x_27 == 0)
{
lean_object* x_28; lean_object* x_29; 
x_28 = lean_ctor_get(x_25, 0);
lean_dec(x_28);
x_29 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_25, 0, x_29);
return x_25;
}
else
{
lean_object* x_30; lean_object* x_31; lean_object* x_32; 
x_30 = lean_ctor_get(x_25, 1);
lean_inc(x_30);
lean_dec(x_25);
x_31 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_32 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_32, 0, x_31);
lean_ctor_set(x_32, 1, x_30);
return x_32;
}
}
else
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; 
x_33 = lean_ctor_get(x_25, 1);
lean_inc(x_33);
lean_dec(x_25);
x_34 = lean_ctor_get(x_26, 0);
lean_inc(x_34);
lean_dec(x_26);
x_35 = l_Lean_Expr_appArg_x21(x_22);
lean_dec(x_22);
x_36 = l_Nat_fromExpr_x3f(x_35, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_33);
lean_dec(x_6);
if (lean_obj_tag(x_36) == 0)
{
lean_object* x_37; 
x_37 = lean_ctor_get(x_36, 0);
lean_inc(x_37);
if (lean_obj_tag(x_37) == 0)
{
uint8_t x_38; 
lean_dec(x_34);
lean_dec(x_21);
x_38 = !lean_is_exclusive(x_36);
if (x_38 == 0)
{
lean_object* x_39; lean_object* x_40; 
x_39 = lean_ctor_get(x_36, 0);
lean_dec(x_39);
x_40 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_36, 0, x_40);
return x_36;
}
else
{
lean_object* x_41; lean_object* x_42; lean_object* x_43; 
x_41 = lean_ctor_get(x_36, 1);
lean_inc(x_41);
lean_dec(x_36);
x_42 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_43 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_43, 0, x_42);
lean_ctor_set(x_43, 1, x_41);
return x_43;
}
}
else
{
uint8_t x_44; 
x_44 = !lean_is_exclusive(x_36);
if (x_44 == 0)
{
lean_object* x_45; lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; lean_object* x_51; uint32_t x_52; uint8_t x_53; lean_object* x_54; lean_object* x_55; 
x_45 = lean_ctor_get(x_36, 0);
lean_dec(x_45);
x_46 = lean_ctor_get(x_37, 0);
lean_inc(x_46);
lean_dec(x_37);
x_47 = lean_ctor_get(x_21, 1);
lean_inc(x_47);
lean_dec(x_21);
x_48 = l_Std_BitVec_extractLsb_x27___rarg(x_34, x_46, x_47);
lean_dec(x_47);
lean_dec(x_34);
x_49 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_49, 0, x_46);
lean_ctor_set(x_49, 1, x_48);
x_50 = l_Std_BitVec_Literal_toExpr(x_49);
x_51 = lean_box(0);
x_52 = 0;
x_53 = 1;
x_54 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_54, 0, x_50);
lean_ctor_set(x_54, 1, x_51);
lean_ctor_set_uint32(x_54, sizeof(void*)*2, x_52);
lean_ctor_set_uint8(x_54, sizeof(void*)*2 + 4, x_53);
x_55 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_55, 0, x_54);
lean_ctor_set(x_36, 0, x_55);
return x_36;
}
else
{
lean_object* x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; lean_object* x_60; lean_object* x_61; lean_object* x_62; uint32_t x_63; uint8_t x_64; lean_object* x_65; lean_object* x_66; lean_object* x_67; 
x_56 = lean_ctor_get(x_36, 1);
lean_inc(x_56);
lean_dec(x_36);
x_57 = lean_ctor_get(x_37, 0);
lean_inc(x_57);
lean_dec(x_37);
x_58 = lean_ctor_get(x_21, 1);
lean_inc(x_58);
lean_dec(x_21);
x_59 = l_Std_BitVec_extractLsb_x27___rarg(x_34, x_57, x_58);
lean_dec(x_58);
lean_dec(x_34);
x_60 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_60, 0, x_57);
lean_ctor_set(x_60, 1, x_59);
x_61 = l_Std_BitVec_Literal_toExpr(x_60);
x_62 = lean_box(0);
x_63 = 0;
x_64 = 1;
x_65 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_65, 0, x_61);
lean_ctor_set(x_65, 1, x_62);
lean_ctor_set_uint32(x_65, sizeof(void*)*2, x_63);
lean_ctor_set_uint8(x_65, sizeof(void*)*2 + 4, x_64);
x_66 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_66, 0, x_65);
x_67 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_67, 0, x_66);
lean_ctor_set(x_67, 1, x_56);
return x_67;
}
}
}
else
{
uint8_t x_68; 
lean_dec(x_34);
lean_dec(x_21);
x_68 = !lean_is_exclusive(x_36);
if (x_68 == 0)
{
return x_36;
}
else
{
lean_object* x_69; lean_object* x_70; lean_object* x_71; 
x_69 = lean_ctor_get(x_36, 0);
x_70 = lean_ctor_get(x_36, 1);
lean_inc(x_70);
lean_inc(x_69);
lean_dec(x_36);
x_71 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_71, 0, x_69);
lean_ctor_set(x_71, 1, x_70);
return x_71;
}
}
}
}
else
{
uint8_t x_72; 
lean_dec(x_22);
lean_dec(x_21);
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
x_72 = !lean_is_exclusive(x_25);
if (x_72 == 0)
{
return x_25;
}
else
{
lean_object* x_73; lean_object* x_74; lean_object* x_75; 
x_73 = lean_ctor_get(x_25, 0);
x_74 = lean_ctor_get(x_25, 1);
lean_inc(x_74);
lean_inc(x_73);
lean_dec(x_25);
x_75 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_75, 0, x_73);
lean_ctor_set(x_75, 1, x_74);
return x_75;
}
}
}
}
else
{
uint8_t x_76; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_76 = !lean_is_exclusive(x_12);
if (x_76 == 0)
{
return x_12;
}
else
{
lean_object* x_77; lean_object* x_78; lean_object* x_79; 
x_77 = lean_ctor_get(x_12, 0);
x_78 = lean_ctor_get(x_12, 1);
lean_inc(x_78);
lean_inc(x_77);
lean_dec(x_12);
x_79 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_79, 0, x_77);
lean_ctor_set(x_79, 1, x_78);
return x_79;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceExtracLsb_x27___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("extractLsb'", 11);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceExtracLsb_x27___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceExtracLsb_x27___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceExtracLsb_x27(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceExtracLsb_x27___closed__2;
x_11 = lean_unsigned_to_nat(4u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceExtracLsb_x27___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceExtracLsb_x27___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceExtracLsb_x27___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceExtracLsb'", 16);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceExtracLsb_x27___closed__2;
x_2 = lean_unsigned_to_nat(4u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__5;
x_2 = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__8() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__7;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__9() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceExtracLsb_x27), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__8;
x_4 = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__9;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3673_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__9;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3675_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__9;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceReplicate___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; 
x_11 = l_Lean_Expr_appArg_x21(x_1);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_12 = l_Std_BitVec_fromExpr_x3f(x_11, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_12) == 0)
{
lean_object* x_13; 
x_13 = lean_ctor_get(x_12, 0);
lean_inc(x_13);
if (lean_obj_tag(x_13) == 0)
{
uint8_t x_14; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_14 = !lean_is_exclusive(x_12);
if (x_14 == 0)
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_ctor_get(x_12, 0);
lean_dec(x_15);
x_16 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_12, 0, x_16);
return x_12;
}
else
{
lean_object* x_17; lean_object* x_18; lean_object* x_19; 
x_17 = lean_ctor_get(x_12, 1);
lean_inc(x_17);
lean_dec(x_12);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_19 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_19, 0, x_18);
lean_ctor_set(x_19, 1, x_17);
return x_19;
}
}
else
{
lean_object* x_20; lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_20 = lean_ctor_get(x_12, 1);
lean_inc(x_20);
lean_dec(x_12);
x_21 = lean_ctor_get(x_13, 0);
lean_inc(x_21);
lean_dec(x_13);
x_22 = l_Lean_Expr_appFn_x21(x_1);
x_23 = l_Lean_Expr_appArg_x21(x_22);
lean_dec(x_22);
x_24 = l_Nat_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_20);
lean_dec(x_6);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_21);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; lean_object* x_40; lean_object* x_41; uint32_t x_42; uint8_t x_43; lean_object* x_44; lean_object* x_45; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = lean_ctor_get(x_21, 0);
lean_inc(x_35);
x_36 = lean_nat_mul(x_35, x_34);
x_37 = lean_ctor_get(x_21, 1);
lean_inc(x_37);
lean_dec(x_21);
x_38 = l_Std_BitVec_replicate(x_35, x_34, x_37);
lean_dec(x_37);
lean_dec(x_34);
lean_dec(x_35);
x_39 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_39, 0, x_36);
lean_ctor_set(x_39, 1, x_38);
x_40 = l_Std_BitVec_Literal_toExpr(x_39);
x_41 = lean_box(0);
x_42 = 0;
x_43 = 1;
x_44 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_44, 0, x_40);
lean_ctor_set(x_44, 1, x_41);
lean_ctor_set_uint32(x_44, sizeof(void*)*2, x_42);
lean_ctor_set_uint8(x_44, sizeof(void*)*2 + 4, x_43);
x_45 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_45, 0, x_44);
lean_ctor_set(x_24, 0, x_45);
return x_24;
}
else
{
lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; lean_object* x_53; lean_object* x_54; uint32_t x_55; uint8_t x_56; lean_object* x_57; lean_object* x_58; lean_object* x_59; 
x_46 = lean_ctor_get(x_24, 1);
lean_inc(x_46);
lean_dec(x_24);
x_47 = lean_ctor_get(x_25, 0);
lean_inc(x_47);
lean_dec(x_25);
x_48 = lean_ctor_get(x_21, 0);
lean_inc(x_48);
x_49 = lean_nat_mul(x_48, x_47);
x_50 = lean_ctor_get(x_21, 1);
lean_inc(x_50);
lean_dec(x_21);
x_51 = l_Std_BitVec_replicate(x_48, x_47, x_50);
lean_dec(x_50);
lean_dec(x_47);
lean_dec(x_48);
x_52 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_52, 0, x_49);
lean_ctor_set(x_52, 1, x_51);
x_53 = l_Std_BitVec_Literal_toExpr(x_52);
x_54 = lean_box(0);
x_55 = 0;
x_56 = 1;
x_57 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_57, 0, x_53);
lean_ctor_set(x_57, 1, x_54);
lean_ctor_set_uint32(x_57, sizeof(void*)*2, x_55);
lean_ctor_set_uint8(x_57, sizeof(void*)*2 + 4, x_56);
x_58 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_58, 0, x_57);
x_59 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_59, 0, x_58);
lean_ctor_set(x_59, 1, x_46);
return x_59;
}
}
}
else
{
uint8_t x_60; 
lean_dec(x_21);
x_60 = !lean_is_exclusive(x_24);
if (x_60 == 0)
{
return x_24;
}
else
{
lean_object* x_61; lean_object* x_62; lean_object* x_63; 
x_61 = lean_ctor_get(x_24, 0);
x_62 = lean_ctor_get(x_24, 1);
lean_inc(x_62);
lean_inc(x_61);
lean_dec(x_24);
x_63 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_63, 0, x_61);
lean_ctor_set(x_63, 1, x_62);
return x_63;
}
}
}
}
else
{
uint8_t x_64; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_64 = !lean_is_exclusive(x_12);
if (x_64 == 0)
{
return x_12;
}
else
{
lean_object* x_65; lean_object* x_66; lean_object* x_67; 
x_65 = lean_ctor_get(x_12, 0);
x_66 = lean_ctor_get(x_12, 1);
lean_inc(x_66);
lean_inc(x_65);
lean_dec(x_12);
x_67 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_67, 0, x_65);
lean_ctor_set(x_67, 1, x_66);
return x_67;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceReplicate___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("replicate", 9);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceReplicate___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceReplicate___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceReplicate(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceReplicate___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceReplicate___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceReplicate___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceReplicate___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceReplicate", 15);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceReplicate___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceReplicate), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3831_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3833_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceZeroExtend___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; 
x_11 = l_Lean_Expr_appArg_x21(x_1);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_12 = l_Std_BitVec_fromExpr_x3f(x_11, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_12) == 0)
{
lean_object* x_13; 
x_13 = lean_ctor_get(x_12, 0);
lean_inc(x_13);
if (lean_obj_tag(x_13) == 0)
{
uint8_t x_14; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_14 = !lean_is_exclusive(x_12);
if (x_14 == 0)
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_ctor_get(x_12, 0);
lean_dec(x_15);
x_16 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_12, 0, x_16);
return x_12;
}
else
{
lean_object* x_17; lean_object* x_18; lean_object* x_19; 
x_17 = lean_ctor_get(x_12, 1);
lean_inc(x_17);
lean_dec(x_12);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_19 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_19, 0, x_18);
lean_ctor_set(x_19, 1, x_17);
return x_19;
}
}
else
{
lean_object* x_20; lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_20 = lean_ctor_get(x_12, 1);
lean_inc(x_20);
lean_dec(x_12);
x_21 = lean_ctor_get(x_13, 0);
lean_inc(x_21);
lean_dec(x_13);
x_22 = l_Lean_Expr_appFn_x21(x_1);
x_23 = l_Lean_Expr_appArg_x21(x_22);
lean_dec(x_22);
x_24 = l_Nat_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_20);
lean_dec(x_6);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_21);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; lean_object* x_40; uint32_t x_41; uint8_t x_42; lean_object* x_43; lean_object* x_44; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = lean_ctor_get(x_21, 0);
lean_inc(x_35);
x_36 = lean_ctor_get(x_21, 1);
lean_inc(x_36);
lean_dec(x_21);
x_37 = l_Std_BitVec_zeroExtend(x_35, x_34, x_36);
lean_dec(x_36);
lean_dec(x_35);
x_38 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_38, 0, x_34);
lean_ctor_set(x_38, 1, x_37);
x_39 = l_Std_BitVec_Literal_toExpr(x_38);
x_40 = lean_box(0);
x_41 = 0;
x_42 = 1;
x_43 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_43, 0, x_39);
lean_ctor_set(x_43, 1, x_40);
lean_ctor_set_uint32(x_43, sizeof(void*)*2, x_41);
lean_ctor_set_uint8(x_43, sizeof(void*)*2 + 4, x_42);
x_44 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_44, 0, x_43);
lean_ctor_set(x_24, 0, x_44);
return x_24;
}
else
{
lean_object* x_45; lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; uint32_t x_53; uint8_t x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; 
x_45 = lean_ctor_get(x_24, 1);
lean_inc(x_45);
lean_dec(x_24);
x_46 = lean_ctor_get(x_25, 0);
lean_inc(x_46);
lean_dec(x_25);
x_47 = lean_ctor_get(x_21, 0);
lean_inc(x_47);
x_48 = lean_ctor_get(x_21, 1);
lean_inc(x_48);
lean_dec(x_21);
x_49 = l_Std_BitVec_zeroExtend(x_47, x_46, x_48);
lean_dec(x_48);
lean_dec(x_47);
x_50 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_50, 0, x_46);
lean_ctor_set(x_50, 1, x_49);
x_51 = l_Std_BitVec_Literal_toExpr(x_50);
x_52 = lean_box(0);
x_53 = 0;
x_54 = 1;
x_55 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_55, 0, x_51);
lean_ctor_set(x_55, 1, x_52);
lean_ctor_set_uint32(x_55, sizeof(void*)*2, x_53);
lean_ctor_set_uint8(x_55, sizeof(void*)*2 + 4, x_54);
x_56 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_56, 0, x_55);
x_57 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_57, 0, x_56);
lean_ctor_set(x_57, 1, x_45);
return x_57;
}
}
}
else
{
uint8_t x_58; 
lean_dec(x_21);
x_58 = !lean_is_exclusive(x_24);
if (x_58 == 0)
{
return x_24;
}
else
{
lean_object* x_59; lean_object* x_60; lean_object* x_61; 
x_59 = lean_ctor_get(x_24, 0);
x_60 = lean_ctor_get(x_24, 1);
lean_inc(x_60);
lean_inc(x_59);
lean_dec(x_24);
x_61 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_61, 0, x_59);
lean_ctor_set(x_61, 1, x_60);
return x_61;
}
}
}
}
else
{
uint8_t x_62; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_62 = !lean_is_exclusive(x_12);
if (x_62 == 0)
{
return x_12;
}
else
{
lean_object* x_63; lean_object* x_64; lean_object* x_65; 
x_63 = lean_ctor_get(x_12, 0);
x_64 = lean_ctor_get(x_12, 1);
lean_inc(x_64);
lean_inc(x_63);
lean_dec(x_12);
x_65 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_65, 0, x_63);
lean_ctor_set(x_65, 1, x_64);
return x_65;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceZeroExtend___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("zeroExtend", 10);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceZeroExtend___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceZeroExtend___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceZeroExtend(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceZeroExtend___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceZeroExtend___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceZeroExtend___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceZeroExtend___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceZeroExtend", 16);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceZeroExtend___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceZeroExtend), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3852_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3854_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSignExtend___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; 
x_11 = l_Lean_Expr_appArg_x21(x_1);
lean_inc(x_9);
lean_inc(x_8);
lean_inc(x_7);
lean_inc(x_6);
x_12 = l_Std_BitVec_fromExpr_x3f(x_11, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_12) == 0)
{
lean_object* x_13; 
x_13 = lean_ctor_get(x_12, 0);
lean_inc(x_13);
if (lean_obj_tag(x_13) == 0)
{
uint8_t x_14; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_14 = !lean_is_exclusive(x_12);
if (x_14 == 0)
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_ctor_get(x_12, 0);
lean_dec(x_15);
x_16 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_12, 0, x_16);
return x_12;
}
else
{
lean_object* x_17; lean_object* x_18; lean_object* x_19; 
x_17 = lean_ctor_get(x_12, 1);
lean_inc(x_17);
lean_dec(x_12);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_19 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_19, 0, x_18);
lean_ctor_set(x_19, 1, x_17);
return x_19;
}
}
else
{
lean_object* x_20; lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; 
x_20 = lean_ctor_get(x_12, 1);
lean_inc(x_20);
lean_dec(x_12);
x_21 = lean_ctor_get(x_13, 0);
lean_inc(x_21);
lean_dec(x_13);
x_22 = l_Lean_Expr_appFn_x21(x_1);
x_23 = l_Lean_Expr_appArg_x21(x_22);
lean_dec(x_22);
x_24 = l_Nat_fromExpr_x3f(x_23, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_20);
lean_dec(x_6);
if (lean_obj_tag(x_24) == 0)
{
lean_object* x_25; 
x_25 = lean_ctor_get(x_24, 0);
lean_inc(x_25);
if (lean_obj_tag(x_25) == 0)
{
uint8_t x_26; 
lean_dec(x_21);
x_26 = !lean_is_exclusive(x_24);
if (x_26 == 0)
{
lean_object* x_27; lean_object* x_28; 
x_27 = lean_ctor_get(x_24, 0);
lean_dec(x_27);
x_28 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_24, 0, x_28);
return x_24;
}
else
{
lean_object* x_29; lean_object* x_30; lean_object* x_31; 
x_29 = lean_ctor_get(x_24, 1);
lean_inc(x_29);
lean_dec(x_24);
x_30 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_31 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_31, 0, x_30);
lean_ctor_set(x_31, 1, x_29);
return x_31;
}
}
else
{
uint8_t x_32; 
x_32 = !lean_is_exclusive(x_24);
if (x_32 == 0)
{
lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; lean_object* x_37; lean_object* x_38; lean_object* x_39; lean_object* x_40; uint32_t x_41; uint8_t x_42; lean_object* x_43; lean_object* x_44; 
x_33 = lean_ctor_get(x_24, 0);
lean_dec(x_33);
x_34 = lean_ctor_get(x_25, 0);
lean_inc(x_34);
lean_dec(x_25);
x_35 = lean_ctor_get(x_21, 0);
lean_inc(x_35);
x_36 = lean_ctor_get(x_21, 1);
lean_inc(x_36);
lean_dec(x_21);
x_37 = l_Std_BitVec_signExtend(x_35, x_34, x_36);
lean_dec(x_35);
x_38 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_38, 0, x_34);
lean_ctor_set(x_38, 1, x_37);
x_39 = l_Std_BitVec_Literal_toExpr(x_38);
x_40 = lean_box(0);
x_41 = 0;
x_42 = 1;
x_43 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_43, 0, x_39);
lean_ctor_set(x_43, 1, x_40);
lean_ctor_set_uint32(x_43, sizeof(void*)*2, x_41);
lean_ctor_set_uint8(x_43, sizeof(void*)*2 + 4, x_42);
x_44 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_44, 0, x_43);
lean_ctor_set(x_24, 0, x_44);
return x_24;
}
else
{
lean_object* x_45; lean_object* x_46; lean_object* x_47; lean_object* x_48; lean_object* x_49; lean_object* x_50; lean_object* x_51; lean_object* x_52; uint32_t x_53; uint8_t x_54; lean_object* x_55; lean_object* x_56; lean_object* x_57; 
x_45 = lean_ctor_get(x_24, 1);
lean_inc(x_45);
lean_dec(x_24);
x_46 = lean_ctor_get(x_25, 0);
lean_inc(x_46);
lean_dec(x_25);
x_47 = lean_ctor_get(x_21, 0);
lean_inc(x_47);
x_48 = lean_ctor_get(x_21, 1);
lean_inc(x_48);
lean_dec(x_21);
x_49 = l_Std_BitVec_signExtend(x_47, x_46, x_48);
lean_dec(x_47);
x_50 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_50, 0, x_46);
lean_ctor_set(x_50, 1, x_49);
x_51 = l_Std_BitVec_Literal_toExpr(x_50);
x_52 = lean_box(0);
x_53 = 0;
x_54 = 1;
x_55 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_55, 0, x_51);
lean_ctor_set(x_55, 1, x_52);
lean_ctor_set_uint32(x_55, sizeof(void*)*2, x_53);
lean_ctor_set_uint8(x_55, sizeof(void*)*2 + 4, x_54);
x_56 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_56, 0, x_55);
x_57 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_57, 0, x_56);
lean_ctor_set(x_57, 1, x_45);
return x_57;
}
}
}
else
{
uint8_t x_58; 
lean_dec(x_21);
x_58 = !lean_is_exclusive(x_24);
if (x_58 == 0)
{
return x_24;
}
else
{
lean_object* x_59; lean_object* x_60; lean_object* x_61; 
x_59 = lean_ctor_get(x_24, 0);
x_60 = lean_ctor_get(x_24, 1);
lean_inc(x_60);
lean_inc(x_59);
lean_dec(x_24);
x_61 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_61, 0, x_59);
lean_ctor_set(x_61, 1, x_60);
return x_61;
}
}
}
}
else
{
uint8_t x_62; 
lean_dec(x_9);
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_1);
x_62 = !lean_is_exclusive(x_12);
if (x_62 == 0)
{
return x_12;
}
else
{
lean_object* x_63; lean_object* x_64; lean_object* x_65; 
x_63 = lean_ctor_get(x_12, 0);
x_64 = lean_ctor_get(x_12, 1);
lean_inc(x_64);
lean_inc(x_63);
lean_dec(x_12);
x_65 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_65, 0, x_63);
lean_ctor_set(x_65, 1, x_64);
return x_65;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceSignExtend___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("signExtend", 10);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceSignExtend___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceSignExtend___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSignExtend(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceSignExtend___closed__2;
x_11 = lean_unsigned_to_nat(3u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceSignExtend___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceSignExtend___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceSignExtend___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_11;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceSignExtend", 16);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceSignExtend___closed__2;
x_2 = lean_unsigned_to_nat(3u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__4;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__7() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__6;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__8() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceSignExtend), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__7;
x_4 = l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__8;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3873_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3875_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__8;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAllOnes___lambda__1(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; lean_object* x_12; 
x_11 = l_Lean_Expr_appArg_x21(x_1);
x_12 = l_Nat_fromExpr_x3f(x_11, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
if (lean_obj_tag(x_12) == 0)
{
lean_object* x_13; 
x_13 = lean_ctor_get(x_12, 0);
lean_inc(x_13);
if (lean_obj_tag(x_13) == 0)
{
uint8_t x_14; 
x_14 = !lean_is_exclusive(x_12);
if (x_14 == 0)
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_ctor_get(x_12, 0);
lean_dec(x_15);
x_16 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
lean_ctor_set(x_12, 0, x_16);
return x_12;
}
else
{
lean_object* x_17; lean_object* x_18; lean_object* x_19; 
x_17 = lean_ctor_get(x_12, 1);
lean_inc(x_17);
lean_dec(x_12);
x_18 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_19 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_19, 0, x_18);
lean_ctor_set(x_19, 1, x_17);
return x_19;
}
}
else
{
uint8_t x_20; 
x_20 = !lean_is_exclusive(x_12);
if (x_20 == 0)
{
lean_object* x_21; lean_object* x_22; lean_object* x_23; lean_object* x_24; lean_object* x_25; lean_object* x_26; uint32_t x_27; uint8_t x_28; lean_object* x_29; lean_object* x_30; 
x_21 = lean_ctor_get(x_12, 0);
lean_dec(x_21);
x_22 = lean_ctor_get(x_13, 0);
lean_inc(x_22);
lean_dec(x_13);
x_23 = l_Std_BitVec_allOnes(x_22);
x_24 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_24, 0, x_22);
lean_ctor_set(x_24, 1, x_23);
x_25 = l_Std_BitVec_Literal_toExpr(x_24);
x_26 = lean_box(0);
x_27 = 0;
x_28 = 1;
x_29 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_29, 0, x_25);
lean_ctor_set(x_29, 1, x_26);
lean_ctor_set_uint32(x_29, sizeof(void*)*2, x_27);
lean_ctor_set_uint8(x_29, sizeof(void*)*2 + 4, x_28);
x_30 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_30, 0, x_29);
lean_ctor_set(x_12, 0, x_30);
return x_12;
}
else
{
lean_object* x_31; lean_object* x_32; lean_object* x_33; lean_object* x_34; lean_object* x_35; lean_object* x_36; uint32_t x_37; uint8_t x_38; lean_object* x_39; lean_object* x_40; lean_object* x_41; 
x_31 = lean_ctor_get(x_12, 1);
lean_inc(x_31);
lean_dec(x_12);
x_32 = lean_ctor_get(x_13, 0);
lean_inc(x_32);
lean_dec(x_13);
x_33 = l_Std_BitVec_allOnes(x_32);
x_34 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_34, 0, x_32);
lean_ctor_set(x_34, 1, x_33);
x_35 = l_Std_BitVec_Literal_toExpr(x_34);
x_36 = lean_box(0);
x_37 = 0;
x_38 = 1;
x_39 = lean_alloc_ctor(0, 2, 5);
lean_ctor_set(x_39, 0, x_35);
lean_ctor_set(x_39, 1, x_36);
lean_ctor_set_uint32(x_39, sizeof(void*)*2, x_37);
lean_ctor_set_uint8(x_39, sizeof(void*)*2 + 4, x_38);
x_40 = lean_alloc_ctor(0, 1, 0);
lean_ctor_set(x_40, 0, x_39);
x_41 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_41, 0, x_40);
lean_ctor_set(x_41, 1, x_31);
return x_41;
}
}
}
else
{
uint8_t x_42; 
x_42 = !lean_is_exclusive(x_12);
if (x_42 == 0)
{
return x_12;
}
else
{
lean_object* x_43; lean_object* x_44; lean_object* x_45; 
x_43 = lean_ctor_get(x_12, 0);
x_44 = lean_ctor_get(x_12, 1);
lean_inc(x_44);
lean_inc(x_43);
lean_dec(x_12);
x_45 = lean_alloc_ctor(1, 2, 0);
lean_ctor_set(x_45, 0, x_43);
lean_ctor_set(x_45, 1, x_44);
return x_45;
}
}
}
}
static lean_object* _init_l_Std_BitVec_reduceAllOnes___closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("allOnes", 7);
return x_1;
}
}
static lean_object* _init_l_Std_BitVec_reduceAllOnes___closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l_Std_BitVec_reduceAllOnes___closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAllOnes(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; lean_object* x_11; uint8_t x_12; 
x_10 = l_Std_BitVec_reduceAllOnes___closed__2;
x_11 = lean_unsigned_to_nat(1u);
x_12 = l_Lean_Expr_isAppOfArity(x_1, x_10, x_11);
if (x_12 == 0)
{
lean_object* x_13; lean_object* x_14; 
lean_dec(x_8);
lean_dec(x_7);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
x_13 = l_Std_BitVec_reduceUnary___lambda__1___closed__1;
x_14 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_14, 0, x_13);
lean_ctor_set(x_14, 1, x_9);
return x_14;
}
else
{
lean_object* x_15; lean_object* x_16; 
x_15 = lean_box(0);
x_16 = l_Std_BitVec_reduceAllOnes___lambda__1(x_1, x_15, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
return x_16;
}
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAllOnes___lambda__1___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9, lean_object* x_10) {
_start:
{
lean_object* x_11; 
x_11 = l_Std_BitVec_reduceAllOnes___lambda__1(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10);
lean_dec(x_6);
lean_dec(x_5);
lean_dec(x_4);
lean_dec(x_3);
lean_dec(x_2);
lean_dec(x_1);
return x_11;
}
}
LEAN_EXPORT lean_object* l_Std_BitVec_reduceAllOnes___boxed(lean_object* x_1, lean_object* x_2, lean_object* x_3, lean_object* x_4, lean_object* x_5, lean_object* x_6, lean_object* x_7, lean_object* x_8, lean_object* x_9) {
_start:
{
lean_object* x_10; 
x_10 = l_Std_BitVec_reduceAllOnes(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9);
lean_dec(x_1);
return x_10;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__1() {
_start:
{
lean_object* x_1; 
x_1 = lean_mk_string_from_bytes("reduceAllOnes", 13);
return x_1;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__2() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; lean_object* x_4; 
x_1 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4;
x_2 = l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5;
x_3 = l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__1;
x_4 = l_Lean_Name_mkStr3(x_1, x_2, x_3);
return x_4;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__3() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l_Std_BitVec_reduceAllOnes___closed__2;
x_2 = lean_unsigned_to_nat(1u);
x_3 = lean_alloc_ctor(0, 2, 0);
lean_ctor_set(x_3, 0, x_1);
lean_ctor_set(x_3, 1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__4() {
_start:
{
lean_object* x_1; lean_object* x_2; 
x_1 = lean_unsigned_to_nat(2u);
x_2 = lean_mk_empty_array_with_capacity(x_1);
return x_2;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__5() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__4;
x_2 = l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__3;
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__6() {
_start:
{
lean_object* x_1; lean_object* x_2; lean_object* x_3; 
x_1 = l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__5;
x_2 = lean_box(3);
x_3 = lean_array_push(x_1, x_2);
return x_3;
}
}
static lean_object* _init_l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__7() {
_start:
{
lean_object* x_1; 
x_1 = lean_alloc_closure((void*)(l_Std_BitVec_reduceAllOnes___boxed), 9, 0);
return x_1;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; lean_object* x_4; lean_object* x_5; 
x_2 = l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__2;
x_3 = l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__6;
x_4 = l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__7;
x_5 = l_Lean_Meta_Simp_registerBuiltinSimproc(x_2, x_3, x_4, x_1);
return x_5;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3992_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__7;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
LEAN_EXPORT lean_object* l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3994_(lean_object* x_1) {
_start:
{
lean_object* x_2; lean_object* x_3; uint8_t x_4; lean_object* x_5; lean_object* x_6; 
x_2 = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1;
x_3 = l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__2;
x_4 = 1;
x_5 = l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__7;
x_6 = l_Lean_Meta_Simp_addSimprocBuiltinAttrCore(x_2, x_3, x_4, x_5, x_1);
return x_6;
}
}
lean_object* initialize_Lean_Meta_Tactic_Simp_BuiltinSimprocs_Nat(uint8_t builtin, lean_object*);
lean_object* initialize_Lean_Meta_Tactic_Simp_BuiltinSimprocs_Int(uint8_t builtin, lean_object*);
lean_object* initialize_Init_Data_BitVec_Basic(uint8_t builtin, lean_object*);
static bool _G_initialized = false;
LEAN_EXPORT lean_object* initialize_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec(uint8_t builtin, lean_object* w) {
lean_object * res;
if (_G_initialized) return lean_io_result_mk_ok(lean_box(0));
_G_initialized = true;
res = initialize_Lean_Meta_Tactic_Simp_BuiltinSimprocs_Nat(builtin, lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
res = initialize_Lean_Meta_Tactic_Simp_BuiltinSimprocs_Int(builtin, lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
res = initialize_Init_Data_BitVec_Basic(builtin, lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__1 = _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__1();
lean_mark_persistent(l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__1);
l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__2 = _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__2();
lean_mark_persistent(l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__2);
l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__3 = _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__3();
lean_mark_persistent(l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__3);
l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4 = _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4();
lean_mark_persistent(l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__4);
l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5 = _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5();
lean_mark_persistent(l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__5);
l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__6 = _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__6();
lean_mark_persistent(l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__6);
l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__7 = _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__7();
lean_mark_persistent(l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromOfNatExpr_x3f___closed__7);
l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromBitVecExpr_x3f___closed__1 = _init_l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromBitVecExpr_x3f___closed__1();
lean_mark_persistent(l___private_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec_0__Std_BitVec_fromBitVecExpr_x3f___closed__1);
l_Std_BitVec_Literal_toExpr___closed__1 = _init_l_Std_BitVec_Literal_toExpr___closed__1();
lean_mark_persistent(l_Std_BitVec_Literal_toExpr___closed__1);
l_Std_BitVec_reduceUnary___lambda__1___closed__1 = _init_l_Std_BitVec_reduceUnary___lambda__1___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceUnary___lambda__1___closed__1);
l_Std_BitVec_reduceBin___lambda__2___closed__1 = _init_l_Std_BitVec_reduceBin___lambda__2___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceBin___lambda__2___closed__1);
l_Std_BitVec_reduceBin___lambda__2___closed__2 = _init_l_Std_BitVec_reduceBin___lambda__2___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceBin___lambda__2___closed__2);
l_Std_BitVec_reduceBin___lambda__2___closed__3 = _init_l_Std_BitVec_reduceBin___lambda__2___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceBin___lambda__2___closed__3);
l_Std_BitVec_reduceBin___lambda__2___closed__4 = _init_l_Std_BitVec_reduceBin___lambda__2___closed__4();
lean_mark_persistent(l_Std_BitVec_reduceBin___lambda__2___closed__4);
l_Std_BitVec_reduceBin___lambda__2___closed__5 = _init_l_Std_BitVec_reduceBin___lambda__2___closed__5();
lean_mark_persistent(l_Std_BitVec_reduceBin___lambda__2___closed__5);
l_Std_BitVec_reduceBin___lambda__2___closed__6 = _init_l_Std_BitVec_reduceBin___lambda__2___closed__6();
lean_mark_persistent(l_Std_BitVec_reduceBin___lambda__2___closed__6);
l_Std_BitVec_reduceBin___lambda__2___closed__7 = _init_l_Std_BitVec_reduceBin___lambda__2___closed__7();
lean_mark_persistent(l_Std_BitVec_reduceBin___lambda__2___closed__7);
l_Std_BitVec_reduceBin___lambda__2___closed__8 = _init_l_Std_BitVec_reduceBin___lambda__2___closed__8();
lean_mark_persistent(l_Std_BitVec_reduceBin___lambda__2___closed__8);
l_Std_BitVec_reduceBin___lambda__2___closed__9 = _init_l_Std_BitVec_reduceBin___lambda__2___closed__9();
lean_mark_persistent(l_Std_BitVec_reduceBin___lambda__2___closed__9);
l_Std_BitVec_reduceBin___lambda__2___closed__10 = _init_l_Std_BitVec_reduceBin___lambda__2___closed__10();
lean_mark_persistent(l_Std_BitVec_reduceBin___lambda__2___closed__10);
l_Std_BitVec_reduceBin___lambda__2___closed__11 = _init_l_Std_BitVec_reduceBin___lambda__2___closed__11();
lean_mark_persistent(l_Std_BitVec_reduceBin___lambda__2___closed__11);
l_Std_BitVec_reduceBin___lambda__2___closed__12 = _init_l_Std_BitVec_reduceBin___lambda__2___closed__12();
lean_mark_persistent(l_Std_BitVec_reduceBin___lambda__2___closed__12);
l_Std_BitVec_reduceBin___lambda__2___closed__13 = _init_l_Std_BitVec_reduceBin___lambda__2___closed__13();
lean_mark_persistent(l_Std_BitVec_reduceBin___lambda__2___closed__13);
l_Std_BitVec_reduceBin___lambda__2___closed__14 = _init_l_Std_BitVec_reduceBin___lambda__2___closed__14();
lean_mark_persistent(l_Std_BitVec_reduceBin___lambda__2___closed__14);
l_Std_BitVec_reduceBin___lambda__2___closed__15 = _init_l_Std_BitVec_reduceBin___lambda__2___closed__15();
lean_mark_persistent(l_Std_BitVec_reduceBin___lambda__2___closed__15);
l_Std_BitVec_reduceGetBit___lambda__1___closed__1 = _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceGetBit___lambda__1___closed__1);
l_Std_BitVec_reduceGetBit___lambda__1___closed__2 = _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceGetBit___lambda__1___closed__2);
l_Std_BitVec_reduceGetBit___lambda__1___closed__3 = _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceGetBit___lambda__1___closed__3);
l_Std_BitVec_reduceGetBit___lambda__1___closed__4 = _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__4();
lean_mark_persistent(l_Std_BitVec_reduceGetBit___lambda__1___closed__4);
l_Std_BitVec_reduceGetBit___lambda__1___closed__5 = _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__5();
lean_mark_persistent(l_Std_BitVec_reduceGetBit___lambda__1___closed__5);
l_Std_BitVec_reduceGetBit___lambda__1___closed__6 = _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__6();
lean_mark_persistent(l_Std_BitVec_reduceGetBit___lambda__1___closed__6);
l_Std_BitVec_reduceGetBit___lambda__1___closed__7 = _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__7();
lean_mark_persistent(l_Std_BitVec_reduceGetBit___lambda__1___closed__7);
l_Std_BitVec_reduceGetBit___lambda__1___closed__8 = _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__8();
lean_mark_persistent(l_Std_BitVec_reduceGetBit___lambda__1___closed__8);
l_Std_BitVec_reduceGetBit___lambda__1___closed__9 = _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__9();
lean_mark_persistent(l_Std_BitVec_reduceGetBit___lambda__1___closed__9);
l_Std_BitVec_reduceGetBit___lambda__1___closed__10 = _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__10();
lean_mark_persistent(l_Std_BitVec_reduceGetBit___lambda__1___closed__10);
l_Std_BitVec_reduceGetBit___lambda__1___closed__11 = _init_l_Std_BitVec_reduceGetBit___lambda__1___closed__11();
lean_mark_persistent(l_Std_BitVec_reduceGetBit___lambda__1___closed__11);
l_Std_BitVec_reduceNeg___closed__1 = _init_l_Std_BitVec_reduceNeg___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceNeg___closed__1);
l_Std_BitVec_reduceNeg___closed__2 = _init_l_Std_BitVec_reduceNeg___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceNeg___closed__2);
l_Std_BitVec_reduceNeg___closed__3 = _init_l_Std_BitVec_reduceNeg___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceNeg___closed__3);
l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__1);
l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__2);
l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__3);
l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__4);
l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__5);
l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__6);
l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__7);
l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__8);
l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__9);
l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__10 = _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__10();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__10);
l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__11 = _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__11();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371____closed__11);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1371_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373____closed__1);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1373_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375____closed__1);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceNeg_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1375_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceNot___closed__1 = _init_l_Std_BitVec_reduceNot___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceNot___closed__1);
l_Std_BitVec_reduceNot___closed__2 = _init_l_Std_BitVec_reduceNot___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceNot___closed__2);
l_Std_BitVec_reduceNot___closed__3 = _init_l_Std_BitVec_reduceNot___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceNot___closed__3);
l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__1);
l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__2);
l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__3);
l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__4);
l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__5);
l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__6);
l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__7);
l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__8);
l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408____closed__9);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1408_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1410_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceNot_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1412_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceAbs___closed__1 = _init_l_Std_BitVec_reduceAbs___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceAbs___closed__1);
l_Std_BitVec_reduceAbs___closed__2 = _init_l_Std_BitVec_reduceAbs___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceAbs___closed__2);
l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__1);
l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__2);
l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__3);
l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__4);
l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__5);
l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__6);
l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__7);
l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1429_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1431_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceAbs_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1433_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceAnd___closed__1 = _init_l_Std_BitVec_reduceAnd___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceAnd___closed__1);
l_Std_BitVec_reduceAnd___closed__2 = _init_l_Std_BitVec_reduceAnd___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceAnd___closed__2);
l_Std_BitVec_reduceAnd___closed__3 = _init_l_Std_BitVec_reduceAnd___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceAnd___closed__3);
l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__1);
l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__2);
l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__3);
l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__4);
l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__5);
l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__6);
l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__7);
l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__8);
l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__9);
l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__10 = _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__10();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__10);
l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__11 = _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__11();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__11);
l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__12 = _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__12();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__12);
l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__13 = _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__13();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__13);
l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__14 = _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__14();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__14);
l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__15 = _init_l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__15();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471____closed__15);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1471_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1473_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceAnd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1475_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceOr___closed__1 = _init_l_Std_BitVec_reduceOr___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceOr___closed__1);
l_Std_BitVec_reduceOr___closed__2 = _init_l_Std_BitVec_reduceOr___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceOr___closed__2);
l_Std_BitVec_reduceOr___closed__3 = _init_l_Std_BitVec_reduceOr___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceOr___closed__3);
l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__1);
l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__2);
l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__3);
l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__4);
l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__5);
l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__6);
l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__7);
l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__8);
l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__9);
l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__10 = _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__10();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__10);
l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__11 = _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__11();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__11);
l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__12 = _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__12();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__12);
l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__13 = _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__13();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__13);
l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__14 = _init_l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__14();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513____closed__14);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1513_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1515_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1517_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceXOr___closed__1 = _init_l_Std_BitVec_reduceXOr___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceXOr___closed__1);
l_Std_BitVec_reduceXOr___closed__2 = _init_l_Std_BitVec_reduceXOr___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceXOr___closed__2);
l_Std_BitVec_reduceXOr___closed__3 = _init_l_Std_BitVec_reduceXOr___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceXOr___closed__3);
l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__1);
l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__2);
l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__3);
l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__4);
l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__5);
l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__6);
l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__7);
l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__8);
l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__9);
l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__10 = _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__10();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__10);
l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__11 = _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__11();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__11);
l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__12 = _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__12();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__12);
l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__13 = _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__13();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__13);
l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__14 = _init_l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__14();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555____closed__14);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1555_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1557_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceXOr_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1559_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceAdd___closed__1 = _init_l_Std_BitVec_reduceAdd___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceAdd___closed__1);
l_Std_BitVec_reduceAdd___closed__2 = _init_l_Std_BitVec_reduceAdd___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceAdd___closed__2);
l_Std_BitVec_reduceAdd___closed__3 = _init_l_Std_BitVec_reduceAdd___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceAdd___closed__3);
l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__1);
l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__2);
l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__3);
l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__4);
l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__5);
l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__6);
l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__7);
l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__8);
l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__9);
l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__10 = _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__10();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__10);
l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__11 = _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__11();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__11);
l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__12 = _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__12();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__12);
l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__13 = _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__13();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__13);
l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__14 = _init_l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__14();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597____closed__14);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1597_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1599_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceAdd_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1601_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceMul___closed__1 = _init_l_Std_BitVec_reduceMul___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceMul___closed__1);
l_Std_BitVec_reduceMul___closed__2 = _init_l_Std_BitVec_reduceMul___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceMul___closed__2);
l_Std_BitVec_reduceMul___closed__3 = _init_l_Std_BitVec_reduceMul___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceMul___closed__3);
l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__1);
l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__2);
l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__3);
l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__4);
l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__5);
l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__6);
l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__7);
l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__8);
l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__9);
l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__10 = _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__10();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__10);
l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__11 = _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__11();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__11);
l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__12 = _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__12();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__12);
l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__13 = _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__13();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__13);
l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__14 = _init_l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__14();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639____closed__14);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1639_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1641_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceMul_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1643_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceSub___closed__1 = _init_l_Std_BitVec_reduceSub___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceSub___closed__1);
l_Std_BitVec_reduceSub___closed__2 = _init_l_Std_BitVec_reduceSub___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceSub___closed__2);
l_Std_BitVec_reduceSub___closed__3 = _init_l_Std_BitVec_reduceSub___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceSub___closed__3);
l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__1);
l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__2);
l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__3);
l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__4);
l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__5);
l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__6);
l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__7);
l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__8);
l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__9);
l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__10 = _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__10();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__10);
l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__11 = _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__11();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__11);
l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__12 = _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__12();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__12);
l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__13 = _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__13();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__13);
l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__14 = _init_l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__14();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681____closed__14);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1681_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1683_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSub_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1685_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceDiv___closed__1 = _init_l_Std_BitVec_reduceDiv___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceDiv___closed__1);
l_Std_BitVec_reduceDiv___closed__2 = _init_l_Std_BitVec_reduceDiv___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceDiv___closed__2);
l_Std_BitVec_reduceDiv___closed__3 = _init_l_Std_BitVec_reduceDiv___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceDiv___closed__3);
l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__1);
l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__2);
l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__3);
l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__4);
l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__5);
l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__6);
l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__7);
l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__8);
l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__9);
l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__10 = _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__10();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__10);
l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__11 = _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__11();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__11);
l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__12 = _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__12();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__12);
l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__13 = _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__13();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__13);
l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__14 = _init_l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__14();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723____closed__14);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1723_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1725_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1727_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceMod___closed__1 = _init_l_Std_BitVec_reduceMod___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceMod___closed__1);
l_Std_BitVec_reduceMod___closed__2 = _init_l_Std_BitVec_reduceMod___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceMod___closed__2);
l_Std_BitVec_reduceMod___closed__3 = _init_l_Std_BitVec_reduceMod___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceMod___closed__3);
l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__1);
l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__2);
l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__3);
l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__4);
l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__5);
l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__6);
l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__7);
l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__8);
l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__9);
l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__10 = _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__10();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__10);
l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__11 = _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__11();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__11);
l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__12 = _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__12();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__12);
l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__13 = _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__13();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__13);
l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__14 = _init_l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__14();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765____closed__14);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1765_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1767_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1769_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceUMod___closed__1 = _init_l_Std_BitVec_reduceUMod___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceUMod___closed__1);
l_Std_BitVec_reduceUMod___closed__2 = _init_l_Std_BitVec_reduceUMod___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceUMod___closed__2);
l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__1);
l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__2);
l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__3);
l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__4);
l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__5);
l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__6);
l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__7);
l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__8);
l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791____closed__9);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1791_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1793_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceUMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1795_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceUDiv___closed__1 = _init_l_Std_BitVec_reduceUDiv___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceUDiv___closed__1);
l_Std_BitVec_reduceUDiv___closed__2 = _init_l_Std_BitVec_reduceUDiv___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceUDiv___closed__2);
l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__1);
l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__2);
l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__3);
l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__4);
l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__5);
l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__6);
l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__7);
l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1817_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1819_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1821_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceSMTUDiv___closed__1 = _init_l_Std_BitVec_reduceSMTUDiv___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceSMTUDiv___closed__1);
l_Std_BitVec_reduceSMTUDiv___closed__2 = _init_l_Std_BitVec_reduceSMTUDiv___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceSMTUDiv___closed__2);
l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__1);
l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__2);
l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__3);
l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__4);
l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__5);
l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__6);
l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__7);
l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1843_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1845_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSMTUDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1847_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceSMod___closed__1 = _init_l_Std_BitVec_reduceSMod___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceSMod___closed__1);
l_Std_BitVec_reduceSMod___closed__2 = _init_l_Std_BitVec_reduceSMod___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceSMod___closed__2);
l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__1);
l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__2);
l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__3);
l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__4);
l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__5);
l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__6);
l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__7);
l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1869_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1871_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSMod_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1873_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceSRem___closed__1 = _init_l_Std_BitVec_reduceSRem___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceSRem___closed__1);
l_Std_BitVec_reduceSRem___closed__2 = _init_l_Std_BitVec_reduceSRem___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceSRem___closed__2);
l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__1);
l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__2);
l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__3);
l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__4);
l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__5);
l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__6);
l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__7);
l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1895_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1897_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSRem_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1899_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceSDiv___closed__1 = _init_l_Std_BitVec_reduceSDiv___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceSDiv___closed__1);
l_Std_BitVec_reduceSDiv___closed__2 = _init_l_Std_BitVec_reduceSDiv___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceSDiv___closed__2);
l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__1);
l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__2);
l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__3);
l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__4);
l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__5);
l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__6);
l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__7);
l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1921_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1923_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1925_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceSMTSDiv___closed__1 = _init_l_Std_BitVec_reduceSMTSDiv___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceSMTSDiv___closed__1);
l_Std_BitVec_reduceSMTSDiv___closed__2 = _init_l_Std_BitVec_reduceSMTSDiv___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceSMTSDiv___closed__2);
l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__1);
l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__2);
l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__3);
l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__4);
l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__5);
l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__6);
l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__7);
l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1947_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1949_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSMTSDiv_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1951_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceGetLsb___closed__1 = _init_l_Std_BitVec_reduceGetLsb___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceGetLsb___closed__1);
l_Std_BitVec_reduceGetLsb___closed__2 = _init_l_Std_BitVec_reduceGetLsb___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceGetLsb___closed__2);
l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__1);
l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__2);
l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__3);
l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__4);
l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__5);
l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__6);
l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__7);
l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1968_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1970_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceGetLsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1972_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceGetMsb___closed__1 = _init_l_Std_BitVec_reduceGetMsb___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceGetMsb___closed__1);
l_Std_BitVec_reduceGetMsb___closed__2 = _init_l_Std_BitVec_reduceGetMsb___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceGetMsb___closed__2);
l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__1);
l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__2);
l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__3);
l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__4);
l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__5);
l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__6);
l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__7);
l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1989_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1991_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceGetMsb_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_1993_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceShiftLeft___closed__1 = _init_l_Std_BitVec_reduceShiftLeft___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceShiftLeft___closed__1);
l_Std_BitVec_reduceShiftLeft___closed__2 = _init_l_Std_BitVec_reduceShiftLeft___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceShiftLeft___closed__2);
l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__1);
l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__2);
l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__3);
l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__4);
l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__5);
l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__6);
l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__7);
l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2011_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2013_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2015_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceUShiftRight___closed__1 = _init_l_Std_BitVec_reduceUShiftRight___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceUShiftRight___closed__1);
l_Std_BitVec_reduceUShiftRight___closed__2 = _init_l_Std_BitVec_reduceUShiftRight___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceUShiftRight___closed__2);
l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__1);
l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__2);
l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__3);
l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__4);
l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__5);
l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__6);
l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__7);
l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2033_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2035_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceUShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2037_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceSShiftRight___closed__1 = _init_l_Std_BitVec_reduceSShiftRight___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceSShiftRight___closed__1);
l_Std_BitVec_reduceSShiftRight___closed__2 = _init_l_Std_BitVec_reduceSShiftRight___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceSShiftRight___closed__2);
l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__1);
l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__2);
l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__3);
l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__4);
l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__5);
l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__6);
l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__7);
l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2055_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2057_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2059_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceHShiftLeft___closed__1 = _init_l_Std_BitVec_reduceHShiftLeft___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceHShiftLeft___closed__1);
l_Std_BitVec_reduceHShiftLeft___closed__2 = _init_l_Std_BitVec_reduceHShiftLeft___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceHShiftLeft___closed__2);
l_Std_BitVec_reduceHShiftLeft___closed__3 = _init_l_Std_BitVec_reduceHShiftLeft___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceHShiftLeft___closed__3);
l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__1);
l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__2);
l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__3);
l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__4);
l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__5);
l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__6);
l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__7);
l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__8);
l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__9);
l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__10 = _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__10();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__10);
l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__11 = _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__11();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__11);
l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__12 = _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__12();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__12);
l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__13 = _init_l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__13();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097____closed__13);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2097_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2099_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceHShiftLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2101_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceHShiftRight___closed__1 = _init_l_Std_BitVec_reduceHShiftRight___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceHShiftRight___closed__1);
l_Std_BitVec_reduceHShiftRight___closed__2 = _init_l_Std_BitVec_reduceHShiftRight___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceHShiftRight___closed__2);
l_Std_BitVec_reduceHShiftRight___closed__3 = _init_l_Std_BitVec_reduceHShiftRight___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceHShiftRight___closed__3);
l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__1);
l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__2);
l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__3);
l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__4);
l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__5);
l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__6);
l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__7);
l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__8);
l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__9);
l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__10 = _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__10();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__10);
l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__11 = _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__11();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__11);
l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__12 = _init_l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__12();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139____closed__12);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2139_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2141_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceHShiftRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2143_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceRotateLeft___closed__1 = _init_l_Std_BitVec_reduceRotateLeft___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceRotateLeft___closed__1);
l_Std_BitVec_reduceRotateLeft___closed__2 = _init_l_Std_BitVec_reduceRotateLeft___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceRotateLeft___closed__2);
l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__1);
l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__2);
l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__3);
l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__4);
l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__5);
l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__6);
l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__7);
l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2161_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2163_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceRotateLeft_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2165_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceRotateRight___closed__1 = _init_l_Std_BitVec_reduceRotateRight___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceRotateRight___closed__1);
l_Std_BitVec_reduceRotateRight___closed__2 = _init_l_Std_BitVec_reduceRotateRight___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceRotateRight___closed__2);
l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__1);
l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__2);
l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__3);
l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__4);
l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__5);
l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__6);
l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__7);
l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2183_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2185_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceRotateRight_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2187_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceAppend___closed__1 = _init_l_Std_BitVec_reduceAppend___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceAppend___closed__1);
l_Std_BitVec_reduceAppend___closed__2 = _init_l_Std_BitVec_reduceAppend___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceAppend___closed__2);
l_Std_BitVec_reduceAppend___closed__3 = _init_l_Std_BitVec_reduceAppend___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceAppend___closed__3);
l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__1);
l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__2);
l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__3);
l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__4);
l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__5);
l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__6);
l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__7);
l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__8);
l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__9);
l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__10 = _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__10();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__10);
l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__11 = _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__11();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__11);
l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__12 = _init_l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__12();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350____closed__12);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2350_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2352_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceAppend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2354_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceCast___closed__1 = _init_l_Std_BitVec_reduceCast___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceCast___closed__1);
l_Std_BitVec_reduceCast___closed__2 = _init_l_Std_BitVec_reduceCast___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceCast___closed__2);
l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__1);
l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__2);
l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__3);
l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__4);
l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__5);
l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__6);
l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__7);
l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__8);
l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505____closed__9);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2505_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2507_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceCast_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2509_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceToNat___closed__1 = _init_l_Std_BitVec_reduceToNat___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceToNat___closed__1);
l_Std_BitVec_reduceToNat___closed__2 = _init_l_Std_BitVec_reduceToNat___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceToNat___closed__2);
l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__1);
l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__2);
l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__3);
l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__4);
l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__5);
l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__6);
l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613____closed__7);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2613_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2615_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceToNat_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2617_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceToInt___closed__1 = _init_l_Std_BitVec_reduceToInt___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceToInt___closed__1);
l_Std_BitVec_reduceToInt___closed__2 = _init_l_Std_BitVec_reduceToInt___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceToInt___closed__2);
l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__1);
l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__2);
l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__3);
l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__4);
l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__5);
l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__6);
l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721____closed__7);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2721_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2723_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceToInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2725_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceOfInt___closed__1 = _init_l_Std_BitVec_reduceOfInt___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceOfInt___closed__1);
l_Std_BitVec_reduceOfInt___closed__2 = _init_l_Std_BitVec_reduceOfInt___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceOfInt___closed__2);
l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__1);
l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__2);
l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__3);
l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__4);
l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__5);
l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__6);
l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878____closed__7);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2878_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2880_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceOfInt_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2882_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceLT___closed__1 = _init_l_Std_BitVec_reduceLT___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceLT___closed__1);
l_Std_BitVec_reduceLT___closed__2 = _init_l_Std_BitVec_reduceLT___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceLT___closed__2);
l_Std_BitVec_reduceLT___closed__3 = _init_l_Std_BitVec_reduceLT___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceLT___closed__3);
l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__1);
l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__2);
l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__3);
l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__4);
l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__5);
l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__6);
l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__7);
l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__8);
l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__9);
l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__10 = _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__10();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__10);
l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__11 = _init_l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__11();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921____closed__11);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2921_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2923_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2925_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceLE___closed__1 = _init_l_Std_BitVec_reduceLE___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceLE___closed__1);
l_Std_BitVec_reduceLE___closed__2 = _init_l_Std_BitVec_reduceLE___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceLE___closed__2);
l_Std_BitVec_reduceLE___closed__3 = _init_l_Std_BitVec_reduceLE___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceLE___closed__3);
l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__1);
l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__2);
l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__3);
l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__4);
l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__5);
l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__6);
l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__7);
l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__8);
l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__9);
l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__10 = _init_l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__10();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964____closed__10);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2964_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2966_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_2968_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceGT___closed__1 = _init_l_Std_BitVec_reduceGT___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceGT___closed__1);
l_Std_BitVec_reduceGT___closed__2 = _init_l_Std_BitVec_reduceGT___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceGT___closed__2);
l_Std_BitVec_reduceGT___closed__3 = _init_l_Std_BitVec_reduceGT___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceGT___closed__3);
l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__1);
l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__2);
l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007____closed__3);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3007_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3009_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceGT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3011_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceGE___closed__1 = _init_l_Std_BitVec_reduceGE___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceGE___closed__1);
l_Std_BitVec_reduceGE___closed__2 = _init_l_Std_BitVec_reduceGE___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceGE___closed__2);
l_Std_BitVec_reduceGE___closed__3 = _init_l_Std_BitVec_reduceGE___closed__3();
lean_mark_persistent(l_Std_BitVec_reduceGE___closed__3);
l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__1);
l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__2);
l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050____closed__3);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3050_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3052_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceGE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3054_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceULT___closed__1 = _init_l_Std_BitVec_reduceULT___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceULT___closed__1);
l_Std_BitVec_reduceULT___closed__2 = _init_l_Std_BitVec_reduceULT___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceULT___closed__2);
l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__1);
l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__2);
l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__3);
l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__4);
l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__5);
l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__6);
l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__7);
l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3073_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3075_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceULT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3077_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceULE___closed__1 = _init_l_Std_BitVec_reduceULE___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceULE___closed__1);
l_Std_BitVec_reduceULE___closed__2 = _init_l_Std_BitVec_reduceULE___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceULE___closed__2);
l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__1);
l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__2);
l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__3);
l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__4);
l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__5);
l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__6);
l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__7);
l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3096_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3098_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceULE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3100_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceSLT___closed__1 = _init_l_Std_BitVec_reduceSLT___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceSLT___closed__1);
l_Std_BitVec_reduceSLT___closed__2 = _init_l_Std_BitVec_reduceSLT___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceSLT___closed__2);
l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__1);
l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__2);
l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__3);
l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__4);
l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__5);
l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__6);
l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__7);
l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3119_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3121_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSLT_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3123_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceSLE___closed__1 = _init_l_Std_BitVec_reduceSLE___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceSLE___closed__1);
l_Std_BitVec_reduceSLE___closed__2 = _init_l_Std_BitVec_reduceSLE___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceSLE___closed__2);
l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__1);
l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__2);
l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__3);
l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__4);
l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__5);
l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__6);
l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__7);
l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3142_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3144_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSLE_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3146_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceZeroExtend_x27___closed__1 = _init_l_Std_BitVec_reduceZeroExtend_x27___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceZeroExtend_x27___closed__1);
l_Std_BitVec_reduceZeroExtend_x27___closed__2 = _init_l_Std_BitVec_reduceZeroExtend_x27___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceZeroExtend_x27___closed__2);
l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__1);
l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__2);
l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__3);
l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__4);
l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__5);
l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__6);
l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__7);
l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__8);
l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321____closed__9);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3321_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3323_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceZeroExtend_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3325_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceShiftLeftZeroExtend___closed__1 = _init_l_Std_BitVec_reduceShiftLeftZeroExtend___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceShiftLeftZeroExtend___closed__1);
l_Std_BitVec_reduceShiftLeftZeroExtend___closed__2 = _init_l_Std_BitVec_reduceShiftLeftZeroExtend___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceShiftLeftZeroExtend___closed__2);
l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__1);
l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__2);
l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__3);
l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__4);
l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__5);
l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__6);
l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__7);
l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3479_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3481_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceShiftLeftZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3483_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceExtracLsb_x27___closed__1 = _init_l_Std_BitVec_reduceExtracLsb_x27___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceExtracLsb_x27___closed__1);
l_Std_BitVec_reduceExtracLsb_x27___closed__2 = _init_l_Std_BitVec_reduceExtracLsb_x27___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceExtracLsb_x27___closed__2);
l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__1);
l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__2);
l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__3);
l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__4);
l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__5);
l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__6);
l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__7);
l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__8);
l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__9 = _init_l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__9();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671____closed__9);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3671_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3673_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceExtracLsb_x27_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3675_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceReplicate___closed__1 = _init_l_Std_BitVec_reduceReplicate___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceReplicate___closed__1);
l_Std_BitVec_reduceReplicate___closed__2 = _init_l_Std_BitVec_reduceReplicate___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceReplicate___closed__2);
l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__1);
l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__2);
l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__3);
l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__4);
l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__5);
l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__6);
l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__7);
l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3829_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3831_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceReplicate_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3833_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceZeroExtend___closed__1 = _init_l_Std_BitVec_reduceZeroExtend___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceZeroExtend___closed__1);
l_Std_BitVec_reduceZeroExtend___closed__2 = _init_l_Std_BitVec_reduceZeroExtend___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceZeroExtend___closed__2);
l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__1);
l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__2);
l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__3);
l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__4);
l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__5);
l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__6);
l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__7);
l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3850_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3852_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceZeroExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3854_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceSignExtend___closed__1 = _init_l_Std_BitVec_reduceSignExtend___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceSignExtend___closed__1);
l_Std_BitVec_reduceSignExtend___closed__2 = _init_l_Std_BitVec_reduceSignExtend___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceSignExtend___closed__2);
l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__1);
l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__2);
l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__3);
l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__4);
l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__5);
l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__6);
l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__7);
l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__8 = _init_l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__8();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871____closed__8);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3871_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3873_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceSignExtend_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3875_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}l_Std_BitVec_reduceAllOnes___closed__1 = _init_l_Std_BitVec_reduceAllOnes___closed__1();
lean_mark_persistent(l_Std_BitVec_reduceAllOnes___closed__1);
l_Std_BitVec_reduceAllOnes___closed__2 = _init_l_Std_BitVec_reduceAllOnes___closed__2();
lean_mark_persistent(l_Std_BitVec_reduceAllOnes___closed__2);
l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__1 = _init_l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__1();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__1);
l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__2 = _init_l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__2();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__2);
l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__3 = _init_l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__3();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__3);
l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__4 = _init_l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__4();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__4);
l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__5 = _init_l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__5();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__5);
l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__6 = _init_l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__6();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__6);
l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__7 = _init_l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__7();
lean_mark_persistent(l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990____closed__7);
if (builtin) {res = l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3990_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3992_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}if (builtin) {res = l___regBuiltin_Std_BitVec_reduceAllOnes_declare____x40_Lean_Meta_Tactic_Simp_BuiltinSimprocs_BitVec___hyg_3994_(lean_io_mk_world());
if (lean_io_result_is_error(res)) return res;
lean_dec_ref(res);
}return lean_io_result_mk_ok(lean_box(0));
}
#ifdef __cplusplus
}
#endif
